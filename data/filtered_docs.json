[
  {
    "title": "How to interact with the deployment using RemoteGraph",
    "source": "how-tos/use-remote-graph.md",
    "content": "# How to interact with the deployment using RemoteGraph\n\n!!! info \"Prerequisites\"\n\n    - [LangGraph Platform](../concepts/langgraph_platform.md)\n    - [LangGraph Server](../concepts/langgraph_server.md)\n\n`RemoteGraph` is an interface that allows you to interact with your LangGraph Platform deployment as if it were a regular, locally-defined LangGraph graph (e.g. a `CompiledGraph`). This guide shows you how you can initialize a `RemoteGraph` and interact with it.\n\n## Initializing the graph\n\nWhen initializing a `RemoteGraph`, you must always specify:\n\n- `name`: the name of the graph you want to interact with. This is the same graph name you use in `langgraph.json` configuration file for your deployment.\n- `api_key`: a valid LangSmith API key. Can be set as an environment variable (`LANGSMITH_API_KEY`) or passed directly via the `api_key` argument. The API key could also be provided via the `client` / `sync_client` arguments, if `LangGraphClient` / `SyncLangGraphClient` were initialized with `api_key` argument.\n\nAdditionally, you have to provide one of the following:\n\n- `url`: URL of the deployment you want to interact with. If you pass `url` argument, both sync and async clients will be created using the provided URL, headers (if provided) and default configuration values (e.g. timeout, etc).\n- `client`: a `LangGraphClient` instance for interacting with the deployment asynchronously (e.g. using `.astream()`, `.ainvoke()`, `.aget_state()`, `.aupdate_state()`, etc.)\n- `sync_client`: a `SyncLangGraphClient` instance for interacting with the deployment synchronously (e.g. using `.stream()`, `.invoke()`, `.get_state()`, `.update_state()`, etc.)\n\n!!! Note\n\n    If you pass both `client` or `sync_client` as well as `url` argument, they will take precedence over the `url` argument. If none of the `client` / `sync_client` / `url` arguments are provided, `RemoteGraph` will raise a `ValueError` at runtime.\n\n\n\n\n\n### Using URL\n\n```python\nfrom langgraph.pregel.remote import RemoteGraph\n\nurl = <DEPLOYMENT_URL>\ngraph_name = \"agent\"\nremote_graph = RemoteGraph(graph_name, url=url)\n```\n\n\n\n\n\n### Using clients\n\n```python\nfrom langgraph_sdk import get_client, get_sync_client\nfrom langgraph.pregel.remote import RemoteGraph\n\nurl = <DEPLOYMENT_URL>\ngraph_name = \"agent\"\nclient = get_client(url=url)\nsync_client = get_sync_client(url=url)\nremote_graph = RemoteGraph(graph_name, client=client, sync_client=sync_client)\n```\n\n\n\n\n\n## Invoking the graph\n\nSince `RemoteGraph` is a `Runnable` that implements the same methods as `CompiledGraph`, you can interact with it the same way you normally would with a compiled graph, i.e. by calling `.invoke()`, `.stream()`, `.get_state()`, `.update_state()`, etc (as well as their async counterparts).\n\n### Asynchronously\n\n!!! Note\n\n    To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.\n\n```python\n# invoke the graph\nresult = await remote_graph.ainvoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]\n})\n\n# stream outputs from the graph\nasync for chunk in remote_graph.astream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in la\"}]\n}):\n    print(chunk)\n```\n\n### Synchronously\n\n!!! Note\n\n    To use the graph synchronously, you must provide either the `url` or `sync_client` when initializing the `RemoteGraph`.\n\n```python\n# invoke the graph\nresult = remote_graph.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]\n})\n\n# stream outputs from the graph\nfor chunk in remote_graph.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in la\"}]\n}):\n    print(chunk)\n```\n\n\n\n\n\n## Thread-level persistence\n\nBy default, the graph runs (i.e. `.invoke()` or `.stream()` invocations) are stateless - the checkpoints and the final state of the graph are not persisted. If you would like to persist the outputs of the graph run (for example, to enable human-in-the-loop features), you can create a thread and provide the thread ID via the `config` argument, same as you would with a regular compiled graph:\n\n```python\nfrom langgraph_sdk import get_sync_client\nurl = <DEPLOYMENT_URL>\ngraph_name = \"agent\"\nsync_client = get_sync_client(url=url)\nremote_graph = RemoteGraph(graph_name, url=url)\n\n# create a thread (or use an existing thread instead)\nthread = sync_client.threads.create()\n\n# invoke the graph with the thread config\nconfig = {\"configurable\": {\"thread_id\": thread[\"thread_id\"]}}\nresult = remote_graph.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]\n}, config=config)\n\n# verify that the state was persisted to the thread\nthread_state = remote_graph.get_state(config)\nprint(thread_state)\n```\n\n\n\n\n\n## Using as a subgraph\n\n!!! Note\n\n    If you need to use a `checkpointer` with a graph that has a `RemoteGraph` subgraph node, make sure to use UUIDs as thread IDs.\n\nSince the `RemoteGraph` behaves the same way as a regular `CompiledGraph`, it can be also used as a subgraph in another graph. For example:\n\n```python\nfrom langgraph_sdk import get_sync_client\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom typing import TypedDict\n\nurl = <DEPLOYMENT_URL>\ngraph_name = \"agent\"\nremote_graph = RemoteGraph(graph_name, url=url)\n\n# define parent graph\nbuilder = StateGraph(MessagesState)\n# add remote graph directly as a node\nbuilder.add_node(\"child\", remote_graph)\nbuilder.add_edge(START, \"child\")\ngraph = builder.compile()\n\n# invoke the parent graph\nresult = graph.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]\n})\nprint(result)\n\n# stream outputs from both the parent graph and subgraph\nfor chunk in graph.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]\n}, subgraphs=True):\n    print(chunk)\n```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 5820,
    "word_count": 774
  },
  {
    "title": "How to integrate LangGraph with AutoGen, CrewAI, and other frameworks",
    "source": "how-tos/autogen-integration.md",
    "content": "# How to integrate LangGraph with AutoGen, CrewAI, and other frameworks\n\nThis guide shows how to integrate AutoGen agents with LangGraph to leverage features like persistence, streaming, and memory, and then deploy the integrated solution to LangGraph Platform for scalable production use. In this guide we show how to build a LangGraph chatbot that integrates with AutoGen, but you can follow the same approach with other frameworks.\n\nIntegrating AutoGen with LangGraph provides several benefits:\n\n- Enhanced features: Add [persistence](../concepts/persistence.md), [streaming](../concepts/streaming.md), [short and long-term memory](../concepts/memory.md) and more to your AutoGen agents.\n- Multi-agent systems: Build [multi-agent systems](../concepts/multi_agent.md) where individual agents are built with different frameworks.\n- Production deployment: Deploy your integrated solution to [LangGraph Platform](../concepts/langgraph_platform.md) for scalable production use.\n\n## Prerequisites\n\n- Python 3.9+\n- Autogen: `pip install autogen`\n- LangGraph: `pip install langgraph`\n- OpenAI API key\n\n## Setup\n\nSet your your environment:\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n## 1. Define AutoGen agent\n\nCreate an AutoGen agent that can execute code. This example is adapted from AutoGen's [official tutorials](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb):\n\n```python\nimport autogen\nimport os\n\nconfig_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n\nllm_config = {\n    \"timeout\": 600,\n    \"cache_seed\": 42,\n    \"config_list\": config_list,\n    \"temperature\": 0,\n}\n\nautogen_agent = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config,\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n    code_execution_config={\n        \"work_dir\": \"web\",\n        \"use_docker\": False,\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n    llm_config=llm_config,\n    system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\n)\n```\n\n## 2. Create the graph\n\nWe will now create a LangGraph chatbot graph that calls AutoGen agent.\n\n```python\nfrom langchain_core.messages import convert_to_openai_messages\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ndef call_autogen_agent(state: MessagesState):\n    # Convert LangGraph messages to OpenAI format for AutoGen\n    messages = convert_to_openai_messages(state[\"messages\"])\n    \n    # Get the last user message\n    last_message = messages[-1]\n    \n    # Pass previous message history as context (excluding the last message)\n    carryover = messages[:-1] if len(messages) > 1 else []\n    \n    # Initiate chat with AutoGen\n    response = user_proxy.initiate_chat(\n        autogen_agent,\n        message=last_message,\n        carryover=carryover\n    )\n    \n    # Extract the final response from the agent\n    final_content = response.chat_history[-1][\"content\"]\n    \n    # Return the response in LangGraph format\n    return {\"messages\": {\"role\": \"assistant\", \"content\": final_content}}\n\n# Create the graph with memory for persistence\ncheckpointer = InMemorySaver()\n\n# Build the graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"autogen\", call_autogen_agent)\nbuilder.add_edge(START, \"autogen\")\n\n# Compile with checkpointer for persistence\ngraph = builder.compile(checkpointer=checkpointer)\n```\n\n```python\nfrom IPython.display import display, Image\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Graph](./assets/autogen-output.png)\n\n## 3. Test the graph locally\n\nBefore deploying to LangGraph Platform, you can test the graph locally:\n\n```python hl_lines=\"2 13\"\n# pass the thread ID to persist agent outputs for future interactions\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\",\n            }\n        ]\n    },\n    config,\n):\n    print(chunk)\n```\n\n**Output:**\n```\nuser_proxy (to assistant):\n\nFind numbers between 10 and 30 in fibonacci sequence\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nTo find numbers between 10 and 30 in the Fibonacci sequence, we can generate the Fibonacci sequence and check which numbers fall within this range. Here's a plan:\n\n1. Generate Fibonacci numbers starting from 0.\n2. Continue generating until the numbers exceed 30.\n3. Collect and print the numbers that are between 10 and 30.\n\n...\n```\n\nSince we're leveraging LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) features we can now continue the conversation using the same thread ID -- LangGraph will automatically pass previous history to the AutoGen agent:\n\n```python hl_lines=\"10\"\nfor chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Multiply the last number by 3\",\n            }\n        ]\n    },\n    config,\n):\n    print(chunk)\n```\n\n**Output:**\n```\nuser_proxy (to assistant):\n\nMultiply the last number by 3\nContext: \nFind numbers between 10 and 30 in fibonacci sequence\nThe Fibonacci numbers between 10 and 30 are 13 and 21. \n\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \n\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nThe last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\n\n21 * 3 = 63\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n{'call_autogen_agent': {'messages': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}}\n``` \n\n## 4. Prepare for deployment\n\nTo deploy to LangGraph Platform, create a file structure like the following:\n\n```\nmy-autogen-agent/\n├── agent.py          # Your main agent code\n├── requirements.txt  # Python dependencies\n└── langgraph.json   # LangGraph configuration\n```\n\n=== \"agent.py\"\n\n    ```python\n    import os\n    import autogen\n    from langchain_core.messages import convert_to_openai_messages\n    from langgraph.graph import StateGraph, MessagesState, START\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    # AutoGen configuration\n    config_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n\n    llm_config = {\n        \"timeout\": 600,\n        \"cache_seed\": 42,\n        \"config_list\": config_list,\n        \"temperature\": 0,\n    }\n\n    # Create AutoGen agents\n    autogen_agent = autogen.AssistantAgent(\n        name=\"assistant\",\n        llm_config=llm_config,\n    )\n\n    user_proxy = autogen.UserProxyAgent(\n        name=\"user_proxy\",\n        human_input_mode=\"NEVER\",\n        max_consecutive_auto_reply=10,\n        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n        code_execution_config={\n            \"work_dir\": \"/tmp/autogen_work\",\n            \"use_docker\": False,\n        },\n        llm_config=llm_config,\n        system_message=\"Reply TERMINATE if the task has been solved at full satisfaction.\",\n    )\n\n    def call_autogen_agent(state: MessagesState):\n        \"\"\"Node function that calls the AutoGen agent\"\"\"\n        messages = convert_to_openai_messages(state[\"messages\"])\n        last_message = messages[-1]\n        carryover = messages[:-1] if len(messages) > 1 else []\n        \n        response = user_proxy.initiate_chat(\n            autogen_agent,\n            message=last_message,\n            carryover=carryover\n        )\n        \n        final_content = response.chat_history[-1][\"content\"]\n        return {\"messages\": {\"role\": \"assistant\", \"content\": final_content}}\n\n    # Create and compile the graph\n    def create_graph():\n        checkpointer = InMemorySaver()\n        builder = StateGraph(MessagesState)\n        builder.add_node(\"autogen\", call_autogen_agent)\n        builder.add_edge(START, \"autogen\")\n        return builder.compile(checkpointer=checkpointer)\n\n    # Export the graph for LangGraph Platform\n    graph = create_graph()\n    ```\n\n=== \"requirements.txt\"\n\n    ```\n    langgraph>=0.1.0\n    ag2>=0.2.0\n    langchain-core>=0.1.0\n    langchain-openai>=0.0.5\n    ```\n\n=== \"langgraph.json\"\n\n    ```json\n    {\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"autogen_agent\": \"./agent.py:graph\"\n    },\n    \"env\": \".env\"\n    }\n    ```\n\n\n## 5. Deploy to LangGraph Platform\n\nDeploy the graph with the LangGraph Platform CLI:\n\n```\npip install -U langgraph-cli\n```\n\n```\nlanggraph deploy --config langgraph.json \n```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 9423,
    "word_count": 1012
  },
  {
    "title": "Enable tracing for your application",
    "source": "how-tos/enable-tracing.md",
    "content": "# Enable tracing for your application\n\nTo enable [tracing](../concepts/tracing.md) for your application, set the following environment variables:\n\n```python\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n```\n\nFor more information, see [Trace with LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph).\n\n## Learn more\n\n- [Graph runs in LangSmith](../how-tos/run-id-langsmith.md)\n- [LangSmith Observability quickstart](https://docs.smith.langchain.com/observability)\n- [Tracing conceptual guide](https://docs.smith.langchain.com/observability/concepts#traces)",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 620,
    "word_count": 46
  },
  {
    "title": "How to use the graph API",
    "source": "how-tos/graph-api.md",
    "content": "# How to use the graph API\n\nThis guide demonstrates the basics of LangGraph's Graph API. It walks through [state](#define-and-update-state), as well as composing common graph structures such as [sequences](#create-a-sequence-of-steps), [branches](#create-branches), and [loops](#create-and-control-loops). It also covers LangGraph's control features, including the [Send API](#map-reduce-and-the-send-api) for map-reduce workflows and the [Command API](#combine-control-flow-and-state-updates-with-command) for combining state updates with \"hops\" across nodes.\n\n## Setup\n\nInstall `langgraph`:\n\n```bash\npip install -U langgraph\n```\n\n\n\n\n!!! tip \"Set up LangSmith for better debugging\"\n\n    Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](https://docs.smith.langchain.com).\n\n## Define and update state\n\nHere we show how to define and update [state](../concepts/low_level.md#state) in LangGraph. We will demonstrate:\n\n1. How to use state to define a graph's [schema](../concepts/low_level.md#schema)\n2. How to use [reducers](../concepts/low_level.md#reducers) to control how state updates are processed.\n\n### Define state\n\n[State](../concepts/low_level.md#state) in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See [this section](#use-pydantic-models-for-graph-state) for detail on using Pydantic.\n\n\n\n\nBy default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.\n\nLet's consider a simple example using [messages](../concepts/low_level.md#working-with-messages-in-graph-state). This represents a versatile formulation of state for many LLM applications. See our [concepts page](../concepts/low_level.md#working-with-messages-in-graph-state) for more detail.\n\n```python\nfrom langchain_core.messages import AnyMessage\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    messages: list[AnyMessage]\n    extra_field: int\n```\n\nThis state tracks a list of [message](https://python.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.\n\n\n\n\n### Update state\n\nLet's build an example graph with a single node. Our [node](../concepts/low_level.md#nodes) is just a Python function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:\n\n```python\nfrom langchain_core.messages import AIMessage\n\ndef node(state: State):\n    messages = state[\"messages\"]\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\n```\n\nThis node simply appends a message to our message list, and populates an extra field.\n\n\n\n\n!!! important\n\n    Nodes should return updates to the state directly, instead of mutating the state.\n\nLet's next define a simple graph containing this node. We use [StateGraph](../concepts/low_level.md#stategraph) to define a graph that operates on this state. We then use [add_node](../concepts/low_level.md#nodes) populate our graph.\n\n```python\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(State)\nbuilder.add_node(node)\nbuilder.set_entry_point(\"node\")\ngraph = builder.compile()\n```\n\n\n\n\nLangGraph provides built-in utilities for visualizing your graph. Let's inspect our graph. See [this section](#visualize-your-graph) for detail on visualization.\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Simple graph with single node](assets/graph_api_image_1.png)\n\n\n\n\nIn this case, our graph just executes a single node. Let's proceed with a simple invocation:\n\n```python\nfrom langchain_core.messages import HumanMessage\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\nresult\n```\n\n```\n{'messages': [HumanMessage(content='Hi'), AIMessage(content='Hello!')], 'extra_field': 10}\n```\n\n\n\n\nNote that:\n\n- We kicked off invocation by updating a single key of the state.\n- We receive the entire state in the invocation result.\n\nFor convenience, we frequently inspect the content of [message objects](https://python.langchain.com/docs/concepts/messages/) via pretty-print:\n\n```python\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n\n```\n================================ Human Message ================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n```\n\n\n\n\n### Process state updates with reducers\n\nEach key in the state can have its own independent [reducer](../concepts/low_level.md#reducers) function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\n\nFor `TypedDict` state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\n\nIn the earlier example, our node updated the `\"messages\"` key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\n\n```python hl_lines=\"8\"\nfrom typing_extensions import Annotated\n\ndef add(left, right):\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\n    return left + right\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add]\n    extra_field: int\n```\n\nNow our node can be simplified:\n\n```python hl_lines=\"3\"\ndef node(state: State):\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": [new_message], \"extra_field\": 10}\n```\n\n\n\n\n```python\nfrom langgraph.graph import START\n\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n\n```\n================================ Human Message ================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n```\n\n\n\n\n#### MessagesState\n\nIn practice, there are additional considerations for updating lists of messages:\n\n- We may wish to update an existing message in the state.\n- We may want to accept short-hands for [message formats](../concepts/low_level.md#using-messages-in-your-graph), such as [OpenAI format](https://python.langchain.com/docs/concepts/messages/#openai-format).\n\nLangGraph includes a built-in reducer `add_messages` that handles these considerations:\n\n```python hl_lines=\"4\"\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    extra_field: int\n\ndef node(state: State):\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": [new_message], \"extra_field\": 10}\n\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\n```\n\n```python hl_lines=\"1\"\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}\n\nresult = graph.invoke({\"messages\": [input_message]})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n\n```\n================================ Human Message ================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n```\n\nThis is a versatile representation of state for applications involving [chat models](https://python.langchain.com/docs/concepts/chat_models/). LangGraph includes a pre-built `MessagesState` for convenience, so that we can have:\n\n```python\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    extra_field: int\n```\n\n\n\n\n### Define input and output schemas\n\nBy default, `StateGraph` operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph.\n\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\n\nBelow, we'll see how to define distinct input and output schema.\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# Define the schema for the input\nclass InputState(TypedDict):\n    question: str\n\n# Define the schema for the output\nclass OutputState(TypedDict):\n    answer: str\n\n# Define the overall schema, combining both input and output\nclass OverallState(InputState, OutputState):\n    pass\n\n# Define the node that processes the input and generates an answer\ndef answer_node(state: InputState):\n    # Example answer and an extra key\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n\n# Build the graph with input and output schemas specified\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(answer_node)  # Add the answer node\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\ngraph = builder.compile()  # Compile the graph\n\n# Invoke the graph with an input and print the result\nprint(graph.invoke({\"question\": \"hi\"}))\n```\n\n```\n{'answer': 'bye'}\n```\n\n\n\n\nNotice that the output of invoke only includes the output schema.\n\n### Pass private state between nodes\n\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn't need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\n\nBelow, we'll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(TypedDict):\n    a: str\n\n# Output from node_1 contains private data that is not part of the overall state\nclass Node1Output(TypedDict):\n    private_data: str\n\n# The private data is only shared between node_1 and node_2\ndef node_1(state: OverallState) -> Node1Output:\n    output = {\"private_data\": \"set by node_1\"}\n    print(f\"Entered node `node_1`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\n# Node 2 input only requests the private data available after node_1\nclass Node2Input(TypedDict):\n    private_data: str\n\ndef node_2(state: Node2Input) -> OverallState:\n    output = {\"a\": \"set by node_2\"}\n    print(f\"Entered node `node_2`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\n# Node 3 only has access to the overall state (no access to private data from node_1)\ndef node_3(state: OverallState) -> OverallState:\n    output = {\"a\": \"set by node_3\"}\n    print(f\"Entered node `node_3`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\n# Connect nodes in a sequence\n# node_2 accepts private data from node_1, whereas\n# node_3 does not see the private data.\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n\n# Invoke the graph with the initial state\nresponse = graph.invoke(\n    {\n        \"a\": \"set at start\",\n    }\n)\n\nprint()\nprint(f\"Output of graph invocation: {response}\")\n```\n\n```\nEntered node `node_1`:\n\tInput: {'a': 'set at start'}.\n\tReturned: {'private_data': 'set by node_1'}\nEntered node `node_2`:\n\tInput: {'private_data': 'set by node_1'}.\n\tReturned: {'a': 'set by node_2'}\nEntered node `node_3`:\n\tInput: {'a': 'set by node_2'}.\n\tReturned: {'a': 'set by node_3'}\n\nOutput of graph invocation: {'a': 'set by node_3'}\n```\n\n\n\n\n### Use Pydantic models for graph state\n\nA [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.StateGraph) accepts a `state_schema` argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\n\nIn our examples, we typically use a python-native `TypedDict` or [`dataclass`](https://docs.python.org/3/library/dataclasses.html) for `state_schema`, but `state_schema` can be any [type](https://docs.python.org/3/library/stdtypes.html#type-objects).\n\nHere, we'll see how a [Pydantic BaseModel](https://docs.pydantic.dev/latest/api/base_model/) can be used for `state_schema` to add run-time validation on **inputs**.\n\n!!! note \"Known Limitations\" \n\n    - Currently, the output of the graph will **NOT** be an instance of a pydantic model. \n    - Run-time validation only occurs on inputs into nodes, not on the outputs. \n    - The validation error trace from pydantic does not show which node the error arises in. \n    - Pydantic's recursive validation can be slow. For performance-sensitive applications, you may want to consider using a `dataclass` instead.\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(BaseModel):\n    a: str\n\ndef node(state: OverallState):\n    return {\"a\": \"goodbye\"}\n\n# Build the state graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(node)  # node_1 is the first node\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\ngraph = builder.compile()\n\n# Test the graph with a valid input\ngraph.invoke({\"a\": \"hello\"})\n```\n\nInvoke the graph with an **invalid** input\n\n```python\ntry:\n    graph.invoke({\"a\": 123})  # Should be a string\nexcept Exception as e:\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\n    print(e)\n```\n\n```\nAn exception was raised because `a` is an integer rather than a string.\n1 validation error for OverallState\na\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\n```\n\nSee below for additional features of Pydantic model state:\n\n??? example \"Serialization Behavior\"\n\n    When using Pydantic models as state schemas, it's important to understand how serialization works, especially when:\n    - Passing Pydantic objects as inputs\n    - Receiving outputs from the graph\n    - Working with nested Pydantic models\n\n    Let's see these behaviors in action.\n\n    ```python\n    from langgraph.graph import StateGraph, START, END\n    from pydantic import BaseModel\n\n    class NestedModel(BaseModel):\n        value: str\n\n    class ComplexState(BaseModel):\n        text: str\n        count: int\n        nested: NestedModel\n\n    def process_node(state: ComplexState):\n        # Node receives a validated Pydantic object\n        print(f\"Input state type: {type(state)}\")\n        print(f\"Nested type: {type(state.nested)}\")\n        # Return a dictionary update\n        return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\n\n    # Build the graph\n    builder = StateGraph(ComplexState)\n    builder.add_node(\"process\", process_node)\n    builder.add_edge(START, \"process\")\n    builder.add_edge(\"process\", END)\n    graph = builder.compile()\n\n    # Create a Pydantic instance for input\n    input_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\n    print(f\"Input object type: {type(input_state)}\")\n\n    # Invoke graph with a Pydantic instance\n    result = graph.invoke(input_state)\n    print(f\"Output type: {type(result)}\")\n    print(f\"Output content: {result}\")\n\n    # Convert back to Pydantic model if needed\n    output_model = ComplexState(**result)\n    print(f\"Converted back to Pydantic: {type(output_model)}\")\n    ```\n\n??? example \"Runtime Type Coercion\"\n\n    Pydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you're not aware of it.\n\n    ```python\n    from langgraph.graph import StateGraph, START, END\n    from pydantic import BaseModel\n\n    class CoercionExample(BaseModel):\n        # Pydantic will coerce string numbers to integers\n        number: int\n        # Pydantic will parse string booleans to bool\n        flag: bool\n\n    def inspect_node(state: CoercionExample):\n        print(f\"number: {state.number} (type: {type(state.number)})\")\n        print(f\"flag: {state.flag} (type: {type(state.flag)})\")\n        return {}\n\n    builder = StateGraph(CoercionExample)\n    builder.add_node(\"inspect\", inspect_node)\n    builder.add_edge(START, \"inspect\")\n    builder.add_edge(\"inspect\", END)\n    graph = builder.compile()\n\n    # Demonstrate coercion with string inputs that will be converted\n    result = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\n\n    # This would fail with a validation error\n    try:\n        graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\n    except Exception as e:\n        print(f\"\\nExpected validation error: {e}\")\n    ```\n\n??? example \"Working with Message Models\"\n\n    When working with LangChain message types in your state schema, there are important considerations for serialization. You should use `AnyMessage` (rather than `BaseMessage`) for proper serialization/deserialization when using message objects over the wire.\n\n    ```python\n    from langgraph.graph import StateGraph, START, END\n    from pydantic import BaseModel\n    from langchain_core.messages import HumanMessage, AIMessage, AnyMessage\n    from typing import List\n\n    class ChatState(BaseModel):\n        messages: List[AnyMessage]\n        context: str\n\n    def add_message(state: ChatState):\n        return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\n\n    builder = StateGraph(ChatState)\n    builder.add_node(\"add_message\", add_message)\n    builder.add_edge(START, \"add_message\")\n    builder.add_edge(\"add_message\", END)\n    graph = builder.compile()\n\n    # Create input with a message\n    initial_state = ChatState(\n        messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\n    )\n\n    result = graph.invoke(initial_state)\n    print(f\"Output: {result}\")\n\n    # Convert back to Pydantic model to see message types\n    output_model = ChatState(**result)\n    for i, msg in enumerate(output_model.messages):\n        print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\n    ```\n\n\n\n\n## Add runtime configuration\n\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, _without polluting the graph state with these parameters_.\n\nTo add runtime configuration:\n\n1. Specify a schema for your configuration\n2. Add the configuration to the function signature for nodes or conditional edges\n3. Pass the configuration into the graph.\n\nSee below for a simple example:\n\n```python hl_lines=\"13 14 16 21 29 30\"\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.runtime import Runtime\nfrom typing_extensions import TypedDict\n\n# 1. Specify config schema\nclass ContextSchema(TypedDict):\n    my_runtime_value: str\n\n# 2. Define a graph that accesses the config in a node\nclass State(TypedDict):\n    my_state_value: str\n\ndef node(state: State, runtime: Runtime[ContextSchema]):\n    if runtime.context[\"my_runtime_value\"] == \"a\":\n        return {\"my_state_value\": 1}\n    elif runtime.context[\"my_runtime_value\"] == \"b\":\n        return {\"my_state_value\": 2}\n    else:\n        raise ValueError(\"Unknown values.\")\n\nbuilder = StateGraph(State, context_schema=ContextSchema)\nbuilder.add_node(node)\nbuilder.add_edge(START, \"node\")\nbuilder.add_edge(\"node\", END)\n\ngraph = builder.compile()\n\n# 3. Pass in configuration at runtime:\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))\n```\n\n```\n{'my_state_value': 1}\n{'my_state_value': 2}\n```\n\n\n\n\n??? example \"Extended example: specifying LLM at runtime\"\n\n    Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\n\n    ```python\n    from dataclasses import dataclass\n\n    from langchain.chat_models import init_chat_model\n    from langgraph.graph import MessagesState, END, StateGraph, START\n    from langgraph.runtime import Runtime\n    from typing_extensions import TypedDict\n\n    @dataclass\n    class ContextSchema:\n        model_provider: str = \"anthropic\"\n\n    MODELS = {\n        \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\n        \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\n    }\n\n    def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\n        model = MODELS[runtime.context.model_provider]\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": [response]}\n\n    builder = StateGraph(MessagesState, context_schema=ContextSchema)\n    builder.add_node(\"model\", call_model)\n    builder.add_edge(START, \"model\")\n    builder.add_edge(\"model\", END)\n\n    graph = builder.compile()\n\n    # Usage\n    input_message = {\"role\": \"user\", \"content\": \"hi\"}\n    # With no configuration, uses default (Anthropic)\n    response_1 = graph.invoke({\"messages\": [input_message]}, context=ContextSchema())[\"messages\"][-1]\n    # Or, can set OpenAI\n    response_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\n\n    print(response_1.response_metadata[\"model_name\"])\n    print(response_2.response_metadata[\"model_name\"])\n    ```\n    ```\n    claude-3-5-haiku-20241022\n    gpt-4.1-mini-2025-04-14\n    ```\n\n\n\n\n??? example \"Extended example: specifying model and system message at runtime\"\n\n    Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\n\n    ```python\n    from dataclasses import dataclass\n    from typing import Optional\n    from langchain.chat_models import init_chat_model\n    from langchain_core.messages import SystemMessage\n    from langgraph.graph import END, MessagesState, StateGraph, START\n    from langgraph.runtime import Runtime\n    from typing_extensions import TypedDict\n\n    @dataclass\n    class ContextSchema:\n        model_provider: str = \"anthropic\"\n        system_message: str | None = None\n\n    MODELS = {\n        \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\n        \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\n    }\n\n    def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\n        model = MODELS[runtime.context.model_provider]\n        messages = state[\"messages\"]\n        if (system_message := runtime.context.system_message):\n            messages = [SystemMessage(system_message)] + messages\n        response = model.invoke(messages)\n        return {\"messages\": [response]}\n\n    builder = StateGraph(MessagesState, context_schema=ContextSchema)\n    builder.add_node(\"model\", call_model)\n    builder.add_edge(START, \"model\")\n    builder.add_edge(\"model\", END)\n\n    graph = builder.compile()\n\n    # Usage\n    input_message = {\"role\": \"user\", \"content\": \"hi\"}\n    response = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\n    for message in response[\"messages\"]:\n        message.pretty_print()\n    ```\n    ```\n    ================================ Human Message ================================\n\n    hi\n    ================================== Ai Message ==================================\n\n    Ciao! Come posso aiutarti oggi?\n    ```\n\n\n\n\n## Add retry policies\n\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\n\nTo configure a retry policy, pass the `retry_policy` parameter to the [add_node](../reference/graphs.md#langgraph.graph.state.StateGraph.add_node). The `retry_policy` parameter takes in a `RetryPolicy` named tuple object. Below we instantiate a `RetryPolicy` object with the default parameters and associate it with a node:\n\n```python\nfrom langgraph.types import RetryPolicy\n\nbuilder.add_node(\n    \"node_name\",\n    node_function,\n    retry_policy=RetryPolicy(),\n)\n```\n\nBy default, the `retry_on` parameter uses the `default_retry_on` function, which retries on any exception except for the following:\n\n- `ValueError`\n- `TypeError`\n- `ArithmeticError`\n- `ImportError`\n- `LookupError`\n- `NameError`\n- `SyntaxError`\n- `RuntimeError`\n- `ReferenceError`\n- `StopIteration`\n- `StopAsyncIteration`\n- `OSError`\n\nIn addition, for exceptions from popular http request libraries such as `requests` and `httpx` it only retries on 5xx status codes.\n\n\n\n\n??? example \"Extended example: customizing retry policies\"\n\n    Consider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\n\n    ```python\n    import sqlite3\n    from typing_extensions import TypedDict\n    from langchain.chat_models import init_chat_model\n    from langgraph.graph import END, MessagesState, StateGraph, START\n    from langgraph.types import RetryPolicy\n    from langchain_community.utilities import SQLDatabase\n    from langchain_core.messages import AIMessage\n\n    db = SQLDatabase.from_uri(\"sqlite:///:memory:\")\n    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n\n    def query_database(state: MessagesState):\n        query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\n        return {\"messages\": [AIMessage(content=query_result)]}\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": [response]}\n\n    # Define a new graph\n    builder = StateGraph(MessagesState)\n    builder.add_node(\n        \"query_database\",\n        query_database,\n        retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\n    )\n    builder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\n    builder.add_edge(START, \"model\")\n    builder.add_edge(\"model\", \"query_database\")\n    builder.add_edge(\"query_database\", END)\n    graph = builder.compile()\n    ```\n\n\n\n\n## Add node caching\n\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\n\nTo configure a cache policy, pass the `cache_policy` parameter to the [add_node](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.state.StateGraph.add_node) function. In the following example, a [`CachePolicy`](https://langchain-ai.github.io/langgraph/reference/types/?h=cachepolicy#langgraph.types.CachePolicy) object is instantiated with a time to live of 120 seconds and the default `key_func` generator. Then it is associated with a node:\n\n```python\nfrom langgraph.types import CachePolicy\n\nbuilder.add_node(\n    \"node_name\",\n    node_function,\n    cache_policy=CachePolicy(ttl=120),\n)\n```\n\nThen, to enable node-level caching for a graph, set the `cache` argument when compiling the graph. The example below uses `InMemoryCache` to set up a graph with in-memory cache, but `SqliteCache` is also available.\n\n```python\nfrom langgraph.cache.memory import InMemoryCache\n\ngraph = builder.compile(cache=InMemoryCache())\n```\n\n\n## Create a sequence of steps\n\n!!! info \"Prerequisites\"\n\n    This guide assumes familiarity with the above section on [state](#define-and-update-state).\n\nHere we demonstrate how to construct a simple sequence of steps. We will show:\n\n1. How to build a sequential graph\n2. Built-in short-hand for constructing similar graphs.\n\nTo add a sequence of nodes, we use the `.add_node` and `.add_edge` methods of our [graph](../concepts/low_level.md#stategraph):\n\n```python\nfrom langgraph.graph import START, StateGraph\n\nbuilder = StateGraph(State)\n\n# Add nodes\nbuilder.add_node(step_1)\nbuilder.add_node(step_2)\nbuilder.add_node(step_3)\n\n# Add edges\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"step_2\")\nbuilder.add_edge(\"step_2\", \"step_3\")\n```\n\nWe can also use the built-in shorthand `.add_sequence`:\n\n```python\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\nbuilder.add_edge(START, \"step_1\")\n```\n\n\n\n\n??? info \"Why split application steps into a sequence with LangGraph?\"\n    LangGraph makes it easy to add an underlying persistence layer to your application.\n    This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\n\n- How state updates are [checkpointed](../concepts/persistence.md)\n- How interruptions are resumed in [human-in-the-loop](../concepts/human_in_the_loop.md) workflows\n- How we can \"rewind\" and branch-off executions using LangGraph's [time travel](../concepts/time-travel.md) features\n\nThey also determine how execution steps are [streamed](../concepts/streaming.md), and how your application is visualized\nand debugged using [LangGraph Studio](../concepts/langgraph_studio.md).\n\nLet's demonstrate an end-to-end example. We will create a sequence of three steps:\n\n1. Populate a value in a key of the state\n2. Update the same value\n3. Populate a different value\n\nLet's first define our [state](../concepts/low_level.md#state). This governs the [schema of the graph](../concepts/low_level.md#schema), and can also specify how to apply updates. See [this section](#process-state-updates-with-reducers) for more detail.\n\nIn our case, we will just keep track of two values:\n\n```python\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    value_1: str\n    value_2: int\n```\n\n\n\n\nOur [nodes](../concepts/low_level.md#nodes) are just Python functions that read our graph's state and make updates to it. The first argument to this function will always be the state:\n\n```python\ndef step_1(state: State):\n    return {\"value_1\": \"a\"}\n\ndef step_2(state: State):\n    current_value_1 = state[\"value_1\"]\n    return {\"value_1\": f\"{current_value_1} b\"}\n\ndef step_3(state: State):\n    return {\"value_2\": 10}\n```\n\n\n\n\n!!! note\n\n    Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\n\n    By default, this will **overwrite** the value of the corresponding key. You can also use [reducers](../concepts/low_level.md#reducers) to control how updates are processed— for example, you can append successive updates to a key instead. See [this section](#process-state-updates-with-reducers) for more detail.\n\nFinally, we define the graph. We use [StateGraph](../concepts/low_level.md#stategraph) to define a graph that operates on this state.\n\nWe will then use [add_node](../concepts/low_level.md#messagesstate) and [add_edge](../concepts/low_level.md#edges) to populate our graph and define its control flow.\n\n```python\nfrom langgraph.graph import START, StateGraph\n\nbuilder = StateGraph(State)\n\n# Add nodes\nbuilder.add_node(step_1)\nbuilder.add_node(step_2)\nbuilder.add_node(step_3)\n\n# Add edges\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"step_2\")\nbuilder.add_edge(\"step_2\", \"step_3\")\n```\n\n\n\n\n!!! tip \"Specifying custom names\"\n\n    You can specify custom names for nodes using `.add_node`:\n\n    ```python\n    builder.add_node(\"my_node\", step_1)\n    ```\n\n\n\n\nNote that:\n\n- `.add_edge` takes the names of nodes, which for functions defaults to `node.__name__`.\n- We must specify the entry point of the graph. For this we add an edge with the [START node](../concepts/low_level.md#start-node).\n- The graph halts when there are no more nodes to execute.\n\nWe next [compile](../concepts/low_level.md#compiling-your-graph) our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a [checkpointer](../concepts/persistence.md), it would also be passed in here.\n\n```python\ngraph = builder.compile()\n```\n\n\n\n\nLangGraph provides built-in utilities for visualizing your graph. Let's inspect our sequence. See [this guide](#visualize-your-graph) for detail on visualization.\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Sequence of steps graph](assets/graph_api_image_2.png)\n\n\n\n\nLet's proceed with a simple invocation:\n\n```python\ngraph.invoke({\"value_1\": \"c\"})\n```\n\n```\n{'value_1': 'a b', 'value_2': 10}\n```\n\n\n\n\nNote that:\n\n- We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\n- The value we passed in was overwritten by the first node.\n- The second node updated the value.\n- The third node populated a different value.\n\n!!! tip \"Built-in shorthand\"\n\n    `langgraph>=0.2.46` includes a built-in short-hand `add_sequence` for adding node sequences. You can compile the same graph as follows:\n\n    ```python hl_lines=\"1\"\n    builder = StateGraph(State).add_sequence([step_1, step_2, step_3])\n    builder.add_edge(START, \"step_1\")\n\n    graph = builder.compile()\n\n    graph.invoke({\"value_1\": \"c\"})\n    ```\n\n\n## Create branches\n\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and [conditional_edges](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.MessageGraph.add_conditional_edges). Below are some examples showing how to add create branching dataflows that work for you.\n\n### Run graph nodes in parallel\n\nIn this example, we fan out from `Node A` to `B and C` and then fan in to `D`. With our state, [we specify the reducer add operation](https://langchain-ai.github.io/langgraph/concepts/low_level.md#reducers). This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on [state reducers](#process-state-updates-with-reducers) for more detail on updating state with reducers.\n\n```python\nimport operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n```\n\n\n\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Parallel execution graph](assets/graph_api_image_3.png)\n\n\n\n\nWith the reducer, you can see that the values added in each node are accumulated.\n\n```python\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n```\n\n```\nAdding \"A\" to []\nAdding \"B\" to ['A']\nAdding \"C\" to ['A']\nAdding \"D\" to ['A', 'B', 'C']\n```\n\n\n\n\n!!! note\n\n    In the above example, nodes `\"b\"` and `\"c\"` are executed concurrently in the same [superstep](../concepts/low_level.md#graphs). Because they are in the same step, node `\"d\"` executes after both `\"b\"` and `\"c\"` are finished.\n\n    Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\n\n??? note \"Exception handling?\"\n\n    LangGraph executes nodes within [supersteps](../concepts/low_level.md#graphs), meaning that while parallel branches are executed in parallel, the entire superstep is **transactional**. If any of these branches raises an exception, **none** of the updates are applied to the state (the entire superstep errors).\n\n    Importantly, when using a [checkpointer](../concepts/persistence.md), results from successful nodes within a superstep are saved, and don't repeat when resumed.\n\n    If you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\n\n    1. You can write regular python code within your node to catch and handle exceptions.\n    2. You can set a **[retry_policy](../reference/types.md#langgraph.types.RetryPolicy)** to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn't worry about performing redundant work.\n\n    Together, these let you perform parallel execution and fully control exception handling.\n\n### Defer node execution\n\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\n\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let's add a node `\"b_2\"` in the `\"b\"` branch:\n\n```python hl_lines=\"35\"\nimport operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef b_2(state: State):\n    print(f'Adding \"B_2\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B_2\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(b_2)\nbuilder.add_node(c)\nbuilder.add_node(d, defer=True)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b_2\")\nbuilder.add_edge(\"b_2\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n```\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Deferred execution graph](assets/graph_api_image_4.png)\n\n```python\ngraph.invoke({\"aggregate\": []})\n```\n\n```\nAdding \"A\" to []\nAdding \"B\" to ['A']\nAdding \"C\" to ['A']\nAdding \"B_2\" to ['A', 'B', 'C']\nAdding \"D\" to ['A', 'B', 'C', 'B_2']\n```\n\nIn the above example, nodes `\"b\"` and `\"c\"` are executed concurrently in the same superstep. We set `defer=True` on node `d` so it will not execute until all pending tasks are finished. In this case, this means that `\"d\"` waits to execute until the entire `\"b\"` branch is finished.\n\n\n### Conditional branching\n\nIf your fan-out should vary at runtime based on the state, you can use [add_conditional_edges](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.StateGraph.add_conditional_edges) to select one or more paths using the graph state. See example below, where node `a` generates a state update that determines the following node.\n\n```python hl_lines=\"14 37\"\nimport operator\nfrom typing import Annotated, Literal, Sequence\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n    # Add a key to the state. We will set this key to determine\n    # how we branch.\n    which: str\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"b\", END)\nbuilder.add_edge(\"c\", END)\n\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\n    # Fill in arbitrary logic here that uses the state\n    # to determine the next node\n    return state[\"which\"]\n\nbuilder.add_conditional_edges(\"a\", conditional_edge)\n\ngraph = builder.compile()\n```\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Conditional branching graph](assets/graph_api_image_5.png)\n\n```python\nresult = graph.invoke({\"aggregate\": []})\nprint(result)\n```\n\n```\nAdding \"A\" to []\nAdding \"C\" to ['A']\n{'aggregate': ['A', 'C'], 'which': 'c'}\n```\n\n\n\n\n!!! tip\n\n    Your conditional edges can route to multiple destination nodes. For example:\n\n    ```python\n    def route_bc_or_cd(state: State) -> Sequence[str]:\n        if state[\"which\"] == \"cd\":\n            return [\"c\", \"d\"]\n        return [\"b\", \"c\"]\n    ```\n\n\n\n\n## Map-Reduce and the Send API\n\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Send\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\nclass OverallState(TypedDict):\n    topic: str\n    subjects: list[str]\n    jokes: Annotated[list[str], operator.add]\n    best_selected_joke: str\n\ndef generate_topics(state: OverallState):\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\n\ndef generate_joke(state: OverallState):\n    joke_map = {\n        \"lions\": \"Why don't lions like fast food? Because they can't catch it!\",\n        \"elephants\": \"Why don't elephants use computers? They're afraid of the mouse!\",\n        \"penguins\": \"Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\n    }\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\n\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n\ndef best_joke(state: OverallState):\n    return {\"best_selected_joke\": \"penguins\"}\n\nbuilder = StateGraph(OverallState)\nbuilder.add_node(\"generate_topics\", generate_topics)\nbuilder.add_node(\"generate_joke\", generate_joke)\nbuilder.add_node(\"best_joke\", best_joke)\nbuilder.add_edge(START, \"generate_topics\")\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\nbuilder.add_edge(\"best_joke\", END)\ngraph = builder.compile()\n```\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Map-reduce graph with fanout](assets/graph_api_image_6.png)\n\n```python\n# Call the graph: here we call it to generate a list of jokes\nfor step in graph.stream({\"topic\": \"animals\"}):\n    print(step)\n```\n\n```\n{'generate_topics': {'subjects': ['lions', 'elephants', 'penguins']}}\n{'generate_joke': {'jokes': [\"Why don't lions like fast food? Because they can't catch it!\"]}}\n{'generate_joke': {'jokes': [\"Why don't elephants use computers? They're afraid of the mouse!\"]}}\n{'generate_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}}\n{'best_joke': {'best_selected_joke': 'penguins'}}\n```\n\n\n\n\n## Create and control loops\n\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](../concepts/low_level.md#conditional-edges) that routes to the [END](../concepts/low_level.md#end-node) node once we reach some termination condition.\n\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](../concepts/low_level.md#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](../concepts/low_level.md#recursion-limit).\n\nLet's consider a simple graph with a loop to better understand how these mechanisms work.\n\n!!! tip\n\n    To return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).\n\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\n\n```python\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\ndef route(state: State) -> Literal[\"b\", END]:\n    if termination_condition(state):\n        return END\n    else:\n        return \"b\"\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n```\n\n\n\n\nTo control the recursion limit, specify `\"recursionLimit\"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:\n\n```python\nfrom langgraph.errors import GraphRecursionError\n\ntry:\n    graph.invoke(inputs, {\"recursion_limit\": 3})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n```\n\n\n\n\nLet's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\n\n```python\nimport operator\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Node A sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Node B sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n# Define nodes\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\n# Define edges\ndef route(state: State) -> Literal[\"b\", END]:\n    if len(state[\"aggregate\"]) < 7:\n        return \"b\"\n    else:\n        return END\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n```\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Simple loop graph](assets/graph_api_image_7.png)\n\n\n\n\nThis architecture is similar to a [React agent](../agents/overview.md) in which node `\"a\"` is a tool-calling model, and node `\"b\"` represents the tools.\n\nIn our `route` conditional edge, we specify that we should end after the `\"aggregate\"` list in the state passes a threshold length.\n\nInvoking the graph, we see that we alternate between nodes `\"a\"` and `\"b\"` before terminating once we reach the termination condition.\n\n```python\ngraph.invoke({\"aggregate\": []})\n```\n\n```\nNode A sees []\nNode B sees ['A']\nNode A sees ['A', 'B']\nNode B sees ['A', 'B', 'A']\nNode A sees ['A', 'B', 'A', 'B']\nNode B sees ['A', 'B', 'A', 'B', 'A']\nNode A sees ['A', 'B', 'A', 'B', 'A', 'B']\n```\n\n\n\n\n### Impose a recursion limit\n\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](../concepts/low_level.md#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](../concepts/low_level.md#graphs). We can then catch and handle this exception:\n\n```python\nfrom langgraph.errors import GraphRecursionError\n\ntry:\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n```\n\n```\nNode A sees []\nNode B sees ['A']\nNode C sees ['A', 'B']\nNode D sees ['A', 'B']\nNode A sees ['A', 'B', 'C', 'D']\nRecursion Error\n```\n\n\n\n\n\n??? example \"Extended example: return state on hitting recursion limit\"\n\n    Instead of raising `GraphRecursionError`, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\n\n    LangGraph implements a special `RemainingSteps` annotation. Under the hood, it creates a `ManagedValue` channel -- a state channel that will exist for the duration of our graph run and no longer.\n\n    ```python\n    import operator\n    from typing import Annotated, Literal\n    from typing_extensions import TypedDict\n    from langgraph.graph import StateGraph, START, END\n    from langgraph.managed.is_last_step import RemainingSteps\n\n    class State(TypedDict):\n        aggregate: Annotated[list, operator.add]\n        remaining_steps: RemainingSteps\n\n    def a(state: State):\n        print(f'Node A sees {state[\"aggregate\"]}')\n        return {\"aggregate\": [\"A\"]}\n\n    def b(state: State):\n        print(f'Node B sees {state[\"aggregate\"]}')\n        return {\"aggregate\": [\"B\"]}\n\n    # Define nodes\n    builder = StateGraph(State)\n    builder.add_node(a)\n    builder.add_node(b)\n\n    # Define edges\n    def route(state: State) -> Literal[\"b\", END]:\n        if state[\"remaining_steps\"] <= 2:\n            return END\n        else:\n            return \"b\"\n\n    builder.add_edge(START, \"a\")\n    builder.add_conditional_edges(\"a\", route)\n    builder.add_edge(\"b\", \"a\")\n    graph = builder.compile()\n\n    # Test it out\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\n    print(result)\n    ```\n    ```\n    Node A sees []\n    Node B sees ['A']\n    Node A sees ['A', 'B']\n    {'aggregate': ['A', 'B', 'A']}\n    ```\n\n\n??? example \"Extended example: loops with branches\"\n\n    To better understand how the recursion limit works, let's consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\n\n    ```python\n    import operator\n    from typing import Annotated, Literal\n    from typing_extensions import TypedDict\n    from langgraph.graph import StateGraph, START, END\n\n    class State(TypedDict):\n        aggregate: Annotated[list, operator.add]\n\n    def a(state: State):\n        print(f'Node A sees {state[\"aggregate\"]}')\n        return {\"aggregate\": [\"A\"]}\n\n    def b(state: State):\n        print(f'Node B sees {state[\"aggregate\"]}')\n        return {\"aggregate\": [\"B\"]}\n\n    def c(state: State):\n        print(f'Node C sees {state[\"aggregate\"]}')\n        return {\"aggregate\": [\"C\"]}\n\n    def d(state: State):\n        print(f'Node D sees {state[\"aggregate\"]}')\n        return {\"aggregate\": [\"D\"]}\n\n    # Define nodes\n    builder = StateGraph(State)\n    builder.add_node(a)\n    builder.add_node(b)\n    builder.add_node(c)\n    builder.add_node(d)\n\n    # Define edges\n    def route(state: State) -> Literal[\"b\", END]:\n        if len(state[\"aggregate\"]) < 7:\n            return \"b\"\n        else:\n            return END\n\n    builder.add_edge(START, \"a\")\n    builder.add_conditional_edges(\"a\", route)\n    builder.add_edge(\"b\", \"c\")\n    builder.add_edge(\"b\", \"d\")\n    builder.add_edge([\"c\", \"d\"], \"a\")\n    graph = builder.compile()\n    ```\n\n    ```python\n    from IPython.display import Image, display\n\n    display(Image(graph.get_graph().draw_mermaid_png()))\n    ```\n\n    ![Complex loop graph with branches](assets/graph_api_image_8.png)\n\n    This graph looks complex, but can be conceptualized as loop of [supersteps](../concepts/low_level.md#graphs):\n\n    1. Node A\n    2. Node B\n    3. Nodes C and D\n    4. Node A\n    5. ...\n\n    We have a loop of four supersteps, where nodes C and D are executed concurrently.\n\n    Invoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\n\n    ```python\n    result = graph.invoke({\"aggregate\": []})\n    ```\n    ```\n    Node A sees []\n    Node B sees ['A']\n    Node D sees ['A', 'B']\n    Node C sees ['A', 'B']\n    Node A sees ['A', 'B', 'C', 'D']\n    Node B sees ['A', 'B', 'C', 'D', 'A']\n    Node D sees ['A', 'B', 'C', 'D', 'A', 'B']\n    Node C sees ['A', 'B', 'C', 'D', 'A', 'B']\n    Node A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D']\n    ```\n\n    However, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\n\n    ```python\n    from langgraph.errors import GraphRecursionError\n\n    try:\n        result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\n    except GraphRecursionError:\n        print(\"Recursion Error\")\n    ```\n    ```\n    Node A sees []\n    Node B sees ['A']\n    Node C sees ['A', 'B']\n    Node D sees ['A', 'B']\n    Node A sees ['A', 'B', 'C', 'D']\n    Recursion Error\n    ```\n\n\n## Async\n\nUsing the async programming paradigm can produce significant performance improvements when running [IO-bound](https://en.wikipedia.org/wiki/I/O_bound) code concurrently (e.g., making concurrent API requests to a chat model provider).\n\nTo convert a `sync` implementation of the graph to an `async` implementation, you will need to:\n\n1. Update `nodes` use `async def` instead of `def`.\n2. Update the code inside to use `await` appropriately.\n3. Invoke the graph with `.ainvoke` or `.astream` as desired.\n\nBecause many LangChain objects implement the [Runnable Protocol](https://python.langchain.com/docs/expression_language/interface/) which has `async` variants of all the `sync` methods it's typically fairly quick to upgrade a `sync` graph to an `async` graph.\n\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\n\n{% include-markdown \"../../snippets/chat_model_tabs.md\" %}\n\n```python hl_lines=\"4 5 12\"\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import MessagesState, StateGraph\n\nasync def node(state: MessagesState): # (1)!\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\n    return {\"messages\": [new_message]}\n\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\ngraph = builder.compile()\n\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\n```\n\n1. Declare nodes to be async functions.\n2. Use async invocations when available within the node.\n3. Use async invocations on the graph object itself.\n\n!!! tip \"Async streaming\"\n\n    See the [streaming guide](./streaming.md) for examples of streaming with async.\n\n\n\n## Combine control flow and state updates with `Command`\n\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [Command](../reference/types.md#langgraph.types.Command) object from node functions:\n\n```python\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n```\n\n\n\n\nWe show an end-to-end example below. Let's create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\n\n```python\nimport random\nfrom typing_extensions import TypedDict, Literal\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.types import Command\n\n# Define graph state\nclass State(TypedDict):\n    foo: str\n\n# Define the nodes\n\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\n    print(\"Called A\")\n    value = random.choice([\"b\", \"c\"])\n    # this is a replacement for a conditional edge function\n    if value == \"b\":\n        goto = \"node_b\"\n    else:\n        goto = \"node_c\"\n\n    # note how Command allows you to BOTH update the graph state AND route to the next node\n    return Command(\n        # this is the state update\n        update={\"foo\": value},\n        # this is a replacement for an edge\n        goto=goto,\n    )\n\ndef node_b(state: State):\n    print(\"Called B\")\n    return {\"foo\": state[\"foo\"] + \"b\"}\n\ndef node_c(state: State):\n    print(\"Called C\")\n    return {\"foo\": state[\"foo\"] + \"c\"}\n```\n\nWe can now create the `StateGraph` with the above nodes. Notice that the graph doesn't have [conditional edges](../concepts/low_level.md#conditional-edges) for routing! This is because control flow is defined with `Command` inside `node_a`.\n\n```python\nbuilder = StateGraph(State)\nbuilder.add_edge(START, \"node_a\")\nbuilder.add_node(node_a)\nbuilder.add_node(node_b)\nbuilder.add_node(node_c)\n# NOTE: there are no edges between nodes A, B and C!\n\ngraph = builder.compile()\n```\n\n!!! important\n\n    You might have noticed that we used `Command` as a return type annotation, e.g. `Command[Literal[\"node_b\", \"node_c\"]]`. This is necessary for the graph rendering and tells LangGraph that `node_a` can navigate to `node_b` and `node_c`.\n\n```python\nfrom IPython.display import display, Image\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n![Command-based graph navigation](assets/graph_api_image_11.png)\n\nIf we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A.\n\n```python\ngraph.invoke({\"foo\": \"\"})\n```\n\n```\nCalled A\nCalled C\n```\n\n\n\n\n### Navigate to a node in a parent graph\n\nIf you are using [subgraphs](../concepts/subgraphs.md), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:\n\n```python\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n```\n\n\n\n\nLet's demonstrate this using the above example. We'll do so by changing `nodeA` in the above example into a single-node graph that we'll add as a subgraph to our parent graph.\n\n!!! important \"State updates with `Command.PARENT`\"\n\n    When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](../concepts/low_level.md#schema), you **must** define a [reducer](../concepts/low_level.md#reducers) for the key you're updating in the parent graph state. See the example below.\n\n```python hl_lines=\"6 23 33 37\"\nimport operator\nfrom typing_extensions import Annotated\n\nclass State(TypedDict):\n    # NOTE: we define a reducer here\n    foo: Annotated[str, operator.add]\n\ndef node_a(state: State):\n    print(\"Called A\")\n    value = random.choice([\"a\", \"b\"])\n    # this is a replacement for a conditional edge function\n    if value == \"a\":\n        goto = \"node_b\"\n    else:\n        goto = \"node_c\"\n\n    # note how Command allows you to BOTH update the graph state AND route to the next node\n    return Command(\n        update={\"foo\": value},\n        goto=goto,\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\n        graph=Command.PARENT,\n    )\n\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\n\ndef node_b(state: State):\n    print(\"Called B\")\n    # NOTE: since we've defined a reducer, we don't need to manually append\n    # new characters to existing 'foo' value. instead, reducer will append these\n    # automatically (via operator.add)\n    return {\"foo\": \"b\"}\n\ndef node_c(state: State):\n    print(\"Called C\")\n    return {\"foo\": \"c\"}\n\nbuilder = StateGraph(State)\nbuilder.add_edge(START, \"subgraph\")\nbuilder.add_node(\"subgraph\", subgraph)\nbuilder.add_node(node_b)\nbuilder.add_node(node_c)\n\ngraph = builder.compile()\n```\n\n```python\ngraph.invoke({\"foo\": \"\"})\n```\n\n```\nCalled A\nCalled C\n```\n\n\n\n\n### Use inside tools\n\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return `Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]})` from the tool:\n\n```python\n@tool\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\n    return Command(\n        update={\n            # update the state keys\n            \"user_info\": user_info,\n            # update the message history\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\n        }\n    )\n```\n\n\n\n\n!!! important\n\n    You MUST include `messages` (or any state key used for the message history) in `Command.update` when returning `Command` from a tool and the list of messages in `messages` MUST contain a `ToolMessage`. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\n\nIf you are using tools that update state via `Command`, we recommend using prebuilt [`ToolNode`](../reference/agents.md#langgraph.prebuilt.tool_node.ToolNode) which automatically handles tools returning `Command` objects and propagates them to the graph state. If you're writing a custom node that calls tools, you would need to manually propagate `Command` objects returned by the tools as the update from the node.\n\n## Visualize your graph\n\nHere we demonstrate how to visualize the graphs you create.\n\nYou can visualize any arbitrary [Graph](https://langchain-ai.github.io/langgraph/reference/graphs/), including [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.state.StateGraph). \n\nLet's have some fun by drawing fractals :).\n\n```python\nimport random\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\nclass MyNode:\n    def __init__(self, name: str):\n        self.name = name\n    def __call__(self, state: State):\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\n\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\n    if len(state[\"messages\"]) > 10:\n        return \"__end__\"\n    return \"entry_node\"\n\ndef add_fractal_nodes(builder, current_node, level, max_level):\n    if level > max_level:\n        return\n    # Number of nodes to create at this level\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\n    for i in range(num_nodes):\n        nm = [\"A\", \"B\", \"C\"][i]\n        node_name = f\"node_{current_node}_{nm}\"\n        builder.add_node(node_name, MyNode(node_name))\n        builder.add_edge(current_node, node_name)\n        # Recursively add more nodes\n        r = random.random()\n        if r > 0.2 and level + 1 < max_level:\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\n        elif r > 0.05:\n            builder.add_conditional_edges(node_name, route, node_name)\n        else:\n            # End\n            builder.add_edge(node_name, \"__end__\")\n\ndef build_fractal_graph(max_level: int):\n    builder = StateGraph(State)\n    entry_point = \"entry_node\"\n    builder.add_node(entry_point, MyNode(entry_point))\n    builder.add_edge(START, entry_point)\n    add_fractal_nodes(builder, entry_point, 1, max_level)\n    # Optional: set a finish point if required\n    builder.add_edge(entry_point, END)  # or any specific node\n    return builder.compile()\n\napp = build_fractal_graph(3)\n```\n\n\n\n\n### Mermaid\n\nWe can also convert a graph class into Mermaid syntax.\n\n```python\nprint(app.get_graph().draw_mermaid())\n```\n\n```\n%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n\t__start__([<p>__start__</p>]):::first\n\tentry_node(entry_node)\n\tnode_entry_node_A(node_entry_node_A)\n\tnode_entry_node_B(node_entry_node_B)\n\tnode_node_entry_node_B_A(node_node_entry_node_B_A)\n\tnode_node_entry_node_B_B(node_node_entry_node_B_B)\n\tnode_node_entry_node_B_C(node_node_entry_node_B_C)\n\t__end__([<p>__end__</p>]):::last\n\t__start__ --> entry_node;\n\tentry_node --> __end__;\n\tentry_node --> node_entry_node_A;\n\tentry_node --> node_entry_node_B;\n\tnode_entry_node_B --> node_node_entry_node_B_A;\n\tnode_entry_node_B --> node_node_entry_node_B_B;\n\tnode_entry_node_B --> node_node_entry_node_B_C;\n\tnode_entry_node_A -.-> entry_node;\n\tnode_entry_node_A -.-> __end__;\n\tnode_node_entry_node_B_A -.-> entry_node;\n\tnode_node_entry_node_B_A -.-> __end__;\n\tnode_node_entry_node_B_B -.-> entry_node;\n\tnode_node_entry_node_B_B -.-> __end__;\n\tnode_node_entry_node_B_C -.-> entry_node;\n\tnode_node_entry_node_B_C -.-> __end__;\n\tclassDef default fill:#f2f0ff,line-height:1.2\n\tclassDef first fill-opacity:0\n\tclassDef last fill:#bfb6fc\n```\n\n\n\n\n### PNG\n\nIf preferred, we could render the Graph into a `.png`. Here we could use three options:\n\n- Using Mermaid.ink API (does not require additional packages)\n- Using Mermaid + Pyppeteer (requires `pip install pyppeteer`)\n- Using graphviz (which requires `pip install graphviz`)\n\n**Using Mermaid.Ink**\n\nBy default, `draw_mermaid_png()` uses Mermaid.Ink's API to generate the diagram.\n\n```python\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\ndisplay(Image(app.get_graph().draw_mermaid_png()))\n```\n\n![Fractal graph visualization](assets/graph_api_image_10.png)\n\n**Using Mermaid + Pyppeteer**\n\n```python\nimport nest_asyncio\n\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\n\ndisplay(\n    Image(\n        app.get_graph().draw_mermaid_png(\n            curve_style=CurveStyle.LINEAR,\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n            wrap_label_n_words=9,\n            output_file_path=None,\n            draw_method=MermaidDrawMethod.PYPPETEER,\n            background_color=\"white\",\n            padding=10,\n        )\n    )\n)\n```\n\n**Using Graphviz**\n\n```python\ntry:\n    display(Image(app.get_graph().draw_png()))\nexcept ImportError:\n    print(\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\n    )\n```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 66839,
    "word_count": 7648
  },
  {
    "title": "Stream outputs",
    "source": "how-tos/streaming.md",
    "content": "# Stream outputs\n\nYou can [stream outputs](../concepts/streaming.md) from a LangGraph agent or workflow.\n\n## Supported stream modes\n\nPass one or more of the following stream modes as a list to the [`stream()`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream()`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods:\n\n\n\n\n| Mode       | Description                                                                                                                                                                         |\n| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `values`   | Streams the full value of the state after each step of the graph.                                                                                                                   |\n| `updates`  | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |\n| `custom`   | Streams custom data from inside your graph nodes.                                                                                                                                   |\n| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.                                                                                                |\n| `debug`    | Streams as much information as possible throughout the execution of the graph.                                                                                                      |\n\n## Stream from an agent\n\n### Agent progress\n\nTo stream agent progress, use the [`stream()`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream()`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods with `stream_mode=\"updates\"`. This emits an event after every agent step.\n\n\n\n\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n\n- **LLM node**: AI message with tool call requests\n- **Tool node**: Tool message with execution result\n- **LLM node**: Final AI response\n\n=== \"Sync\"\n\n    ```python hl_lines=\"5 7\"\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n    for chunk in agent.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=\"updates\"\n    ):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n=== \"Async\"\n\n    ```python hl_lines=\"5 7\"\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n    async for chunk in agent.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=\"updates\"\n    ):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n\n\n\n\n### LLM tokens\n\nTo stream tokens as they are produced by the LLM, use `stream_mode=\"messages\"`:\n\n=== \"Sync\"\n\n    ```python hl_lines=\"5 7\"\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n    for token, metadata in agent.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=\"messages\"\n    ):\n        print(\"Token\", token)\n        print(\"Metadata\", metadata)\n        print(\"\\n\")\n    ```\n\n=== \"Async\"\n\n    ```python hl_lines=\"5 7\"\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n    async for token, metadata in agent.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=\"messages\"\n    ):\n        print(\"Token\", token)\n        print(\"Metadata\", metadata)\n        print(\"\\n\")\n    ```\n\n\n\n\n\n### Tool updates\n\nTo stream updates from tools as they are executed, you can use [get_stream_writer](https://langchain-ai.github.io/langgraph/reference/config/#langgraph.config.get_stream_writer).\n\n=== \"Sync\"\n\n    ```python hl_lines=\"1 5 7 17\"\n    from langgraph.config import get_stream_writer\n\n    def get_weather(city: str) -> str:\n        \"\"\"Get weather for a given city.\"\"\"\n        writer = get_stream_writer()\n        # stream any arbitrary data\n        writer(f\"Looking up data for city: {city}\")\n        return f\"It's always sunny in {city}!\"\n\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n\n    for chunk in agent.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=\"custom\"\n    ):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n=== \"Async\"\n\n    ```python hl_lines=\"1 5 7 17\"\n    from langgraph.config import get_stream_writer\n\n    def get_weather(city: str) -> str:\n        \"\"\"Get weather for a given city.\"\"\"\n        writer = get_stream_writer()\n        # stream any arbitrary data\n        writer(f\"Looking up data for city: {city}\")\n        return f\"It's always sunny in {city}!\"\n\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n\n    async for chunk in agent.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=\"custom\"\n    ):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n!!! Note\n\n      If you add `get_stream_writer` inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.\n\n\n\n\n\n### Stream multiple modes\n\nYou can specify multiple streaming modes by passing stream mode as a list: `stream_mode=[\"updates\", \"messages\", \"custom\"]`:\n\n=== \"Sync\"\n\n    ```python hl_lines=\"8\"\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n\n    for stream_mode, chunk in agent.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=[\"updates\", \"messages\", \"custom\"]\n    ):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n=== \"Async\"\n\n    ```python hl_lines=\"8\"\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_weather],\n    )\n\n    async for stream_mode, chunk in agent.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        stream_mode=[\"updates\", \"messages\", \"custom\"]\n    ):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n\n\n\n\n### Disable streaming\n\nIn some applications you might need to disable streaming of individual tokens for a given model. This is useful in [multi-agent](../agents/multi-agent.md) systems to control which agents stream their output.\n\nSee the [Models](../agents/models.md#disable-streaming) guide to learn how to disable streaming.\n\n## Stream from a workflow\n\n### Basic usage example\n\nLangGraph graphs expose the [`.stream()`](https://langchain-ai.github.io/langgraph/reference/pregel/#langgraph.pregel.Pregel.stream) (sync) and [`.astream()`](https://langchain-ai.github.io/langgraph/reference/pregel/#langgraph.pregel.Pregel.astream) (async) methods to yield streamed outputs as iterators.\n\n=== \"Sync\"\n\n    ```python\n    for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n        print(chunk)\n    ```\n\n=== \"Async\"\n\n    ```python\n    async for chunk in graph.astream(inputs, stream_mode=\"updates\"):\n        print(chunk)\n    ```\n\n\n\n\n\n??? example \"Extended example: streaming updates\"\n\n      ```python hl_lines=\"24 26\"\n      from typing import TypedDict\n      from langgraph.graph import StateGraph, START, END\n\n      class State(TypedDict):\n          topic: str\n          joke: str\n\n      def refine_topic(state: State):\n          return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n      def generate_joke(state: State):\n          return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\n      graph = (\n          StateGraph(State)\n          .add_node(refine_topic)\n          .add_node(generate_joke)\n          .add_edge(START, \"refine_topic\")\n          .add_edge(\"refine_topic\", \"generate_joke\")\n          .add_edge(\"generate_joke\", END)\n          .compile()\n      )\n\n      for chunk in graph.stream( # (1)!\n          {\"topic\": \"ice cream\"},\n          stream_mode=\"updates\", # (2)!\n      ):\n          print(chunk)\n      ```\n\n      1. The `stream()` method returns an iterator that yields streamed outputs.\n      2. Set `stream_mode=\"updates\"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.\n\n\n\n\n      ```output\n      {'refineTopic': {'topic': 'ice cream and cats'}}\n      {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}\n      ```                                                                                                   |\n\n### Stream multiple modes\n\nYou can pass a list as the `stream_mode` parameter to stream multiple modes at once.\n\nThe streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.\n\n=== \"Sync\"\n\n    ```python\n    for mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n        print(chunk)\n    ```\n\n=== \"Async\"\n\n    ```python\n    async for mode, chunk in graph.astream(inputs, stream_mode=[\"updates\", \"custom\"]):\n        print(chunk)\n    ```\n\n\n\n\n\n### Stream graph state\n\nUse the stream modes `updates` and `values` to stream the state of the graph as it executes.\n\n- `updates` streams the **updates** to the state after each step of the graph.\n- `values` streams the **full value** of the state after each step of the graph.\n\n```python\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n  topic: str\n  joke: str\n\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n  StateGraph(State)\n  .add_node(refine_topic)\n  .add_node(generate_joke)\n  .add_edge(START, \"refine_topic\")\n  .add_edge(\"refine_topic\", \"generate_joke\")\n  .add_edge(\"generate_joke\", END)\n  .compile()\n)\n```\n\n\n\n\n\n=== \"updates\"\n\n    Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.\n\n    ```python hl_lines=\"3\"\n    for chunk in graph.stream(\n        {\"topic\": \"ice cream\"},\n        stream_mode=\"updates\",\n    ):\n        print(chunk)\n    ```\n\n\n\n\n=== \"values\"\n\n    Use this to stream the **full state** of the graph after each step.\n\n    ```python hl_lines=\"3\"\n    for chunk in graph.stream(\n        {\"topic\": \"ice cream\"},\n        stream_mode=\"values\",\n    ):\n        print(chunk)\n    ```\n\n\n\n\n### Stream subgraph outputs\n\nTo include outputs from [subgraphs](../concepts/subgraphs.md) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n\nThe outputs will be streamed as tuples `(namespace, data)`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `(\"parent_node:<task_id>\", \"child_node:<task_id>\")`.\n\n```python hl_lines=\"3\"\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    subgraphs=True, # (1)!\n    stream_mode=\"updates\",\n):\n    print(chunk)\n```\n\n1. Set `subgraphs=True` to stream outputs from subgraphs.\n\n\n\n\n??? example \"Extended example: streaming from subgraphs\"\n\n      ```python hl_lines=\"39\"\n      from langgraph.graph import START, StateGraph\n      from typing import TypedDict\n\n      # Define subgraph\n      class SubgraphState(TypedDict):\n          foo: str  # note that this key is shared with the parent graph state\n          bar: str\n\n      def subgraph_node_1(state: SubgraphState):\n          return {\"bar\": \"bar\"}\n\n      def subgraph_node_2(state: SubgraphState):\n          return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\n      subgraph_builder = StateGraph(SubgraphState)\n      subgraph_builder.add_node(subgraph_node_1)\n      subgraph_builder.add_node(subgraph_node_2)\n      subgraph_builder.add_edge(START, \"subgraph_node_1\")\n      subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n      subgraph = subgraph_builder.compile()\n\n      # Define parent graph\n      class ParentState(TypedDict):\n          foo: str\n\n      def node_1(state: ParentState):\n          return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n      builder = StateGraph(ParentState)\n      builder.add_node(\"node_1\", node_1)\n      builder.add_node(\"node_2\", subgraph)\n      builder.add_edge(START, \"node_1\")\n      builder.add_edge(\"node_1\", \"node_2\")\n      graph = builder.compile()\n\n      for chunk in graph.stream(\n          {\"foo\": \"foo\"},\n          stream_mode=\"updates\",\n          subgraphs=True, # (1)!\n      ):\n          print(chunk)\n      ```\n\n      1. Set `subgraphs=True` to stream outputs from subgraphs.\n\n\n\n\n      ```\n      ((), {'node_1': {'foo': 'hi! foo'}})\n      (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n      (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n      ((), {'node_2': {'foo': 'hi! foobar'}})\n      ```\n\n\n\n\n      **Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n\n### Debugging {#debug}\n\nUse the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\n\n```python hl_lines=\"3\"\nfor chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"debug\",\n):\n    print(chunk)\n```\n\n\n\n\n\n### LLM tokens {#messages}\n\nUse the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.\n\nThe streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:\n\n- `message_chunk`: the token or message segment from the LLM.\n- `metadata`: a dictionary containing details about the graph node and LLM invocation.\n\n> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.\n\n!!! warning \"Manual config required for async in Python < 3.11\"\n\n    When using Python < 3.11 with async code, you must explicitly pass `RunnableConfig` to `ainvoke()` to enable proper streaming. See [Async with Python < 3.11](#async) for details or upgrade to Python 3.11+.\n\n```python hl_lines=\"17 33\"\nfrom dataclasses import dataclass\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START\n\n\n@dataclass\nclass MyState:\n    topic: str\n    joke: str = \"\"\n\n\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\n\ndef call_model(state: MyState):\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n    llm_response = llm.invoke( # (1)!\n        [\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n        ]\n    )\n    return {\"joke\": llm_response.content}\n\ngraph = (\n    StateGraph(MyState)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\nfor message_chunk, metadata in graph.stream( # (2)!\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"messages\",\n):\n    if message_chunk.content:\n        print(message_chunk.content, end=\"|\", flush=True)\n```\n\n1. Note that the message events are emitted even when the LLM is run using `.invoke` rather than `.stream`.\n2. The \"messages\" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.\n\n\n\n\n#### Filter by LLM invocation\n\nYou can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.\n\n```python hl_lines=\"10\"\nfrom langchain.chat_models import init_chat_model\n\nllm_1 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['joke']) # (1)!\nllm_2 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['poem']) # (2)!\n\ngraph = ... # define a graph that uses these LLMs\n\nasync for msg, metadata in graph.astream(  # (3)!\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",\n):\n    if metadata[\"tags\"] == [\"joke\"]: # (4)!\n        print(msg.content, end=\"|\", flush=True)\n```\n\n1. llm_1 is tagged with \"joke\".\n2. llm_2 is tagged with \"poem\".\n3. The `stream_mode` is set to \"messages\" to stream LLM tokens. The `metadata` contains information about the LLM invocation, including the tags.\n4. Filter the streamed tokens by the `tags` field in the metadata to only include the tokens from the LLM invocation with the \"joke\" tag.\n\n\n\n\n??? example \"Extended example: filtering by tags\"\n\n      ```python hl_lines=\"42\"\n      from typing import TypedDict\n\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import START, StateGraph\n\n      joke_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"joke\"]) # (1)!\n      poem_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"poem\"]) # (2)!\n\n\n      class State(TypedDict):\n            topic: str\n            joke: str\n            poem: str\n\n\n      async def call_model(state, config):\n            topic = state[\"topic\"]\n            print(\"Writing joke...\")\n            # Note: Passing the config through explicitly is required for python < 3.11\n            # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n            joke_response = await joke_model.ainvoke(\n                  [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n                  config, # (3)!\n            )\n            print(\"\\n\\nWriting poem...\")\n            poem_response = await poem_model.ainvoke(\n                  [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n                  config, # (3)!\n            )\n            return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n\n\n      graph = (\n            StateGraph(State)\n            .add_node(call_model)\n            .add_edge(START, \"call_model\")\n            .compile()\n      )\n\n      async for msg, metadata in graph.astream(\n            {\"topic\": \"cats\"},\n            stream_mode=\"messages\", # (4)!\n      ):\n          if metadata[\"tags\"] == [\"joke\"]: # (4)!\n              print(msg.content, end=\"|\", flush=True)\n      ```\n\n      1. The `joke_model` is tagged with \"joke\".\n      2. The `poem_model` is tagged with \"poem\".\n      3. The `config` is passed through explicitly to ensure the context vars are propagated correctly. This is required for Python < 3.11 when using async code. Please see the [async section](#async) for more details.\n      4. The `stream_mode` is set to \"messages\" to stream LLM tokens. The `metadata` contains information about the LLM invocation, including the tags.\n\n\n\n\n#### Filter by node\n\nTo stream tokens only from specific nodes, use `stream_mode=\"messages\"` and filter the outputs by the `langgraph_node` field in the streamed metadata:\n\n```python hl_lines=\"3 5\"\nfor msg, metadata in graph.stream( # (1)!\n    inputs,\n    stream_mode=\"messages\",\n):\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\": # (2)!\n        ...\n```\n\n1. The \"messages\" stream mode returns a tuple of `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.\n2. Filter the streamed tokens by the `langgraph_node` field in the metadata to only include the tokens from the `write_poem` node.\n\n\n\n\n??? example \"Extended example: streaming LLM tokens from specific nodes\"\n\n      ```python hl_lines=\"40 44\"\n      from typing import TypedDict\n      from langgraph.graph import START, StateGraph\n      from langchain_openai import ChatOpenAI\n\n      model = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\n      class State(TypedDict):\n            topic: str\n            joke: str\n            poem: str\n\n\n      def write_joke(state: State):\n            topic = state[\"topic\"]\n            joke_response = model.invoke(\n                  [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n            )\n            return {\"joke\": joke_response.content}\n\n\n      def write_poem(state: State):\n            topic = state[\"topic\"]\n            poem_response = model.invoke(\n                  [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n            )\n            return {\"poem\": poem_response.content}\n\n\n      graph = (\n            StateGraph(State)\n            .add_node(write_joke)\n            .add_node(write_poem)\n            # write both the joke and the poem concurrently\n            .add_edge(START, \"write_joke\")\n            .add_edge(START, \"write_poem\")\n            .compile()\n      )\n\n      for msg, metadata in graph.stream( # (1)!\n          {\"topic\": \"cats\"},\n          stream_mode=\"messages\",\n      ):\n          if msg.content and metadata[\"langgraph_node\"] == \"write_poem\": # (2)!\n              print(msg.content, end=\"|\", flush=True)\n      ```\n\n      1. The \"messages\" stream mode returns a tuple of `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.\n      2. Filter the streamed tokens by the `langgraph_node` field in the metadata to only include the tokens from the `write_poem` node.\n\n\n\n\n### Stream custom data\n\nTo send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:\n\n1. Use `get_stream_writer()` to access the stream writer and emit custom data.\n2. Set `stream_mode=\"custom\"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `[\"updates\", \"custom\"]`), but at least one must be `\"custom\"`.\n\n!!! warning \"No `get_stream_writer()` in async for Python < 3.11\"\n\n    In async code running on Python < 3.11, `get_stream_writer()` will not work.\n    Instead, add a `writer` parameter to your node or tool and pass it manually.\n    See [Async with Python < 3.11](#async) for usage examples.\n\n=== \"node\"\n\n      ```python\n      from typing import TypedDict\n      from langgraph.config import get_stream_writer\n      from langgraph.graph import StateGraph, START\n\n      class State(TypedDict):\n          query: str\n          answer: str\n\n      def node(state: State):\n          writer = get_stream_writer()  # (1)!\n          writer({\"custom_key\": \"Generating custom data inside node\"}) # (2)!\n          return {\"answer\": \"some data\"}\n\n      graph = (\n          StateGraph(State)\n          .add_node(node)\n          .add_edge(START, \"node\")\n          .compile()\n      )\n\n      inputs = {\"query\": \"example\"}\n\n      # Usage\n      for chunk in graph.stream(inputs, stream_mode=\"custom\"):  # (3)!\n          print(chunk)\n      ```\n\n      1. Get the stream writer to send custom data.\n      2. Emit a custom key-value pair (e.g., progress update).\n      3. Set `stream_mode=\"custom\"` to receive the custom data in the stream.\n\n=== \"tool\"\n\n      ```python hl_lines=\"8 10\"\n      from langchain_core.tools import tool\n      from langgraph.config import get_stream_writer\n\n      @tool\n      def query_database(query: str) -> str:\n          \"\"\"Query the database.\"\"\"\n          writer = get_stream_writer() # (1)!\n          writer({\"data\": \"Retrieved 0/100 records\", \"type\": \"progress\"}) # (2)!\n          # perform query\n          writer({\"data\": \"Retrieved 100/100 records\", \"type\": \"progress\"}) # (3)!\n          return \"some-answer\"\n\n\n      graph = ... # define a graph that uses this tool\n\n      for chunk in graph.stream(inputs, stream_mode=\"custom\"): # (4)!\n          print(chunk)\n      ```\n\n      1. Access the stream writer to send custom data.\n      2. Emit a custom key-value pair (e.g., progress update).\n      3. Emit another custom key-value pair.\n      4. Set `stream_mode=\"custom\"` to receive the custom data in the stream.\n\n\n\n\n\n### Use with any LLM\n\nYou can use `stream_mode=\"custom\"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.\n\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\n\n```python hl_lines=\"5 8 20\"\nfrom langgraph.config import get_stream_writer\n\ndef call_arbitrary_model(state):\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n    writer = get_stream_writer() # (1)!\n    # Assume you have a streaming client that yields chunks\n    for chunk in your_custom_streaming_client(state[\"topic\"]): # (2)!\n        writer({\"custom_llm_chunk\": chunk}) # (3)!\n    return {\"result\": \"completed\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_arbitrary_model)\n    # Add other nodes and edges as needed\n    .compile()\n)\n\nfor chunk in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"custom\", # (4)!\n):\n    # The chunk will contain the custom data streamed from the llm\n    print(chunk)\n```\n\n1. Get the stream writer to send custom data.\n2. Generate LLM tokens using your custom streaming client.\n3. Use the writer to send custom data to the stream.\n4. Set `stream_mode=\"custom\"` to receive the custom data in the stream.\n\n\n\n\n??? example \"Extended example: streaming arbitrary chat model\"\n\n      ```python\n      import operator\n      import json\n\n      from typing import TypedDict\n      from typing_extensions import Annotated\n      from langgraph.graph import StateGraph, START\n\n      from openai import AsyncOpenAI\n\n      openai_client = AsyncOpenAI()\n      model_name = \"gpt-4o-mini\"\n\n\n      async def stream_tokens(model_name: str, messages: list[dict]):\n          response = await openai_client.chat.completions.create(\n              messages=messages, model=model_name, stream=True\n          )\n          role = None\n          async for chunk in response:\n              delta = chunk.choices[0].delta\n\n              if delta.role is not None:\n                  role = delta.role\n\n              if delta.content:\n                  yield {\"role\": role, \"content\": delta.content}\n\n\n      # this is our tool\n      async def get_items(place: str) -> str:\n          \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n          writer = get_stream_writer()\n          response = \"\"\n          async for msg_chunk in stream_tokens(\n              model_name,\n              [\n                  {\n                      \"role\": \"user\",\n                      \"content\": (\n                          \"Can you tell me what kind of items \"\n                          f\"i might find in the following place: '{place}'. \"\n                          \"List at least 3 such items separating them by a comma. \"\n                          \"And include a brief description of each item.\"\n                      ),\n                  }\n              ],\n          ):\n              response += msg_chunk[\"content\"]\n              writer(msg_chunk)\n\n          return response\n\n\n      class State(TypedDict):\n          messages: Annotated[list[dict], operator.add]\n\n\n      # this is the tool-calling graph node\n      async def call_tool(state: State):\n          ai_message = state[\"messages\"][-1]\n          tool_call = ai_message[\"tool_calls\"][-1]\n\n          function_name = tool_call[\"function\"][\"name\"]\n          if function_name != \"get_items\":\n              raise ValueError(f\"Tool {function_name} not supported\")\n\n          function_arguments = tool_call[\"function\"][\"arguments\"]\n          arguments = json.loads(function_arguments)\n\n          function_response = await get_items(**arguments)\n          tool_message = {\n              \"tool_call_id\": tool_call[\"id\"],\n              \"role\": \"tool\",\n              \"name\": function_name,\n              \"content\": function_response,\n          }\n          return {\"messages\": [tool_message]}\n\n\n      graph = (\n          StateGraph(State)\n          .add_node(call_tool)\n          .add_edge(START, \"call_tool\")\n          .compile()\n      )\n      ```\n\n      Let's invoke the graph with an AI message that includes a tool call:\n\n      ```python\n      inputs = {\n          \"messages\": [\n              {\n                  \"content\": None,\n                  \"role\": \"assistant\",\n                  \"tool_calls\": [\n                      {\n                          \"id\": \"1\",\n                          \"function\": {\n                              \"arguments\": '{\"place\":\"bedroom\"}',\n                              \"name\": \"get_items\",\n                          },\n                          \"type\": \"function\",\n                      }\n                  ],\n              }\n          ]\n      }\n\n      async for chunk in graph.astream(\n          inputs,\n          stream_mode=\"custom\",\n      ):\n          print(chunk[\"content\"], end=\"|\", flush=True)\n      ```\n\n\n\n\n### Disable streaming for specific chat models\n\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\nmodels that do not support it.\n\nSet `disable_streaming=True` when initializing the model.\n\n=== \"init_chat_model\"\n\n      ```python hl_lines=\"5\"\n      from langchain.chat_models import init_chat_model\n\n      model = init_chat_model(\n          \"anthropic:claude-3-7-sonnet-latest\",\n          disable_streaming=True # (1)!\n      )\n      ```\n\n      1. Set `disable_streaming=True` to disable streaming for the chat model.\n\n=== \"chat model interface\"\n\n      ```python\n      from langchain_openai import ChatOpenAI\n\n      llm = ChatOpenAI(model=\"o1-preview\", disable_streaming=True) # (1)!\n      ```\n\n      1. Set `disable_streaming=True` to disable streaming for the chat model.\n\n\n\n\n\n### Async with Python < 3.11 { #async }\n\nIn Python versions < 3.11, [asyncio tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) do not support the `context` parameter.  \nThis limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:\n\n1. You **must** explicitly pass [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated.\n2. You **cannot** use `get_stream_writer()` in async nodes or tools — you must pass a `writer` argument directly.\n\n??? example \"Extended example: async LLM call with manual config\"\n\n      ```python hl_lines=\"16 29\"\n      from typing import TypedDict\n      from langgraph.graph import START, StateGraph\n      from langchain.chat_models import init_chat_model\n\n      llm = init_chat_model(model=\"openai:gpt-4o-mini\")\n\n      class State(TypedDict):\n          topic: str\n          joke: str\n\n      async def call_model(state, config): # (1)!\n          topic = state[\"topic\"]\n          print(\"Generating joke...\")\n          joke_response = await llm.ainvoke(\n              [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n              config, # (2)!\n          )\n          return {\"joke\": joke_response.content}\n\n      graph = (\n          StateGraph(State)\n          .add_node(call_model)\n          .add_edge(START, \"call_model\")\n          .compile()\n      )\n\n      async for chunk, metadata in graph.astream(\n          {\"topic\": \"ice cream\"},\n          stream_mode=\"messages\", # (3)!\n      ):\n          if chunk.content:\n              print(chunk.content, end=\"|\", flush=True)\n      ```\n\n      1. Accept `config` as an argument in the async node function.\n      2. Pass `config` to `llm.ainvoke()` to ensure proper context propagation.\n      3. Set `stream_mode=\"messages\"` to stream LLM tokens.\n\n??? example \"Extended example: async custom streaming with stream writer\"\n\n      ```python hl_lines=\"8 21\"\n      from typing import TypedDict\n      from langgraph.types import StreamWriter\n\n      class State(TypedDict):\n            topic: str\n            joke: str\n\n      async def generate_joke(state: State, writer: StreamWriter): # (1)!\n            writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n            return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\n      graph = (\n            StateGraph(State)\n            .add_node(generate_joke)\n            .add_edge(START, \"generate_joke\")\n            .compile()\n      )\n\n      async for chunk in graph.astream(\n            {\"topic\": \"ice cream\"},\n            stream_mode=\"custom\", # (2)!\n      ):\n            print(chunk)\n      ```\n\n      1. Add `writer` as an argument in the function signature of the async node or tool. LangGraph will automatically pass the stream writer to the function.\n      2. Set `stream_mode=\"custom\"` to receive the custom data in the stream.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 33325,
    "word_count": 3480
  },
  {
    "title": "Use the functional API",
    "source": "how-tos/use-functional-api.md",
    "content": "# Use the functional API\n\nThe [**Functional API**](../concepts/functional_api.md) allows you to add LangGraph's key features — [persistence](../concepts/persistence.md), [memory](../how-tos/memory/add-memory.md), [human-in-the-loop](../concepts/human_in_the_loop.md), and [streaming](../concepts/streaming.md) — to your applications with minimal changes to your existing code.\n\n!!! tip\n\n    For conceptual information on the functional API, see [Functional API](../concepts/functional_api.md).\n\n## Creating a simple workflow\n\nWhen defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.\n\n```python\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -> int:\n    value = inputs[\"value\"]\n    another_value = inputs[\"another_value\"]\n    ...\n\nmy_workflow.invoke({\"value\": 1, \"another_value\": 2})\n```\n\n\n\n\n\n??? example \"Extended example: simple workflow\"\n\n    ```python\n    import uuid\n    from langgraph.func import entrypoint, task\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    # Task that checks if a number is even\n    @task\n    def is_even(number: int) -> bool:\n        return number % 2 == 0\n\n    # Task that formats a message\n    @task\n    def format_message(is_even: bool) -> str:\n        return \"The number is even.\" if is_even else \"The number is odd.\"\n\n    # Create a checkpointer for persistence\n    checkpointer = InMemorySaver()\n\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(inputs: dict) -> str:\n        \"\"\"Simple workflow to classify a number.\"\"\"\n        even = is_even(inputs[\"number\"]).result()\n        return format_message(even).result()\n\n    # Run the workflow with a unique thread ID\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    result = workflow.invoke({\"number\": 7}, config=config)\n    print(result)\n    ```\n\n\n\n\n??? example \"Extended example: Compose an essay with an LLM\"\n\n    This example demonstrates how to use the `@task` and `@entrypoint` decorators\n    syntactically. Given that a checkpointer is provided, the workflow results will\n    be persisted in the checkpointer.\n\n    ```python\n    import uuid\n    from langchain.chat_models import init_chat_model\n    from langgraph.func import entrypoint, task\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    llm = init_chat_model('openai:gpt-3.5-turbo')\n\n    # Task: generate essay using an LLM\n    @task\n    def compose_essay(topic: str) -> str:\n        \"\"\"Generate an essay about the given topic.\"\"\"\n        return llm.invoke([\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes essays.\"},\n            {\"role\": \"user\", \"content\": f\"Write an essay about {topic}.\"}\n        ]).content\n\n    # Create a checkpointer for persistence\n    checkpointer = InMemorySaver()\n\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(topic: str) -> str:\n        \"\"\"Simple workflow that generates an essay with an LLM.\"\"\"\n        return compose_essay(topic).result()\n\n    # Execute the workflow\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    result = workflow.invoke(\"the history of flight\", config=config)\n    print(result)\n    ```\n\n\n\n\n## Parallel execution\n\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).\n\n```python\n@task\ndef add_one(number: int) -> int:\n    return number + 1\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(numbers: list[int]) -> list[str]:\n    futures = [add_one(i) for i in numbers]\n    return [f.result() for f in futures]\n```\n\n\n\n\n\n??? example \"Extended example: parallel LLM calls\"\n\n    This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.\n\n    ```python\n    import uuid\n    from langchain.chat_models import init_chat_model\n    from langgraph.func import entrypoint, task\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    # Initialize the LLM model\n    llm = init_chat_model(\"openai:gpt-3.5-turbo\")\n\n    # Task that generates a paragraph about a given topic\n    @task\n    def generate_paragraph(topic: str) -> str:\n        response = llm.invoke([\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n            {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n        ])\n        return response.content\n\n    # Create a checkpointer for persistence\n    checkpointer = InMemorySaver()\n\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(topics: list[str]) -> str:\n        \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n        futures = [generate_paragraph(topic) for topic in topics]\n        paragraphs = [f.result() for f in futures]\n        return \"\\n\\n\".join(paragraphs)\n\n    # Run the workflow\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    result = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\n    print(result)\n    ```\n\n\n\n\n    This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.\n\n## Calling graphs\n\nThe **Functional API** and the [**Graph API**](../concepts/low_level.md) can be used together in the same application as they share the same underlying runtime.\n\n```python\nfrom langgraph.func import entrypoint\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph()\n...\nsome_graph = builder.compile()\n\n@entrypoint()\ndef some_workflow(some_input: dict) -> int:\n    # Call a graph defined using the graph API\n    result_1 = some_graph.invoke(...)\n    # Call another graph defined using the graph API\n    result_2 = another_graph.invoke(...)\n    return {\n        \"result_1\": result_1,\n        \"result_2\": result_2\n    }\n```\n\n\n\n\n\n??? example \"Extended example: calling a simple graph from the functional API\"\n\n    ```python\n    import uuid\n    from typing import TypedDict\n    from langgraph.func import entrypoint\n    from langgraph.checkpoint.memory import InMemorySaver\n    from langgraph.graph import StateGraph\n\n    # Define the shared state type\n    class State(TypedDict):\n        foo: int\n\n    # Define a simple transformation node\n    def double(state: State) -> State:\n        return {\"foo\": state[\"foo\"] * 2}\n\n    # Build the graph using the Graph API\n    builder = StateGraph(State)\n    builder.add_node(\"double\", double)\n    builder.set_entry_point(\"double\")\n    graph = builder.compile()\n\n    # Define the functional API workflow\n    checkpointer = InMemorySaver()\n\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(x: int) -> dict:\n        result = graph.invoke({\"foo\": x})\n        return {\"bar\": result[\"foo\"]}\n\n    # Execute the workflow\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    print(workflow.invoke(5, config=config))  # Output: {'bar': 10}\n    ```\n\n\n\n\n## Call other entrypoints\n\nYou can call other **entrypoints** from within an **entrypoint** or a **task**.\n\n```python\n@entrypoint() # Will automatically use the checkpointer from the parent entrypoint\ndef some_other_workflow(inputs: dict) -> int:\n    return inputs[\"value\"]\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -> int:\n    value = some_other_workflow.invoke({\"value\": 1})\n    return value\n```\n\n\n\n\n\n??? example \"Extended example: calling another entrypoint\"\n\n    ```python\n    import uuid\n    from langgraph.func import entrypoint\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    # Initialize a checkpointer\n    checkpointer = InMemorySaver()\n\n    # A reusable sub-workflow that multiplies a number\n    @entrypoint()\n    def multiply(inputs: dict) -> int:\n        return inputs[\"a\"] * inputs[\"b\"]\n\n    # Main workflow that invokes the sub-workflow\n    @entrypoint(checkpointer=checkpointer)\n    def main(inputs: dict) -> dict:\n        result = multiply.invoke({\"a\": inputs[\"x\"], \"b\": inputs[\"y\"]})\n        return {\"product\": result}\n\n    # Execute the main workflow\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    print(main.invoke({\"x\": 6, \"y\": 7}, config=config))  # Output: {'product': 42}\n    ```\n\n\n\n\n## Streaming\n\nThe **Functional API** uses the same streaming mechanism as the **Graph API**. Please\nread the [**streaming guide**](../concepts/streaming.md) section for more details.\n\nExample of using the streaming API to stream both updates and custom data.\n\n```python hl_lines=\"17\"\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.config import get_stream_writer # (1)!\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs: dict) -> int:\n    writer = get_stream_writer() # (2)!\n    writer(\"Started processing\") # (3)!\n    result = inputs[\"x\"] * 2\n    writer(f\"Result is {result}\") # (4)!\n    return result\n\nconfig = {\"configurable\": {\"thread_id\": \"abc\"}}\n\nfor mode, chunk in main.stream( # (5)!\n    {\"x\": 5},\n    stream_mode=[\"custom\", \"updates\"], # (6)!\n    config=config\n):\n    print(f\"{mode}: {chunk}\")\n```\n\n1. Import `get_stream_writer` from `langgraph.config`.\n2. Obtain a stream writer instance within the entrypoint.\n3. Emit custom data before computation begins.\n4. Emit another custom message after computing the result.\n5. Use `.stream()` to process streamed output.\n6. Specify which streaming modes to use.\n\n```pycon\n('updates', {'add_one': 2})\n('updates', {'add_two': 3})\n('custom', 'hello')\n('custom', 'world')\n('updates', {'main': 5})\n```\n\n!!! important \"Async with Python < 3.11\"\n\n    If using Python < 3.11 and writing async code, using `get_stream_writer()` will not work. Instead please\n    use the `StreamWriter` class directly. See [Async with Python < 3.11](../how-tos/streaming.md#async) for more details.\n\n    ```python hl_lines=\"4\"\n    from langgraph.types import StreamWriter\n\n    @entrypoint(checkpointer=checkpointer)\n    async def main(inputs: dict, writer: StreamWriter) -> int:\n        ...\n    ```\n\n\n\n\n\n## Retry policy\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import RetryPolicy\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n# Let's configure the RetryPolicy to retry on ValueError.\n# The default RetryPolicy is optimized for retrying specific network errors.\nretry_policy = RetryPolicy(retry_on=ValueError)\n\n@task(retry_policy=retry_policy)\ndef get_info():\n    global attempts\n    attempts += 1\n\n    if attempts < 2:\n        raise ValueError('Failure')\n    return \"OK\"\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer):\n    return get_info().result()\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\n\nmain.invoke({'any_input': 'foobar'}, config=config)\n```\n\n```pycon\n'OK'\n```\n\n\n\n\n\n## Caching Tasks\n\n```python\nimport time\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import CachePolicy\n\n\n@task(cache_policy=CachePolicy(ttl=120))  # (1)!\ndef slow_add(x: int) -> int:\n    time.sleep(1)\n    return x * 2\n\n\n@entrypoint(cache=InMemoryCache())\ndef main(inputs: dict) -> dict[str, int]:\n    result1 = slow_add(inputs[\"x\"]).result()\n    result2 = slow_add(inputs[\"x\"]).result()\n    return {\"result1\": result1, \"result2\": result2}\n\n\nfor chunk in main.stream({\"x\": 5}, stream_mode=\"updates\"):\n    print(chunk)\n\n#> {'slow_add': 10}\n#> {'slow_add': 10, '__metadata__': {'cached': True}}\n#> {'main': {'result1': 10, 'result2': 10}}\n```\n\n1. `ttl` is specified in seconds. The cache will be invalidated after this time.\n\n\n\n\n## Resuming after an error\n\n```python\nimport time\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import StreamWriter\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n@task()\ndef get_info():\n    \"\"\"\n    Simulates a task that fails once before succeeding.\n    Raises an exception on the first attempt, then returns \"OK\" on subsequent tries.\n    \"\"\"\n    global attempts\n    attempts += 1\n\n    if attempts < 2:\n        raise ValueError(\"Failure\")  # Simulate a failure on the first attempt\n    return \"OK\"\n\n# Initialize an in-memory checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@task\ndef slow_task():\n    \"\"\"\n    Simulates a slow-running task by introducing a 1-second delay.\n    \"\"\"\n    time.sleep(1)\n    return \"Ran slow task.\"\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer: StreamWriter):\n    \"\"\"\n    Main workflow function that runs the slow_task and get_info tasks sequentially.\n\n    Parameters:\n    - inputs: Dictionary containing workflow input values.\n    - writer: StreamWriter for streaming custom data.\n\n    The workflow first executes `slow_task` and then attempts to execute `get_info`,\n    which will fail on the first invocation.\n    \"\"\"\n    slow_task_result = slow_task().result()  # Blocking call to slow_task\n    get_info().result()  # Exception will be raised here on the first attempt\n    return slow_task_result\n\n# Workflow execution configuration with a unique thread identifier\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"  # Unique identifier to track workflow execution\n    }\n}\n\n# This invocation will take ~1 second due to the slow_task execution\ntry:\n    # First invocation will raise an exception due to the `get_info` task failing\n    main.invoke({'any_input': 'foobar'}, config=config)\nexcept ValueError:\n    pass  # Handle the failure gracefully\n```\n\nWhen we resume execution, we won't need to re-run the `slow_task` as its result is already saved in the checkpoint.\n\n```python\nmain.invoke(None, config=config)\n```\n\n```pycon\n'Ran slow task.'\n```\n\n\n\n\n\n## Human-in-the-loop\n\nThe functional API supports [human-in-the-loop](../concepts/human_in_the_loop.md) workflows using the `interrupt` function and the `Command` primitive.\n\n### Basic human-in-the-loop workflow\n\nWe will create three [tasks](../concepts/functional_api.md#task):\n\n1. Append `\"bar\"`.\n2. Pause for human input. When resuming, append human input.\n3. Append `\"qux\"`.\n\n```python\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import Command, interrupt\n\n\n@task\ndef step_1(input_query):\n    \"\"\"Append bar.\"\"\"\n    return f\"{input_query} bar\"\n\n\n@task\ndef human_feedback(input_query):\n    \"\"\"Append user input.\"\"\"\n    feedback = interrupt(f\"Please provide feedback: {input_query}\")\n    return f\"{input_query} {feedback}\"\n\n\n@task\ndef step_3(input_query):\n    \"\"\"Append qux.\"\"\"\n    return f\"{input_query} qux\"\n```\n\n\n\n\n\nWe can now compose these tasks in an [entrypoint](../concepts/functional_api.md#entrypoint):\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(input_query):\n    result_1 = step_1(input_query).result()\n    result_2 = human_feedback(result_1).result()\n    result_3 = step_3(result_2).result()\n\n    return result_3\n```\n\n\n\n\n\n[interrupt()](../how-tos/human_in_the_loop/add-human-in-the-loop.md#pause-using-interrupt) is called inside a task, enabling a human to review and edit the output of the previous task. The results of prior tasks-- in this case `step_1`-- are persisted, so that they are not run again following the `interrupt`.\n\nLet's send in a query string:\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in graph.stream(\"foo\", config):\n    print(event)\n    print(\"\\n\")\n```\n\n\n\n\n\nNote that we've paused with an `interrupt` after `step_1`. The interrupt provides instructions to resume the run. To resume, we issue a [Command](../how-tos/human_in_the_loop/add-human-in-the-loop.md#resume-using-the-command-primitive) containing the data expected by the `human_feedback` task.\n\n```python\n# Continue execution\nfor event in graph.stream(Command(resume=\"baz\"), config):\n    print(event)\n    print(\"\\n\")\n```\n\n\n\n\n\nAfter resuming, the run proceeds through the remaining step and terminates as expected.\n\n### Review tool calls\n\nTo review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](../how-tos/human_in_the_loop/add-human-in-the-loop.md#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.\n\nGiven a tool call, our function will `interrupt` for human review. At that point we can either:\n\n- Accept the tool call\n- Revise the tool call and continue\n- Generate a custom tool message (e.g., instructing the model to re-format its tool call)\n\n```python\nfrom typing import Union\n\ndef review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:\n    \"\"\"Review a tool call, returning a validated version.\"\"\"\n    human_review = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            \"tool_call\": tool_call,\n        }\n    )\n    review_action = human_review[\"action\"]\n    review_data = human_review.get(\"data\")\n    if review_action == \"continue\":\n        return tool_call\n    elif review_action == \"update\":\n        updated_tool_call = {**tool_call, **{\"args\": review_data}}\n        return updated_tool_call\n    elif review_action == \"feedback\":\n        return ToolMessage(\n            content=review_data, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]\n        )\n```\n\n\n\n\n\nWe can now update our [entrypoint](../concepts/functional_api.md#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the `ToolMessage` supplied by the human. The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the `interrupt`.\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph.message import add_messages\nfrom langgraph.types import Command, interrupt\n\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef agent(messages, previous):\n    if previous is not None:\n        messages = add_messages(previous, messages)\n\n    llm_response = call_model(messages).result()\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Review tool calls\n        tool_results = []\n        tool_calls = []\n        for i, tool_call in enumerate(llm_response.tool_calls):\n            review = review_tool_call(tool_call)\n            if isinstance(review, ToolMessage):\n                tool_results.append(review)\n            else:  # is a validated tool call\n                tool_calls.append(review)\n                if review != tool_call:\n                    llm_response.tool_calls[i] = review  # update message\n\n        # Execute remaining tool calls\n        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]\n        remaining_tool_results = [fut.result() for fut in tool_result_futures]\n\n        # Append to message list\n        messages = add_messages(\n            messages,\n            [llm_response, *tool_results, *remaining_tool_results],\n        )\n\n        # Call model again\n        llm_response = call_model(messages).result()\n\n    # Generate final response\n    messages = add_messages(messages, llm_response)\n    return entrypoint.final(value=llm_response, save=messages)\n```\n\n\n\n\n\n## Short-term memory\n\nShort-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](../concepts/functional_api.md#short-term-memory) for more details.\n\n### Manage checkpoints\n\nYou can view and delete the information stored by the checkpointer.\n\n#### View thread state (checkpoint)\n\n```python hl_lines=\"3 6 10\"\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\",\n        # optionally provide an ID for a specific checkpoint,\n        # otherwise the latest checkpoint is shown\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n\n    }\n}\ngraph.get_state(config)\n```\n\n```\nStateSnapshot(\n    values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n    metadata={\n        'source': 'loop',\n        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n        'step': 4,\n        'parents': {},\n        'thread_id': '1'\n    },\n    created_at='2025-05-05T16:01:24.680462+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    tasks=(),\n    interrupts=()\n)\n```\n\n\n\n\n\n#### View the history of the thread (checkpoints)\n\n```python hl_lines=\"3 6\"\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\nlist(graph.get_state_history(config))\n```\n\n```\n[\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:24.680462+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\n        next=('call_model',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863421+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=('__start__',),\n        config={...},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863173+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=(),\n        config={...},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.862295+00:00',\n        parent_config={...}\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\n        next=('call_model',),\n        config={...},\n        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.278960+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.277497+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\n        interrupts=()\n    )\n]\n```\n\n\n\n\n\n### Decouple return value from saved value\n\nUse `entrypoint.final` to decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when:\n\n- You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation.\n- You need to control what gets passed to the previous parameter on the next run.\n\n```python\nfrom typing import Optional\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef accumulate(n: int, *, previous: Optional[int]) -> entrypoint.final[int, int]:\n    previous = previous or 0\n    total = previous + n\n    # Return the *previous* value to the caller but save the *new* total to the checkpoint.\n    return entrypoint.final(value=previous, save=total)\n\nconfig = {\"configurable\": {\"thread_id\": \"my-thread\"}}\n\nprint(accumulate.invoke(1, config=config))  # 0\nprint(accumulate.invoke(2, config=config))  # 1\nprint(accumulate.invoke(3, config=config))  # 3\n```\n\n\n\n\n\n### Chatbot example\n\nAn example of a simple chatbot using the functional API and the `InMemorySaver` checkpointer.\nThe bot is able to remember the previous conversation and continue from where it left off.\n\n```python\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n@task\ndef call_model(messages: list[BaseMessage]):\n    response = model.invoke(messages)\n    return response\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):\n    if previous:\n        inputs = add_messages(previous, inputs)\n\n    response = call_model(inputs).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n\ninput_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n```\n\n\n\n\n\n??? example \"Extended example: build a simple chatbot\"\n\n     [How to add thread-level persistence (functional API)](./persistence-functional.ipynb): Shows how to add thread-level persistence to a functional API workflow and implements a simple chatbot.\n\n## Long-term memory\n\n[long-term memory](../concepts/memory.md#long-term-memory) allows storing information across different **thread ids**. This could be useful for learning information about a given user in one conversation and using it in another.\n\n??? example \"Extended example: add long-term memory\"\n\n    [How to add cross-thread persistence (functional API)](./cross-thread-persistence-functional.ipynb): Shows how to add cross-thread persistence to a functional API workflow and implements a simple chatbot.\n\n## Workflows\n\n- [Workflows and agent](../tutorials/workflows.md) guide for more examples of how to build workflows using the Functional API.\n\n## Agents\n\n- [How to create an agent from scratch (Functional API)](./react-agent-from-scratch-functional.ipynb): Shows how to create a simple agent from scratch using the functional API.\n- [How to build a multi-agent network](./multi-agent-network-functional.ipynb): Shows how to build a multi-agent network using the functional API.\n- [How to add multi-turn conversation in a multi-agent application (functional API)](./multi-agent-multi-turn-convo-functional.ipynb): allow an end-user to engage in a multi-turn conversation with one or more agents.\n\n## Integrate with other libraries\n\n- [Add LangGraph's features to other frameworks using the functional API](./autogen-integration-functional.ipynb): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 29737,
    "word_count": 3147
  },
  {
    "title": "Use subgraphs",
    "source": "how-tos/subgraph.md",
    "content": "# Use subgraphs\n\nThis guide explains the mechanics of using [subgraphs](../concepts/subgraphs.md). A common application of subgraphs is to build [multi-agent](../concepts/multi_agent.md) systems.\n\nWhen adding subgraphs, you need to define how the parent graph and the subgraph communicate:\n\n* [Shared state schemas](#shared-state-schemas) — parent and subgraph have **shared state keys** in their state [schemas](../concepts/low_level.md#state)\n* [Different state schemas](#different-state-schemas) — **no shared state keys** in parent and subgraph [schemas](../concepts/low_level.md#state)\n\n## Setup\n\n```bash\npip install -U langgraph\n```\n\n\n\n\n!!! tip \"Set up LangSmith for LangGraph development\"\n\n    Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).\n\n## Shared state schemas\n\nA common case is for the parent graph and subgraph to communicate over a shared state key (channel) in the [schema](../concepts/low_level.md#state). For example, in [multi-agent](../concepts/multi_agent.md) systems, the agents often communicate over a shared [messages](https://langchain-ai.github.io/langgraph/concepts/low_level.md#why-use-messages) key.\n\nIf your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:\n\n1. Define the subgraph workflow (`subgraph_builder` in the example below) and compile it\n2. Pass compiled subgraph to the `.add_node` method when defining the parent graph workflow\n\n```python\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n```\n\n\n\n\n??? example \"Full example: shared state schemas\"\n\n    ```python\n    from typing_extensions import TypedDict\n    from langgraph.graph.state import StateGraph, START\n\n    # Define subgraph\n    class SubgraphState(TypedDict):\n        foo: str  # (1)! \n        bar: str  # (2)!\n    \n    def subgraph_node_1(state: SubgraphState):\n        return {\"bar\": \"bar\"}\n    \n    def subgraph_node_2(state: SubgraphState):\n        # note that this node is using a state key ('bar') that is only available in the subgraph\n        # and is sending update on the shared state key ('foo')\n        return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n    \n    subgraph_builder = StateGraph(SubgraphState)\n    subgraph_builder.add_node(subgraph_node_1)\n    subgraph_builder.add_node(subgraph_node_2)\n    subgraph_builder.add_edge(START, \"subgraph_node_1\")\n    subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n    subgraph = subgraph_builder.compile()\n    \n    # Define parent graph\n    class ParentState(TypedDict):\n        foo: str\n    \n    def node_1(state: ParentState):\n        return {\"foo\": \"hi! \" + state[\"foo\"]}\n    \n    builder = StateGraph(ParentState)\n    builder.add_node(\"node_1\", node_1)\n    builder.add_node(\"node_2\", subgraph)\n    builder.add_edge(START, \"node_1\")\n    builder.add_edge(\"node_1\", \"node_2\")\n    graph = builder.compile()\n    \n    for chunk in graph.stream({\"foo\": \"foo\"}):\n        print(chunk)\n    ```\n\n    1. This key is shared with the parent graph state\n    2. This key is private to the `SubgraphState` and is not visible to the parent graph\n    \n    ```\n    {'node_1': {'foo': 'hi! foo'}}\n    {'node_2': {'foo': 'hi! foobar'}}\n    ```\n\n\n\n\n## Different state schemas\n\nFor more complex systems you might want to define subgraphs that have a **completely different schema** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](../concepts/multi_agent.md) system.\n\nIf that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.\n\n```python\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass SubgraphState(TypedDict):\n    bar: str\n\n# Subgraph\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"hi! \" + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nclass State(TypedDict):\n    foo: str\n\ndef call_subgraph(state: State):\n    subgraph_output = subgraph.invoke({\"bar\": state[\"foo\"]})  # (1)!\n    return {\"foo\": subgraph_output[\"bar\"]}  # (2)!\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", call_subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n```\n\n1. Transform the state to the subgraph state\n2. Transform response back to the parent state\n\n\n\n\n??? example \"Full example: different state schemas\"\n\n    ```python\n    from typing_extensions import TypedDict\n    from langgraph.graph.state import StateGraph, START\n\n    # Define subgraph\n    class SubgraphState(TypedDict):\n        # note that none of these keys are shared with the parent graph state\n        bar: str\n        baz: str\n    \n    def subgraph_node_1(state: SubgraphState):\n        return {\"baz\": \"baz\"}\n    \n    def subgraph_node_2(state: SubgraphState):\n        return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n    \n    subgraph_builder = StateGraph(SubgraphState)\n    subgraph_builder.add_node(subgraph_node_1)\n    subgraph_builder.add_node(subgraph_node_2)\n    subgraph_builder.add_edge(START, \"subgraph_node_1\")\n    subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n    subgraph = subgraph_builder.compile()\n    \n    # Define parent graph\n    class ParentState(TypedDict):\n        foo: str\n    \n    def node_1(state: ParentState):\n        return {\"foo\": \"hi! \" + state[\"foo\"]}\n    \n    def node_2(state: ParentState):\n        response = subgraph.invoke({\"bar\": state[\"foo\"]})  # (1)!\n        return {\"foo\": response[\"bar\"]}  # (2)!\n    \n    \n    builder = StateGraph(ParentState)\n    builder.add_node(\"node_1\", node_1)\n    builder.add_node(\"node_2\", node_2)\n    builder.add_edge(START, \"node_1\")\n    builder.add_edge(\"node_1\", \"node_2\")\n    graph = builder.compile()\n    \n    for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n        print(chunk)\n    ```\n\n    1. Transform the state to the subgraph state\n    2. Transform response back to the parent state\n\n    ```\n    ((), {'node_1': {'foo': 'hi! foo'}})\n    (('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n    (('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_2': {'bar': 'hi! foobaz'}})\n    ((), {'node_2': {'foo': 'hi! foobaz'}})\n    ```\n\n\n\n\n??? example \"Full example: different state schemas (two levels of subgraphs)\"\n\n    This is an example with two levels of subgraphs: parent -> child -> grandchild.\n\n    ```python\n    # Grandchild graph\n    from typing_extensions import TypedDict\n    from langgraph.graph.state import StateGraph, START, END\n    \n    class GrandChildState(TypedDict):\n        my_grandchild_key: str\n    \n    def grandchild_1(state: GrandChildState) -> GrandChildState:\n        # NOTE: child or parent keys will not be accessible here\n        return {\"my_grandchild_key\": state[\"my_grandchild_key\"] + \", how are you\"}\n    \n    \n    grandchild = StateGraph(GrandChildState)\n    grandchild.add_node(\"grandchild_1\", grandchild_1)\n    \n    grandchild.add_edge(START, \"grandchild_1\")\n    grandchild.add_edge(\"grandchild_1\", END)\n    \n    grandchild_graph = grandchild.compile()\n    \n    # Child graph\n    class ChildState(TypedDict):\n        my_child_key: str\n    \n    def call_grandchild_graph(state: ChildState) -> ChildState:\n        # NOTE: parent or grandchild keys won't be accessible here\n        grandchild_graph_input = {\"my_grandchild_key\": state[\"my_child_key\"]}  # (1)!\n        grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)\n        return {\"my_child_key\": grandchild_graph_output[\"my_grandchild_key\"] + \" today?\"}  # (2)!\n    \n    child = StateGraph(ChildState)\n    child.add_node(\"child_1\", call_grandchild_graph)  # (3)!\n    child.add_edge(START, \"child_1\")\n    child.add_edge(\"child_1\", END)\n    child_graph = child.compile()\n    \n    # Parent graph\n    class ParentState(TypedDict):\n        my_key: str\n    \n    def parent_1(state: ParentState) -> ParentState:\n        # NOTE: child or grandchild keys won't be accessible here\n        return {\"my_key\": \"hi \" + state[\"my_key\"]}\n    \n    def parent_2(state: ParentState) -> ParentState:\n        return {\"my_key\": state[\"my_key\"] + \" bye!\"}\n    \n    def call_child_graph(state: ParentState) -> ParentState:\n        child_graph_input = {\"my_child_key\": state[\"my_key\"]}  # (4)!\n        child_graph_output = child_graph.invoke(child_graph_input)\n        return {\"my_key\": child_graph_output[\"my_child_key\"]}  # (5)!\n    \n    parent = StateGraph(ParentState)\n    parent.add_node(\"parent_1\", parent_1)\n    parent.add_node(\"child\", call_child_graph)  # (6)!\n    parent.add_node(\"parent_2\", parent_2)\n    \n    parent.add_edge(START, \"parent_1\")\n    parent.add_edge(\"parent_1\", \"child\")\n    parent.add_edge(\"child\", \"parent_2\")\n    parent.add_edge(\"parent_2\", END)\n    \n    parent_graph = parent.compile()\n    \n    for chunk in parent_graph.stream({\"my_key\": \"Bob\"}, subgraphs=True):\n        print(chunk)\n    ```\n\n    1. We're transforming the state from the child state channels (`my_child_key`) to the child state channels (`my_grandchild_key`)\n    2. We're transforming the state from the grandchild state channels (`my_grandchild_key`) back to the child state channels (`my_child_key`)\n    3. We're passing a function here instead of just compiled graph (`grandchild_graph`)\n    4. We're transforming the state from the parent state channels (`my_key`) to the child state channels (`my_child_key`)\n    5. We're transforming the state from the child state channels (`my_child_key`) back to the parent state channels (`my_key`)\n    6. We're passing a function here instead of just a compiled graph (`child_graph`)\n\n    ```\n    ((), {'parent_1': {'my_key': 'hi Bob'}})\n    (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n    (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n    ((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n    ((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n    ```\n\n\n\n\n## Add persistence \n\nYou only need to **provide the checkpointer when compiling the parent graph**. LangGraph will automatically propagate the checkpointer to the child subgraphs.\n\n```python\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n```    \n\n\n\n\nIf you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](../concepts/multi_agent.md) systems, if you want agents to keep track of their internal message histories:\n\n```python\nsubgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)\n```\n\n\n\n\n## View subgraph state\n\nWhen you enable [persistence](../concepts/persistence.md), you can [inspect the graph state](../concepts/persistence.md#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.\n\nYou can inspect the graph state via `graph.get_state(config)`. To view the subgraph state, you can use `graph.get_state(config, subgraphs=True)`.\n\n\n\n\n!!! important \"Available **only** when interrupted\"\n\n    Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won't be able to access the subgraph state.\n\n??? example \"View interrupted subgraph state\"\n\n    ```python\n    from langgraph.graph import START, StateGraph\n    from langgraph.checkpoint.memory import MemorySaver\n    from langgraph.types import interrupt, Command\n    from typing_extensions import TypedDict\n    \n    class State(TypedDict):\n        foo: str\n    \n    # Subgraph\n    \n    def subgraph_node_1(state: State):\n        value = interrupt(\"Provide value:\")\n        return {\"foo\": state[\"foo\"] + value}\n    \n    subgraph_builder = StateGraph(State)\n    subgraph_builder.add_node(subgraph_node_1)\n    subgraph_builder.add_edge(START, \"subgraph_node_1\")\n    \n    subgraph = subgraph_builder.compile()\n    \n    # Parent graph\n        \n    builder = StateGraph(State)\n    builder.add_node(\"node_1\", subgraph)\n    builder.add_edge(START, \"node_1\")\n    \n    checkpointer = MemorySaver()\n    graph = builder.compile(checkpointer=checkpointer)\n    \n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n    \n    graph.invoke({\"foo\": \"\"}, config)\n    parent_state = graph.get_state(config)\n    subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state  # (1)!\n    \n    # resume the subgraph\n    graph.invoke(Command(resume=\"bar\"), config)\n    ```\n    \n    1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.\n\n\n\n\n## Stream subgraph outputs\n\nTo include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n\n```python\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    subgraphs=True, # (1)!\n    stream_mode=\"updates\",\n):\n    print(chunk)\n```\n\n1. Set `subgraphs=True` to stream outputs from subgraphs.\n\n\n\n\n??? example \"Stream from subgraphs\"\n\n    ```python\n    from typing_extensions import TypedDict\n    from langgraph.graph.state import StateGraph, START\n\n    # Define subgraph\n    class SubgraphState(TypedDict):\n        foo: str\n        bar: str\n    \n    def subgraph_node_1(state: SubgraphState):\n        return {\"bar\": \"bar\"}\n    \n    def subgraph_node_2(state: SubgraphState):\n        # note that this node is using a state key ('bar') that is only available in the subgraph\n        # and is sending update on the shared state key ('foo')\n        return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n    \n    subgraph_builder = StateGraph(SubgraphState)\n    subgraph_builder.add_node(subgraph_node_1)\n    subgraph_builder.add_node(subgraph_node_2)\n    subgraph_builder.add_edge(START, \"subgraph_node_1\")\n    subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n    subgraph = subgraph_builder.compile()\n    \n    # Define parent graph\n    class ParentState(TypedDict):\n        foo: str\n    \n    def node_1(state: ParentState):\n        return {\"foo\": \"hi! \" + state[\"foo\"]}\n    \n    builder = StateGraph(ParentState)\n    builder.add_node(\"node_1\", node_1)\n    builder.add_node(\"node_2\", subgraph)\n    builder.add_edge(START, \"node_1\")\n    builder.add_edge(\"node_1\", \"node_2\")\n    graph = builder.compile()\n\n    for chunk in graph.stream(\n        {\"foo\": \"foo\"},\n        stream_mode=\"updates\",\n        subgraphs=True, # (1)!\n    ):\n        print(chunk)\n    ```\n  \n    1. Set `subgraphs=True` to stream outputs from subgraphs.\n\n    ```\n    ((), {'node_1': {'foo': 'hi! foo'}})\n    (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})\n    (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n    ((), {'node_2': {'foo': 'hi! foobar'}})\n    ```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 16589,
    "word_count": 1651
  },
  {
    "title": "Build multi-agent systems",
    "source": "how-tos/multi_agent.md",
    "content": "# Build multi-agent systems\n\nA single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and composing them into a [multi-agent system](../concepts/multi_agent.md).\n\nIn multi-agent systems, agents need to communicate between each other. They do so via [handoffs](#handoffs) — a primitive that describes which agent to hand control to and the payload to send to that agent.\n\nThis guide covers the following:\n\n* implementing [handoffs](#handoffs) between agents\n* using handoffs and the prebuilt [agent](../agents/agents.md) to [build a custom multi-agent system](#build-a-multi-agent-system)\n\nTo get started with building multi-agent systems, check out LangGraph [prebuilt implementations](#prebuilt-implementations) of two of the most popular multi-agent architectures — [supervisor](../agents/multi-agent.md#supervisor) and [swarm](../agents/multi-agent.md#swarm).\n\n## Handoffs\n\nTo set up communication between the agents in a multi-agent system you can use [**handoffs**](../concepts/multi_agent.md#handoffs) — a pattern where one agent *hands off* control to another. Handoffs allow you to specify:\n\n- **destination**: target agent to navigate to (e.g., name of the LangGraph node to go to)\n- **payload**: information to pass to that agent (e.g., state update)\n\n### Create handoffs\n\nTo implement handoffs, you can return `Command` objects from your agent nodes or tools:\n\n```python hl_lines=\"13 14 23 24 25\"\nfrom typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Transfer to {agent_name}\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        state: Annotated[MessagesState, InjectedState], # (1)!\n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -> Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        return Command(  # (2)!\n            goto=agent_name,  # (3)!\n            update={\"messages\": state[\"messages\"] + [tool_message]},  # (4)!\n            graph=Command.PARENT,  # (5)!\n        )\n    return handoff_tool\n```\n\n1. Access the [state](../concepts/low_level.md#state) of the agent that is calling the handoff tool using the [InjectedState](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.InjectedState) annotation. \n2. The `Command` primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.\n3. Name of the agent or node to hand off to.\n4. Take the agent's messages and **add** them to the parent's **state** as part of the handoff. The next agent will see the parent state.\n5. Indicate to LangGraph that we need to navigate to agent node in a **parent** multi-agent graph.\n\n!!! tip\n\n    If you want to use tools that return `Command`, you can either use prebuilt [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) / [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) components, or implement your own tool-executing node that collects `Command` objects returned by the tools and returns a list of them, e.g.:\n    \n    ```python\n    def call_tools(state):\n        ...\n        commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n        return commands\n    ```\n\n\n\n\n!!! Important\n\n    This handoff implementation assumes that:\n    \n    - each agent receives overall message history (across all agents) in the multi-agent system as its input. If you want more control over agent inputs, see [this section](#control-agent-inputs)\n    - each agent outputs its internal messages history to the overall message history of the multi-agent system. If you want more control over **how agent outputs are added**, wrap the agent in a separate node function:\n\n      ```python hl_lines=\"5\"\n      def call_hotel_assistant(state):\n          # return agent's final response,\n          # excluding inner monologue\n          response = hotel_assistant.invoke(state)\n          return {\"messages\": response[\"messages\"][-1]}\n      ```\n\n\n\n\n### Control agent inputs\n\nYou can use the [`Send()`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) primitive to directly send data to the worker agents during the handoff. For example, you can request that the calling agent populate a task description for the next agent:\n\n```python hl_lines=\"5 26\"\nfrom typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command, Send\n\ndef create_task_description_handoff_tool(\n    *, agent_name: str, description: str | None = None\n):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Ask {agent_name} for help.\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        # this is populated by the calling agent\n        task_description: Annotated[\n            str,\n            \"Description of what the next agent should do, including all of the relevant context.\",\n        ],\n        # these parameters are ignored by the LLM\n        state: Annotated[MessagesState, InjectedState],\n    ) -> Command:\n        task_description_message = {\"role\": \"user\", \"content\": task_description}\n        agent_input = {**state, \"messages\": [task_description_message]}\n        return Command(\n            goto=[Send(agent_name, agent_input)],\n            graph=Command.PARENT,\n        )\n\n    return handoff_tool\n```\n\n\n\n\nSee the multi-agent [supervisor](../tutorials/multi_agent/agent_supervisor.md#4-create-delegation-tasks) example for a full example of using [`Send()`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) in handoffs.\n\n## Build a multi-agent system\n\nYou can use handoffs in any agents built with LangGraph. We recommend using the prebuilt [agent](../agents/overview.md) or [`ToolNode`](./tool-calling.md#toolnode), as they natively support handoffs tools returning `Command`. Below is an example of how you can implement a multi-agent system for booking travel using handoffs:\n\n```python hl_lines=\"16 17 21 22 28 29\"\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.graph import StateGraph, START, MessagesState\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    # same implementation as above\n    ...\n    return Command(...)\n\n# Handoffs\ntransfer_to_hotel_assistant = create_handoff_tool(agent_name=\"hotel_assistant\")\ntransfer_to_flight_assistant = create_handoff_tool(agent_name=\"flight_assistant\")\n\n# Define agents\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[..., transfer_to_hotel_assistant],\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[..., transfer_to_flight_assistant],\n    name=\"hotel_assistant\"\n)\n\n# Define multi-agent graph\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    .add_edge(START, \"flight_assistant\")\n    .compile()\n)\n```\n\n\n\n\n??? example \"Full example: Multi-agent system for booking travel\"\n\n    ```python hl_lines=\"56 57 66 67 68 94 96 100 102 124\"\n    from typing import Annotated\n    from langchain_core.messages import convert_to_messages\n    from langchain_core.tools import tool, InjectedToolCallId\n    from langgraph.prebuilt import create_react_agent, InjectedState\n    from langgraph.graph import StateGraph, START, MessagesState\n    from langgraph.types import Command\n    \n    # We'll use `pretty_print_messages` helper to render the streamed agent outputs nicely\n    \n    def pretty_print_message(message, indent=False):\n        pretty_message = message.pretty_repr(html=True)\n        if not indent:\n            print(pretty_message)\n            return\n    \n        indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n        print(indented)\n    \n    \n    def pretty_print_messages(update, last_message=False):\n        is_subgraph = False\n        if isinstance(update, tuple):\n            ns, update = update\n            # skip parent graph updates in the printouts\n            if len(ns) == 0:\n                return\n    \n            graph_id = ns[-1].split(\":\")[0]\n            print(f\"Update from subgraph {graph_id}:\")\n            print(\"\\n\")\n            is_subgraph = True\n    \n        for node_name, node_update in update.items():\n            update_label = f\"Update from node {node_name}:\"\n            if is_subgraph:\n                update_label = \"\\t\" + update_label\n    \n            print(update_label)\n            print(\"\\n\")\n    \n            messages = convert_to_messages(node_update[\"messages\"])\n            if last_message:\n                messages = messages[-1:]\n    \n            for m in messages:\n                pretty_print_message(m, indent=is_subgraph)\n            print(\"\\n\")\n\n\n    def create_handoff_tool(*, agent_name: str, description: str | None = None):\n        name = f\"transfer_to_{agent_name}\"\n        description = description or f\"Transfer to {agent_name}\"\n    \n        @tool(name, description=description)\n        def handoff_tool(\n            state: Annotated[MessagesState, InjectedState], # (1)!\n            tool_call_id: Annotated[str, InjectedToolCallId],\n        ) -> Command:\n            tool_message = {\n                \"role\": \"tool\",\n                \"content\": f\"Successfully transferred to {agent_name}\",\n                \"name\": name,\n                \"tool_call_id\": tool_call_id,\n            }\n            return Command(  # (2)!\n                goto=agent_name,  # (3)!\n                update={\"messages\": state[\"messages\"] + [tool_message]},  # (4)!\n                graph=Command.PARENT,  # (5)!\n            )\n        return handoff_tool\n    \n    # Handoffs\n    transfer_to_hotel_assistant = create_handoff_tool(\n        agent_name=\"hotel_assistant\",\n        description=\"Transfer user to the hotel-booking assistant.\",\n    )\n    transfer_to_flight_assistant = create_handoff_tool(\n        agent_name=\"flight_assistant\",\n        description=\"Transfer user to the flight-booking assistant.\",\n    )\n    \n    # Simple agent tools\n    def book_hotel(hotel_name: str):\n        \"\"\"Book a hotel\"\"\"\n        return f\"Successfully booked a stay at {hotel_name}.\"\n    \n    def book_flight(from_airport: str, to_airport: str):\n        \"\"\"Book a flight\"\"\"\n        return f\"Successfully booked a flight from {from_airport} to {to_airport}.\"\n    \n    # Define agents\n    flight_assistant = create_react_agent(\n        model=\"anthropic:claude-3-5-sonnet-latest\",\n        tools=[book_flight, transfer_to_hotel_assistant],\n        prompt=\"You are a flight booking assistant\",\n        name=\"flight_assistant\"\n    )\n    hotel_assistant = create_react_agent(\n        model=\"anthropic:claude-3-5-sonnet-latest\",\n        tools=[book_hotel, transfer_to_flight_assistant],\n        prompt=\"You are a hotel booking assistant\",\n        name=\"hotel_assistant\"\n    )\n    \n    # Define multi-agent graph\n    multi_agent_graph = (\n        StateGraph(MessagesState)\n        .add_node(flight_assistant)\n        .add_node(hotel_assistant)\n        .add_edge(START, \"flight_assistant\")\n        .compile()\n    )\n    \n    # Run the multi-agent graph\n    for chunk in multi_agent_graph.stream(\n        {\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n                }\n            ]\n        },\n        subgraphs=True\n    ):\n        pretty_print_messages(chunk)\n    ```\n\n    1. Access agent's state\n    2. The `Command` primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.\n    3. Name of the agent or node to hand off to.\n    4. Take the agent's messages and **add** them to the parent's **state** as part of the handoff. The next agent will see the parent state.\n    5. Indicate to LangGraph that we need to navigate to agent node in a **parent** multi-agent graph.\n\n\n\n\n## Multi-turn conversation\n\nUsers might want to engage in a *multi-turn conversation* with one or more agents. To build a system that can handle this, you can create a node that uses an [`interrupt`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Interrupt) to collect user input and routes back to the **active** agent.\n\nThe agents can then be implemented as nodes in a graph that executes agent steps and determines the next action:\n\n1. **Wait for user input** to continue the conversation, or  \n2. **Route to another agent** (or back to itself, such as in a loop) via a [handoff](#handoffs)\n\n```python\ndef human(state) -> Command[Literal[\"agent\", \"another_agent\"]]:\n    \"\"\"A node for collecting user input.\"\"\"\n    user_input = interrupt(value=\"Ready for user input.\")\n\n    # Determine the active agent.\n    active_agent = ...\n\n    ...\n    return Command(\n        update={\n            \"messages\": [{\n                \"role\": \"human\",\n                \"content\": user_input,\n            }]\n        },\n        goto=active_agent\n    )\n\ndef agent(state) -> Command[Literal[\"agent\", \"another_agent\", \"human\"]]:\n    # The condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    goto = get_next_agent(...)  # 'agent' / 'another_agent'\n    if goto:\n        return Command(goto=goto, update={\"my_state_key\": \"my_state_value\"})\n    else:\n        return Command(goto=\"human\") # Go to human node\n```\n\n\n\n\n??? example \"Full example: multi-agent system for travel recommendations\"\n\n    In this example, we will build a team of travel assistant agents that can communicate with each other via handoffs.\n    \n    We will create 2 agents:\n    \n    * travel_advisor: can help with travel destination recommendations. Can ask hotel_advisor for help.\n    * hotel_advisor: can help with hotel recommendations. Can ask travel_advisor for help.\n\n    ```python\n    from langchain_anthropic import ChatAnthropic\n    from langgraph.graph import MessagesState, StateGraph, START\n    from langgraph.prebuilt import create_react_agent, InjectedState\n    from langgraph.types import Command, interrupt\n    from langgraph.checkpoint.memory import InMemorySaver\n    \n    \n    model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n    class MultiAgentState(MessagesState):\n        last_active_agent: str\n    \n    \n    # Define travel advisor tools and ReAct agent\n    travel_advisor_tools = [\n        get_travel_recommendations,\n        make_handoff_tool(agent_name=\"hotel_advisor\"),\n    ]\n    travel_advisor = create_react_agent(\n        model,\n        travel_advisor_tools,\n        prompt=(\n            \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n            \"If you need hotel recommendations, ask 'hotel_advisor' for help. \"\n            \"You MUST include human-readable response before transferring to another agent.\"\n        ),\n    )\n    \n    \n    def call_travel_advisor(\n        state: MultiAgentState,\n    ) -> Command[Literal[\"hotel_advisor\", \"human\"]]:\n        # You can also add additional logic like changing the input to the agent / output from the agent, etc.\n        # NOTE: we're invoking the ReAct agent with the full history of messages in the state\n        response = travel_advisor.invoke(state)\n        update = {**response, \"last_active_agent\": \"travel_advisor\"}\n        return Command(update=update, goto=\"human\")\n    \n    \n    # Define hotel advisor tools and ReAct agent\n    hotel_advisor_tools = [\n        get_hotel_recommendations,\n        make_handoff_tool(agent_name=\"travel_advisor\"),\n    ]\n    hotel_advisor = create_react_agent(\n        model,\n        hotel_advisor_tools,\n        prompt=(\n            \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\n            \"If you need help picking travel destinations, ask 'travel_advisor' for help.\"\n            \"You MUST include human-readable response before transferring to another agent.\"\n        ),\n    )\n    \n    \n    def call_hotel_advisor(\n        state: MultiAgentState,\n    ) -> Command[Literal[\"travel_advisor\", \"human\"]]:\n        response = hotel_advisor.invoke(state)\n        update = {**response, \"last_active_agent\": \"hotel_advisor\"}\n        return Command(update=update, goto=\"human\")\n    \n    \n    def human_node(\n        state: MultiAgentState, config\n    ) -> Command[Literal[\"hotel_advisor\", \"travel_advisor\", \"human\"]]:\n        \"\"\"A node for collecting user input.\"\"\"\n    \n        user_input = interrupt(value=\"Ready for user input.\")\n        active_agent = state[\"last_active_agent\"]\n    \n        return Command(\n            update={\n                \"messages\": [\n                    {\n                        \"role\": \"human\",\n                        \"content\": user_input,\n                    }\n                ]\n            },\n            goto=active_agent,\n        )\n    \n    \n    builder = StateGraph(MultiAgentState)\n    builder.add_node(\"travel_advisor\", call_travel_advisor)\n    builder.add_node(\"hotel_advisor\", call_hotel_advisor)\n    \n    # This adds a node to collect human input, which will route\n    # back to the active agent.\n    builder.add_node(\"human\", human_node)\n    \n    # We'll always start with a general travel advisor.\n    builder.add_edge(START, \"travel_advisor\")\n    \n    \n    checkpointer = InMemorySaver()\n    graph = builder.compile(checkpointer=checkpointer)\n    ```\n    \n    Let's test a multi turn conversation with this application.\n\n    ```python\n    import uuid\n    \n    thread_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    \n    inputs = [\n        # 1st round of conversation,\n        {\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"i wanna go somewhere warm in the caribbean\"}\n            ]\n        },\n        # Since we're using `interrupt`, we'll need to resume using the Command primitive.\n        # 2nd round of conversation,\n        Command(\n            resume=\"could you recommend a nice hotel in one of the areas and tell me which area it is.\"\n        ),\n        # 3rd round of conversation,\n        Command(\n            resume=\"i like the first one. could you recommend something to do near the hotel?\"\n        ),\n    ]\n    \n    for idx, user_input in enumerate(inputs):\n        print()\n        print(f\"--- Conversation Turn {idx + 1} ---\")\n        print()\n        print(f\"User: {user_input}\")\n        print()\n        for update in graph.stream(\n            user_input,\n            config=thread_config,\n            stream_mode=\"updates\",\n        ):\n            for node_id, value in update.items():\n                if isinstance(value, dict) and value.get(\"messages\", []):\n                    last_message = value[\"messages\"][-1]\n                    if isinstance(last_message, dict) or last_message.type != \"ai\":\n                        continue\n                    print(f\"{node_id}: {last_message.content}\")\n    ```\n    \n    ```\n    --- Conversation Turn 1 ---\n    \n    User: {'messages': [{'role': 'user', 'content': 'i wanna go somewhere warm in the caribbean'}]}\n    \n    travel_advisor: Based on the recommendations, Aruba would be an excellent choice for your Caribbean getaway! Aruba is known as \"One Happy Island\" and offers:\n    - Year-round warm weather with consistent temperatures around 82°F (28°C)\n    - Beautiful white sand beaches like Eagle Beach and Palm Beach\n    - Clear turquoise waters perfect for swimming and snorkeling\n    - Minimal rainfall and location outside the hurricane belt\n    - A blend of Caribbean and Dutch culture\n    - Great dining options and nightlife\n    - Various water sports and activities\n    \n    Would you like me to get some specific hotel recommendations in Aruba for your stay? I can transfer you to our hotel advisor who can help with accommodations.\n    \n    --- Conversation Turn 2 ---\n    \n    User: Command(resume='could you recommend a nice hotel in one of the areas and tell me which area it is.')\n    \n    hotel_advisor: Based on the recommendations, I can suggest two excellent options:\n    \n    1. The Ritz-Carlton, Aruba - Located in Palm Beach\n    - This luxury resort is situated in the vibrant Palm Beach area\n    - Known for its exceptional service and amenities\n    - Perfect if you want to be close to dining, shopping, and entertainment\n    - Features multiple restaurants, a casino, and a world-class spa\n    - Located on a pristine stretch of Palm Beach\n    \n    2. Bucuti & Tara Beach Resort - Located in Eagle Beach\n    - An adults-only boutique resort on Eagle Beach\n    - Known for being more intimate and peaceful\n    - Award-winning for its sustainability practices\n    - Perfect for a romantic getaway or peaceful vacation\n    - Located on one of the most beautiful beaches in the Caribbean\n    \n    Would you like more specific information about either of these properties or their locations?\n    \n    --- Conversation Turn 3 ---\n    \n    User: Command(resume='i like the first one. could you recommend something to do near the hotel?')\n    \n    travel_advisor: Near the Ritz-Carlton in Palm Beach, here are some highly recommended activities:\n    \n    1. Visit the Palm Beach Plaza Mall - Just a short walk from the hotel, featuring shopping, dining, and entertainment\n    2. Try your luck at the Stellaris Casino - It's right in the Ritz-Carlton\n    3. Take a sunset sailing cruise - Many depart from the nearby pier\n    4. Visit the California Lighthouse - A scenic landmark just north of Palm Beach\n    5. Enjoy water sports at Palm Beach:\n       - Jet skiing\n       - Parasailing\n       - Snorkeling\n       - Stand-up paddleboarding\n    \n    Would you like more specific information about any of these activities or would you like to know about other options in the area?\n    ```\n\n\n\n\n## Prebuilt implementations\n\nLangGraph comes with prebuilt implementations of two of the most popular multi-agent architectures:\n\n- [supervisor](../agents/multi-agent.md#supervisor) — individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements. You can use [`langgraph-supervisor`](https://github.com/langchain-ai/langgraph-supervisor-py) library to create a supervisor multi-agent systems.\n- [swarm](../agents/multi-agent.md#supervisor) — agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent. You can use [`langgraph-swarm`](https://github.com/langchain-ai/langgraph-swarm-py) library to create a swarm multi-agent systems.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 23524,
    "word_count": 2499
  },
  {
    "title": "How to pass custom run ID or set tags and metadata for graph runs in LangSmith",
    "source": "how-tos/run-id-langsmith.md",
    "content": "# How to pass custom run ID or set tags and metadata for graph runs in LangSmith\n\n!!! tip \"Prerequisites\"\n    This guide assumes familiarity with the following:\n    \n    - [LangSmith Documentation](https://docs.smith.langchain.com)\n    - [LangSmith Platform](https://smith.langchain.com)\n    - [RunnableConfig](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)\n    - [Add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\n    - [Customize run name](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name)\n\nDebugging graph runs can sometimes be difficult to do in an IDE or terminal. [LangSmith](https://docs.smith.langchain.com) lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read the [LangSmith documentation](https://docs.smith.langchain.com) for more information on how to get started.\n\nTo make it easier to identify and analyzed traces generated during graph invocation, you can set additional configuration at run time (see [RunnableConfig](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)):\n\n| **Field**   | **Type**            | **Description**                                                                                                    |\n|-------------|---------------------|--------------------------------------------------------------------------------------------------------------------|\n| run_name    | `str`               | Name for the tracer run for this call. Defaults to the name of the class.                                          |\n| run_id      | `UUID`              | Unique identifier for the tracer run for this call. If not provided, a new UUID will be generated.                 |\n| tags        | `List[str]`         | Tags for this call and any sub-calls (e.g., a Chain calling an LLM). You can use these to filter calls.            |\n| metadata    | `Dict[str, Any]`    | Metadata for this call and any sub-calls (e.g., a Chain calling an LLM). Keys should be strings, values should be JSON-serializable. |\n\nLangGraph graphs implement the [LangChain Runnable Interface](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) and accept a second argument (`RunnableConfig`) in methods like `invoke`, `ainvoke`, `stream` etc.\n\nThe LangSmith platform will allow you to search and filter traces based on `run_name`, `run_id`, `tags` and `metadata`.\n\n## TLDR\n\n```python\nimport uuid\n# Generate a random UUID -- it must be a UUID\nconfig = {\"run_id\": uuid.uuid4()}, \"tags\": [\"my_tag1\"], \"metadata\": {\"a\": 5}}\n# Works with all standard Runnable methods \n# like invoke, batch, ainvoke, astream_events etc\ngraph.stream(inputs, config, stream_mode=\"values\")\n```\n\nThe rest of the how to guide will show a full agent.\n\n## Setup\n\nFirst, let's install the required packages and set our API keys\n\n```python\n%%capture --no-stderr\n%pip install --quiet -U langgraph langchain_openai\n```\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"LANGSMITH_API_KEY\")\n```\n\n!!! tip\n    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. [LangSmith](https://docs.smith.langchain.com) lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).\n\n## Define the graph\n\nFor this example we will use the [prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/).\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom typing import Literal\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.tools import tool\n\n# First we initialize the model we want to use.\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n\n# For this tutorial we will use custom tool that returns pre-defined values for weather in two cities (NYC & SF)\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\ntools = [get_weather]\n\n\n# Define the graph\ngraph = create_react_agent(model, tools=tools)\n```\n\n## Run your graph\n\nNow that we've defined our graph let's run it once and view the trace in LangSmith. In order for our trace to be easily accessible in LangSmith, we will pass in a custom `run_id` in the config.\n\nThis assumes that you have set your `LANGSMITH_API_KEY` environment variable.\n\nNote that you can also configure what project to trace to by setting the `LANGCHAIN_PROJECT` environment variable, by default runs will be traced to the `default` project.\n\n```python\nimport uuid\n\n\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\n\ninputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n\nconfig = {\"run_name\": \"agent_007\", \"tags\": [\"cats are awesome\"]}\n\nprint_stream(graph.stream(inputs, config, stream_mode=\"values\"))\n```\n\n**Output:**\n```\n================================ Human Message ==================================\n\nwhat is the weather in sf\n================================== Ai Message ===================================\nTool Calls:\n  get_weather (call_9ZudXyMAdlUjptq9oMGtQo8o)\n Call ID: call_9ZudXyMAdlUjptq9oMGtQo8o\n  Args:\n    city: sf\n================================= Tool Message ==================================\nName: get_weather\n\nIt's always sunny in sf\n================================== Ai Message ===================================\n\nThe weather in San Francisco is currently sunny.\n```\n\n## View the trace in LangSmith\n\nNow that we've ran our graph, let's head over to LangSmith and view our trace. First click into the project that you traced to (in our case the default project). You should see a run with the custom run name \"agent_007\".\n\n![LangSmith Trace View](assets/d38d1f2b-0f4c-4707-b531-a3c749de987f.png)\n\nIn addition, you will be able to filter traces after the fact using the tags or metadata provided. For example,\n\n![LangSmith Filter View](assets/410e0089-2ab8-46bb-a61a-827187fd46b3.png)",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 6668,
    "word_count": 753
  },
  {
    "title": "Call tools",
    "source": "how-tos/tool-calling.md",
    "content": "# Call tools\n\n[Tools](../concepts/tools.md) encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and determine the appropriate arguments.\n\nYou can [define your own tools](#define-a-tool) or use [prebuilt tools](#prebuilt-tools)\n\n## Define a tool\n\nDefine a basic tool with the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator:\n\n```python hl_lines=\"3\"\nfrom langchain_core.tools import tool\n\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n```\n\n\n\n\n\n## Run a tool\n\nTools conform to the [Runnable interface](https://python.langchain.com/docs/concepts/runnables/), which means you can run a tool using the `invoke` method:\n\n```python\nmultiply.invoke({\"a\": 6, \"b\": 7})  # returns 42\n```\n\n\n\n\n\nIf the tool is invoked with `type=\"tool_call\"`, it will return a [ToolMessage](https://python.langchain.com/docs/concepts/messages/#toolmessage):\n\n```python\ntool_call = {\n    \"type\": \"tool_call\",\n    \"id\": \"1\",\n    \"args\": {\"a\": 42, \"b\": 7}\n}\nmultiply.invoke(tool_call) # returns a ToolMessage object\n```\n\nOutput:\n\n```pycon\nToolMessage(content='294', name='multiply', tool_call_id='1')\n```\n\n\n\n\n\n## Use in an agent\n\nTo create a tool-calling agent, you can use the prebuilt [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent):\n\n```python hl_lines=\"2 9\"\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet\",\n    tools=[multiply]\n)\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n```\n\n\n\n\n\n### Dynamically select tools\n\nConfigure tool availability at runtime based on context:\n\n```python hl_lines=\"30 42 44 56\"\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.tools import tool\n\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass CustomContext:\n    tools: list[Literal[\"weather\", \"compass\"]]\n\n\n@tool\ndef weather() -> str:\n    \"\"\"Returns the current weather conditions.\"\"\"\n    return \"It's nice and sunny.\"\n\n\n@tool\ndef compass() -> str:\n    \"\"\"Returns the direction the user is facing.\"\"\"\n    return \"North\"\n\nmodel = init_chat_model(\"anthropic:claude-sonnet-4-20250514\")\n\ndef configure_model(state: AgentState, runtime: Runtime[CustomContext]):\n    \"\"\"Configure the model with tools based on runtime context.\"\"\"\n    selected_tools = [\n        tool\n        for tool in [weather, compass]\n        if tool.name in runtime.context.tools\n    ]\n    return model.bind_tools(selected_tools)\n\n\nagent = create_react_agent(\n    # Dynamically configure the model with tools based on runtime context\n    configure_model,\n    # Initialize with all tools available\n    tools=[weather, compass]\n)\n\noutput = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Who are you and what tools do you have access to?\",\n            }\n        ]\n    },\n    context=CustomContext(tools=[\"weather\"]),  # Only enable the weather tool\n)\n\nprint(output[\"messages\"][-1].text())\n```\n\n!!! version-added \"Added in version 0.6.0\"\n\n\n\n## Use in a workflow\n\nIf you are writing a custom workflow, you will need to:\n\n1. register the tools with the chat model\n2. call the tool if the model decides to use it\n\nUse `model.bind_tools()` to register the tools with the model.\n\n```python hl_lines=\"5\"\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\n\nmodel_with_tools = model.bind_tools([multiply])\n```\n\n\n\n\n\nLLMs automatically determine if a tool invocation is necessary and handle calling the tool with the appropriate arguments.\n\n??? example \"Extended example: attach tools to a chat model\"\n\n    ```python hl_lines=\"10\"\n    from langchain_core.tools import tool\n    from langchain.chat_models import init_chat_model\n\n    @tool\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply two numbers.\"\"\"\n        return a * b\n\n    model = init_chat_model(model=\"claude-3-5-haiku-latest\")\n    model_with_tools = model.bind_tools([multiply])\n\n    response_message = model_with_tools.invoke(\"what's 42 x 7?\")\n    tool_call = response_message.tool_calls[0]\n\n    multiply.invoke(tool_call)\n    ```\n\n    ```pycon\n    ToolMessage(\n        content='294',\n        name='multiply',\n        tool_call_id='toolu_0176DV4YKSD8FndkeuuLj36c'\n    )\n    ```\n\n\n\n\n#### ToolNode\n\nTo execute tools in custom workflows, use the prebuilt [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) or implement your own custom node.\n\n`ToolNode` is a specialized node for executing tools in a workflow. It provides the following features:\n\n- Supports both synchronous and asynchronous tools.\n- Executes multiple tools concurrently.\n- Handles errors during tool execution (`handle_tool_errors=True`, enabled by default). See [handling tool errors](#handle-errors) for more details.\n\n`ToolNode` operates on [`MessagesState`](../concepts/low_level.md#messagesstate):\n\n- **Input**: `MessagesState`, where the last message is an `AIMessage` containing the `tool_calls` parameter.\n- **Output**: `MessagesState` updated with the resulting [`ToolMessage`](https://python.langchain.com/docs/concepts/messages/#toolmessage) from executed tools.\n\n```python hl_lines=\"1 14\"\nfrom langgraph.prebuilt import ToolNode\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ndef get_coolest_cities():\n    \"\"\"Get a list of coolest cities\"\"\"\n    return \"nyc, sf\"\n\ntool_node = ToolNode([get_weather, get_coolest_cities])\ntool_node.invoke({\"messages\": [...]})\n```\n\n\n\n\n\n??? example \"Single tool call\"\n\n    ```python hl_lines=\"13\"\n    from langchain_core.messages import AIMessage\n    from langgraph.prebuilt import ToolNode\n\n    # Define tools\n    @tool\n    def get_weather(location: str):\n        \"\"\"Call to get the current weather.\"\"\"\n        if location.lower() in [\"sf\", \"san francisco\"]:\n            return \"It's 60 degrees and foggy.\"\n        else:\n            return \"It's 90 degrees and sunny.\"\n\n    tool_node = ToolNode([get_weather])\n\n    message_with_single_tool_call = AIMessage(\n        content=\"\",\n        tool_calls=[\n            {\n                \"name\": \"get_weather\",\n                \"args\": {\"location\": \"sf\"},\n                \"id\": \"tool_call_id\",\n                \"type\": \"tool_call\",\n            }\n        ],\n    )\n\n    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n    ```\n\n    ```\n    {'messages': [ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='tool_call_id')]}\n    ```\n\n\n\n\n??? example \"Multiple tool calls\"\n\n    ```python hl_lines=\"17 37\"\n    from langchain_core.messages import AIMessage\n    from langgraph.prebuilt import ToolNode\n\n    # Define tools\n\n    def get_weather(location: str):\n        \"\"\"Call to get the current weather.\"\"\"\n        if location.lower() in [\"sf\", \"san francisco\"]:\n            return \"It's 60 degrees and foggy.\"\n        else:\n            return \"It's 90 degrees and sunny.\"\n\n    def get_coolest_cities():\n        \"\"\"Get a list of coolest cities\"\"\"\n        return \"nyc, sf\"\n\n    tool_node = ToolNode([get_weather, get_coolest_cities])\n\n    message_with_multiple_tool_calls = AIMessage(\n        content=\"\",\n        tool_calls=[\n            {\n                \"name\": \"get_coolest_cities\",\n                \"args\": {},\n                \"id\": \"tool_call_id_1\",\n                \"type\": \"tool_call\",\n            },\n            {\n                \"name\": \"get_weather\",\n                \"args\": {\"location\": \"sf\"},\n                \"id\": \"tool_call_id_2\",\n                \"type\": \"tool_call\",\n            },\n        ],\n    )\n\n    tool_node.invoke({\"messages\": [message_with_multiple_tool_calls]})  # (1)!\n    ```\n\n    1. `ToolNode` will execute both tools in parallel\n\n    ```\n    {\n        'messages': [\n            ToolMessage(content='nyc, sf', name='get_coolest_cities', tool_call_id='tool_call_id_1'),\n            ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='tool_call_id_2')\n        ]\n    }\n    ```\n\n\n\n\n??? example \"Use with a chat model\"\n\n    ```python hl_lines=\"11 14 17\"\n    from langchain.chat_models import init_chat_model\n    from langgraph.prebuilt import ToolNode\n\n    def get_weather(location: str):\n        \"\"\"Call to get the current weather.\"\"\"\n        if location.lower() in [\"sf\", \"san francisco\"]:\n            return \"It's 60 degrees and foggy.\"\n        else:\n            return \"It's 90 degrees and sunny.\"\n\n    tool_node = ToolNode([get_weather])\n\n    model = init_chat_model(model=\"claude-3-5-haiku-latest\")\n    model_with_tools = model.bind_tools([get_weather])  # (1)!\n\n\n    response_message = model_with_tools.invoke(\"what's the weather in sf?\")\n    tool_node.invoke({\"messages\": [response_message]})\n    ```\n\n    1. Use `.bind_tools()` to attach the tool schema to the chat model\n\n    ```\n    {'messages': [ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='toolu_01Pnkgw5JeTRxXAU7tyHT4UW')]}\n    ```\n\n\n\n\n??? example \"Use in a tool-calling agent\"\n\n    This is an example of creating a tool-calling agent from scratch using `ToolNode`. You can also use LangGraph's prebuilt [agent](../agents/agents.md).\n\n    ```python hl_lines=\"12 15 33\"\n    from langchain.chat_models import init_chat_model\n    from langgraph.prebuilt import ToolNode\n    from langgraph.graph import StateGraph, MessagesState, START, END\n\n    def get_weather(location: str):\n        \"\"\"Call to get the current weather.\"\"\"\n        if location.lower() in [\"sf\", \"san francisco\"]:\n            return \"It's 60 degrees and foggy.\"\n        else:\n            return \"It's 90 degrees and sunny.\"\n\n    tool_node = ToolNode([get_weather])\n\n    model = init_chat_model(model=\"claude-3-5-haiku-latest\")\n    model_with_tools = model.bind_tools([get_weather])\n\n    def should_continue(state: MessagesState):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        if last_message.tool_calls:\n            return \"tools\"\n        return END\n\n    def call_model(state: MessagesState):\n        messages = state[\"messages\"]\n        response = model_with_tools.invoke(messages)\n        return {\"messages\": [response]}\n\n    builder = StateGraph(MessagesState)\n\n    # Define the two nodes we will cycle between\n    builder.add_node(\"call_model\", call_model)\n    builder.add_node(\"tools\", tool_node)\n\n    builder.add_edge(START, \"call_model\")\n    builder.add_conditional_edges(\"call_model\", should_continue, [\"tools\", END])\n    builder.add_edge(\"tools\", \"call_model\")\n\n    graph = builder.compile()\n\n    graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]})\n    ```\n\n    ```\n    {\n        'messages': [\n            HumanMessage(content=\"what's the weather in sf?\"),\n            AIMessage(\n                content=[{'text': \"I'll help you check the weather in San Francisco right now.\", 'type': 'text'}, {'id': 'toolu_01A4vwUEgBKxfFVc5H3v1CNs', 'input': {'location': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}],\n                tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_01A4vwUEgBKxfFVc5H3v1CNs', 'type': 'tool_call'}]\n            ),\n            ToolMessage(content=\"It's 60 degrees and foggy.\"),\n            AIMessage(content=\"The current weather in San Francisco is 60 degrees and foggy. Typical San Francisco weather with its famous marine layer!\")\n        ]\n    }\n    ```\n\n\n\n\n## Tool customization\n\nFor more control over tool behavior, use the `@tool` decorator.\n\n### Parameter descriptions\n\nAuto-generate descriptions from docstrings:\n\n```python hl_lines=\"1 3\"\nfrom langchain_core.tools import tool\n\n@tool(\"multiply_tool\", parse_docstring=True)\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\n\n    Args:\n        a: First operand\n        b: Second operand\n    \"\"\"\n    return a * b\n```\n\n\n\n\n\n### Explicit input schema\n\nDefine schemas using `args_schema`:\n\n```python hl_lines=\"9\"\nfrom pydantic import BaseModel, Field\nfrom langchain_core.tools import tool\n\nclass MultiplyInputSchema(BaseModel):\n    \"\"\"Multiply two numbers\"\"\"\n    a: int = Field(description=\"First operand\")\n    b: int = Field(description=\"Second operand\")\n\n@tool(\"multiply_tool\", args_schema=MultiplyInputSchema)\ndef multiply(a: int, b: int) -> int:\n    return a * b\n```\n\n\n\n### Tool name\n\nOverride the default tool name using the first argument or name property:\n\n```python hl_lines=\"3\"\nfrom langchain_core.tools import tool\n\n@tool(\"multiply_tool\")\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n```\n\n\n\n\n\n## Context management\n\nTools within LangGraph sometimes require context data, such as runtime-only arguments (e.g., user IDs or session details), that should not be controlled by the model. LangGraph provides three methods for managing such context:\n\n| Type                                    | Usage Scenario                           | Mutable | Lifetime                 |\n| --------------------------------------- | ---------------------------------------- | ------- | ------------------------ |\n| [Configuration](#configuration)         | Static, immutable runtime data           | ❌      | Single invocation        |\n| [Short-term memory](#short-term-memory) | Dynamic, changing data during invocation | ✅      | Single invocation        |\n| [Long-term memory](#long-term-memory)   | Persistent, cross-session data           | ✅      | Across multiple sessions |\n\n### Configuration\n\nUse configuration when you have **immutable** runtime data that tools require, such as user identifiers. You pass these arguments via [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) at invocation and access them in the tool:\n\n```python hl_lines=\"5 13\"\nfrom langchain_core.tools import tool\nfrom langchain_core.runnables import RunnableConfig\n\n@tool\ndef get_user_info(config: RunnableConfig) -> str:\n    \"\"\"Retrieve user information based on user ID.\"\"\"\n    user_id = config[\"configurable\"].get(\"user_id\")\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\n# Invocation example with an agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user info\"}]},\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n```\n\n\n\n\n\n??? example \"Extended example: Access config in tools\"\n\n    ```python hl_lines=\"6 9 19\"\n    from langchain_core.runnables import RunnableConfig\n    from langchain_core.tools import tool\n    from langgraph.prebuilt import create_react_agent\n\n    def get_user_info(\n        config: RunnableConfig,\n    ) -> str:\n        \"\"\"Look up user info.\"\"\"\n        user_id = config[\"configurable\"].get(\"user_id\")\n        return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_user_info],\n    )\n\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n        config={\"configurable\": {\"user_id\": \"user_123\"}}\n    )\n    ```\n\n\n\n\n### Short-term memory\n\nShort-term memory maintains **dynamic** state that changes during a single execution.\n\nTo **access** (read) the graph state inside the tools, you can use a special parameter **annotation** — [`InjectedState`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.InjectedState):\n\n```python hl_lines=\"12\"\nfrom typing import Annotated, NotRequired\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import InjectedState, create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\n\nclass CustomState(AgentState):\n    # The user_name field in short-term state\n    user_name: NotRequired[str]\n\n@tool\ndef get_user_name(\n    state: Annotated[CustomState, InjectedState]\n) -> str:\n    \"\"\"Retrieve the current user-name from state.\"\"\"\n    # Return stored name or a default if not set\n    return state.get(\"user_name\", \"Unknown user\")\n\n# Example agent setup\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_name],\n    state_schema=CustomState,\n)\n\n# Invocation: reads the name from state (initially empty)\nagent.invoke({\"messages\": \"what's my name?\"})\n```\n\n\n\n\n\nUse a tool that returns a `Command` to **update** `user_name` and append a confirmation message:\n\n```python hl_lines=\"12 13 14 15 16 17\"\nfrom typing import Annotated\nfrom langgraph.types import Command\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.tools import tool, InjectedToolCallId\n\n@tool\ndef update_user_name(\n    new_name: str,\n    tool_call_id: Annotated[str, InjectedToolCallId]\n) -> Command:\n    \"\"\"Update user-name in short-term memory.\"\"\"\n    return Command(update={\n        \"user_name\": new_name,\n        \"messages\": [\n            ToolMessage(f\"Updated user name to {new_name}\", tool_call_id=tool_call_id)\n        ]\n    })\n```\n\n\n\n\n\n!!! important\n\n    If you want to use tools that return `Command` and update graph state, you can either use prebuilt [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) / [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) components, or implement your own tool-executing node that collects `Command` objects returned by the tools and returns a list of them, e.g.:\n\n    ```python\n    def call_tools(state):\n        ...\n        commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n        return commands\n    ```\n\n\n\n\n### Long-term memory\n\nUse [long-term memory](../concepts/memory.md#long-term-memory) to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.\n\nTo use long-term memory, you need to:\n\n1. [Configure a store](memory/add-memory.md#add-long-term-memory) to persist data across invocations.\n2. Access the store from within tools.\n\nTo **access** information in the store:\n\n```python hl_lines=\"4 11 13\"\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph\nfrom langgraph.config import get_store\n\n@tool\ndef get_user_info(config: RunnableConfig) -> str:\n    \"\"\"Look up user info.\"\"\"\n    # Same as that provided to `builder.compile(store=store)`\n    # or `create_react_agent`\n    store = get_store()\n    user_id = config[\"configurable\"].get(\"user_id\")\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\nbuilder = StateGraph(...)\n...\ngraph = builder.compile(store=store)\n```\n\n\n\n\n\n??? example \"Access long-term memory\"\n\n    ```python hl_lines=\"7 9 22 24 30 36\"\n    from langchain_core.runnables import RunnableConfig\n    from langchain_core.tools import tool\n    from langgraph.config import get_store\n    from langgraph.prebuilt import create_react_agent\n    from langgraph.store.memory import InMemoryStore\n\n    store = InMemoryStore() # (1)!\n\n    store.put(  # (2)!\n        (\"users\",),  # (3)!\n        \"user_123\",  # (4)!\n        {\n            \"name\": \"John Smith\",\n            \"language\": \"English\",\n        } # (5)!\n    )\n\n    @tool\n    def get_user_info(config: RunnableConfig) -> str:\n        \"\"\"Look up user info.\"\"\"\n        # Same as that provided to `create_react_agent`\n        store = get_store() # (6)!\n        user_id = config[\"configurable\"].get(\"user_id\")\n        user_info = store.get((\"users\",), user_id) # (7)!\n        return str(user_info.value) if user_info else \"Unknown user\"\n\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[get_user_info],\n        store=store # (8)!\n    )\n\n    # Run the agent\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n        config={\"configurable\": {\"user_id\": \"user_123\"}}\n    )\n    ```\n\n    1. The `InMemoryStore` is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the [store documentation][../reference/store.md) for more options. If you're deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.\n    2. For this example, we write some sample data to the store using the `put` method. Please see the [BaseStore.put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) API reference for more details.\n    3. The first argument is the namespace. This is used to group related data together. In this case, we are using the `users` namespace to group user data.\n    4. A key within the namespace. This example uses a user ID for the key.\n    5. The data that we want to store for the given user.\n    6. The `get_store` function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.\n    7. The `get` method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a `StoreValue` object, which contains the value and metadata about the value.\n    8. The `store` is passed to the agent. This enables the agent to access the store when running tools. You can also use the `get_store` function to access the store from anywhere in your code.\n\n\n\n\nTo **update** information in the store:\n\n```python hl_lines=\"4 11 13\"\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph\nfrom langgraph.config import get_store\n\n@tool\ndef save_user_info(user_info: str, config: RunnableConfig) -> str:\n    \"\"\"Save user info.\"\"\"\n    # Same as that provided to `builder.compile(store=store)`\n    # or `create_react_agent`\n    store = get_store()\n    user_id = config[\"configurable\"].get(\"user_id\")\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nbuilder = StateGraph(...)\n...\ngraph = builder.compile(store=store)\n```\n\n\n\n\n\n??? example \"Update long-term memory\"\n\n    ```python hl_lines=\"18 20 26 32\"\n    from typing_extensions import TypedDict\n\n    from langchain_core.tools import tool\n    from langgraph.config import get_store\n    from langchain_core.runnables import RunnableConfig\n    from langgraph.prebuilt import create_react_agent\n    from langgraph.store.memory import InMemoryStore\n\n    store = InMemoryStore() # (1)!\n\n    class UserInfo(TypedDict): # (2)!\n        name: str\n\n    @tool\n    def save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)!\n        \"\"\"Save user info.\"\"\"\n        # Same as that provided to `create_react_agent`\n        store = get_store() # (4)!\n        user_id = config[\"configurable\"].get(\"user_id\")\n        store.put((\"users\",), user_id, user_info) # (5)!\n        return \"Successfully saved user info.\"\n\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[save_user_info],\n        store=store\n    )\n\n    # Run the agent\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n        config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)!\n    )\n\n    # You can access the store directly to get the value\n    store.get((\"users\",), \"user_123\").value\n    ```\n\n    1. The `InMemoryStore` is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the [store documentation](../reference/store.md) for more options. If you're deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.\n    2. The `UserInfo` class is a `TypedDict` that defines the structure of the user information. The LLM will use this to format the response according to the schema.\n    3. The `save_user_info` function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.\n    4. The `get_store` function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.\n    5. The `put` method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.\n    6. The `user_id` is passed in the config. This is used to identify the user whose information is being updated.\n\n\n\n\n## Advanced tool features\n\n### Immediate return\n\nUse `return_direct=True` to immediately return a tool's result without executing additional logic.\n\nThis is useful for tools that should not trigger further processing or tool calls, allowing you to return results directly to the user.\n\n```python hl_lines=\"1\"\n@tool(return_direct=True)\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n```\n\n\n\n\n\n??? example \"Extended example: Using return_direct in a prebuilt agent\"\n\n    ```python hl_lines=\"4\"\n    from langchain_core.tools import tool\n    from langgraph.prebuilt import create_react_agent\n\n    @tool(return_direct=True)\n    def add(a: int, b: int) -> int:\n        \"\"\"Add two numbers\"\"\"\n        return a + b\n\n    agent = create_react_agent(\n        model=\"anthropic:claude-3-7-sonnet-latest\",\n        tools=[add]\n    )\n\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5?\"}]}\n    )\n    ```\n\n\n\n\n!!! important \"Using without prebuilt components\"\n\n    If you are building a custom workflow and are not relying on `create_react_agent` or `ToolNode`, you will also\n    need to implement the control flow to handle `return_direct=True`.\n\n\n\n\n### Force tool use\n\nIf you need to force a specific tool to be used, you will need to configure this at the **model** level using the `tool_choice` parameter in the bind_tools method.\n\nForce specific tool usage via tool_choice:\n\n```python hl_lines=\"11\"\n@tool(return_direct=True)\ndef greet(user_name: str) -> int:\n    \"\"\"Greet user.\"\"\"\n    return f\"Hello {user_name}!\"\n\ntools = [greet]\n\nconfigured_model = model.bind_tools(\n    tools,\n    # Force the use of the 'greet' tool\n    tool_choice={\"type\": \"tool\", \"name\": \"greet\"}\n)\n```\n\n\n\n\n\n??? example \"Extended example: Force tool usage in an agent\"\n\n    To force the agent to use specific tools, you can set the `tool_choice` option in `model.bind_tools()`:\n\n    ```python hl_lines=\"3 11\"\n    from langchain_core.tools import tool\n\n    @tool(return_direct=True)\n    def greet(user_name: str) -> int:\n        \"\"\"Greet user.\"\"\"\n        return f\"Hello {user_name}!\"\n\n    tools = [greet]\n\n    agent = create_react_agent(\n        model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}),\n        tools=tools\n    )\n\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I am Bob\"}]}\n    )\n    ```\n\n\n\n\n!!! Warning \"Avoid infinite loops\"\n\n    Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards:\n\n    - Mark the tool with [`return_direct=True`](#immediate-return) to end the loop after execution.\n    - Set [`recursion_limit`](../concepts/low_level.md#recursion-limit) to restrict the number of execution steps.\n\n\n\n\n!!! tip \"Tool choice configuration\"\n\n    The `tool_choice` parameter is used to configure which tool should be used by the model when it decides to call a tool. This is useful when you want to ensure that a specific tool is always called for a particular task or when you want to override the model's default behavior of choosing a tool based on its internal logic.\n\n    Note that not all models support this feature, and the exact configuration may vary depending on the model you are using.\n\n### Disable parallel calls\n\nFor supported providers, you can disable parallel tool calling by setting `parallel_tool_calls=False` via the `model.bind_tools()` method:\n\n```python hl_lines=\"3\"\nmodel.bind_tools(\n    tools,\n    parallel_tool_calls=False\n)\n```\n\n\n\n\n\n??? example \"Extended example: disable parallel tool calls in a prebuilt agent\"\n\n    ```python hl_lines=\"15\"\n    from langchain.chat_models import init_chat_model\n\n    def add(a: int, b: int) -> int:\n        \"\"\"Add two numbers\"\"\"\n        return a + b\n\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply two numbers.\"\"\"\n        return a * b\n\n    model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n    tools = [add, multiply]\n    agent = create_react_agent(\n        # disable parallel tool calls\n        model=model.bind_tools(tools, parallel_tool_calls=False),\n        tools=tools\n    )\n\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5 and 4 * 7?\"}]}\n    )\n    ```\n\n\n\n\n### Handle errors\n\nLangGraph provides built-in error handling for tool execution through the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) component, used both independently and in prebuilt agents.\n\nBy **default**, `ToolNode` catches exceptions raised during tool execution and returns them as `ToolMessage` objects with a status indicating an error.\n\n```python\nfrom langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\ndef multiply(a: int, b: int) -> int:\n    if a == 42:\n        raise ValueError(\"The ultimate error\")\n    return a * b\n\n# Default error handling (enabled by default)\ntool_node = ToolNode([multiply])\n\nmessage = AIMessage(\n    content=\"\",\n    tool_calls=[{\n        \"name\": \"multiply\",\n        \"args\": {\"a\": 42, \"b\": 7},\n        \"id\": \"tool_call_id\",\n        \"type\": \"tool_call\"\n    }]\n)\n\nresult = tool_node.invoke({\"messages\": [message]})\n```\n\nOutput:\n\n```pycon\n{'messages': [\n    ToolMessage(\n        content=\"Error: ValueError('The ultimate error')\\n Please fix your mistakes.\",\n        name='multiply',\n        tool_call_id='tool_call_id',\n        status='error'\n    )\n]}\n```\n\n\n\n\n\n#### Disable error handling\n\nTo propagate exceptions directly, disable error handling:\n\n```python\ntool_node = ToolNode([multiply], handle_tool_errors=False)\n```\n\n\n\n\n\nWith error handling disabled, exceptions raised by tools will propagate up, requiring explicit management.\n\n#### Custom error messages\n\nProvide a custom error message by setting the error handling parameter to a string:\n\n```python\ntool_node = ToolNode(\n    [multiply],\n    handle_tool_errors=\"Can't use 42 as the first operand, please switch operands!\"\n)\n```\n\nExample output:\n\n```python\n{'messages': [\n    ToolMessage(\n        content=\"Can't use 42 as the first operand, please switch operands!\",\n        name='multiply',\n        tool_call_id='tool_call_id',\n        status='error'\n    )\n]}\n```\n\n\n\n\n\n#### Error handling in agents\n\nError handling in prebuilt agents (`create_react_agent`) leverages `ToolNode`:\n\n```python\nfrom langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[multiply]\n)\n\n# Default error handling\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n```\n\nTo disable or customize error handling in prebuilt agents, explicitly pass a configured `ToolNode`:\n\n```python\ncustom_tool_node = ToolNode(\n    [multiply],\n    handle_tool_errors=\"Cannot use 42 as a first operand!\"\n)\n\nagent_custom = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=custom_tool_node\n)\n\nagent_custom.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n```\n\n\n\n\n\n### Handle large numbers of tools\n\nAs the number of available tools grows, you may want to limit the scope of the LLM's selection, to decrease token consumption and to help manage sources of error in LLM reasoning.\n\nTo address this, you can dynamically adjust the tools available to a model by retrieving relevant tools at runtime using semantic search.\n\nSee [`langgraph-bigtool`](https://github.com/langchain-ai/langgraph-bigtool) prebuilt library for a ready-to-use implementation.\n\n## Prebuilt tools\n\n### LLM provider tools\n\nYou can use prebuilt tools from model providers by passing a dictionary with tool specs to the `tools` parameter of `create_react_agent`. For example, to use the `web_search_preview` tool from OpenAI:\n\n```python\nfrom langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"openai:gpt-4o-mini\",\n    tools=[{\"type\": \"web_search_preview\"}]\n)\nresponse = agent.invoke(\n    {\"messages\": [\"What was a positive news story from today?\"]}\n)\n```\n\nPlease consult the documentation for the specific model you are using to see which tools are available and how to use them.\n\n\n\n\n### LangChain tools\n\nAdditionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development.\n\nYou can browse the full list of available integrations in the [LangChain integrations directory](https://python.langchain.com/docs/integrations/tools/).\n\nSome commonly used tool categories include:\n\n- **Search**: Bing, SerpAPI, Tavily\n- **Code interpreters**: Python REPL, Node.js REPL\n- **Databases**: SQL, MongoDB, Redis\n- **Web data**: Web scraping and browsing\n- **APIs**: OpenWeatherMap, NewsAPI, and others\n\nThese integrations can be configured and added to your agents using the same `tools` parameter shown in the examples above.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 34288,
    "word_count": 3765
  },
  {
    "title": "How to add cross-thread persistence (functional API)",
    "source": "how-tos/cross-thread-persistence-functional.ipynb",
    "content": "# How to add cross-thread persistence (functional API)\n\n!!! info \"Prerequisites\"\n\n    This guide assumes familiarity with the following:\n    \n    - <a href=\"../../concepts/functional_api/\">Functional API</a>\n    - <a href=\"../../concepts/persistence/\">Persistence</a>\n    - <a href=\"../../concepts/memory/\">Memory</a>\n    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n\nLangGraph allows you to persist data across **different <a href=\"../../concepts/persistence/#threads\">threads</a>**. For instance, you can store information about users (their names or preferences) in a shared (cross-thread) memory and reuse them in the new threads (e.g., new conversations).\n\nWhen using the <a href=\"../../concepts/functional_api/\">functional API</a>, you can set it up to store and retrieve memories by using the [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) interface:\n\n1. Create an instance of a `Store`\n\n    ```python\n    from langgraph.store.memory import InMemoryStore, BaseStore\n    \n    store = InMemoryStore()\n    ```\n\n2. Pass the `store` instance to the `entrypoint()` decorator and expose `store` parameter in the function signature:\n\n    ```python\n    from langgraph.func import entrypoint\n\n    @entrypoint(store=store)\n    def workflow(inputs: dict, store: BaseStore):\n        my_task(inputs).result()\n        ...\n    ```\n    \nIn this guide, we will show how to construct and use a workflow that has a shared memory implemented using the [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) interface.\n\n!!! note Note\n\n    Support for the [`Store`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) API that is used in this guide was added in LangGraph `v0.2.32`.\n\n    Support for __index__ and __query__ arguments of the [`Store`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) API that is used in this guide was added in LangGraph `v0.2.54`.\n\n!!! tip \"Note\"\n\n    If you need to add cross-thread persistence to a `StateGraph`, check out this <a href=\"../cross-thread-persistence\">how-to guide</a>.\n\n## Setup\n\nFirst, let's install the required packages and set our API keys\n\n\n```shell\npip install -U langchain_anthropic langchain_openai langgraph\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n_set_env(\"OPENAI_API_KEY\")\n```\n\n!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n\n    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)\n\n## Example: simple chatbot with long-term memory\n\n### Define store\n\nIn this example we will create a workflow that will be able to retrieve information about a user's preferences. We will do so by defining an `InMemoryStore` - an object that can store data in memory and query that data.\n\nWhen storing objects using the `Store` interface you define two things:\n\n* the namespace for the object, a tuple (similar to directories)\n* the object key (similar to filenames)\n\nIn our example, we'll be using `(\"memories\", <user_id>)` as namespace and random UUID as key for each new memory.\n\nImportantly, to determine the user, we will be passing `user_id` via the config keyword argument of the node function.\n\nLet's first define our store!\n\n\n```python\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain_openai import OpenAIEmbeddings\n\nin_memory_store = InMemoryStore(\n    index={\n        \"embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n        \"dims\": 1536,\n    }\n)\n```\n\n### Create workflow\n\n\n```python\nimport uuid\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.func import entrypoint, task\nfrom langgraph.graph import add_messages\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.store.base import BaseStore\n\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n\n@task\ndef call_model(messages: list[BaseMessage], memory_store: BaseStore, user_id: str):\n    namespace = (\"memories\", user_id)\n    last_message = messages[-1]\n    memories = memory_store.search(namespace, query=str(last_message.content))\n    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n    # Store new memories if the user asks the model to remember\n    if \"remember\" in last_message.content.lower():\n        memory = \"User name is Bob\"\n        memory_store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n    response = model.invoke([{\"role\": \"system\", \"content\": system_msg}] + messages)\n    return response\n\n\n# NOTE: we're passing the store object here when creating a workflow via entrypoint()\n@entrypoint(checkpointer=InMemorySaver(), store=in_memory_store)\ndef workflow(\n    inputs: list[BaseMessage],\n    *,\n    previous: list[BaseMessage],\n    config: RunnableConfig,\n    store: BaseStore,\n):\n    user_id = config[\"configurable\"][\"user_id\"]\n    previous = previous or []\n    inputs = add_messages(previous, inputs)\n    response = call_model(inputs, store, user_id).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n```\n\n!!! note Note\n\n    If you're using LangGraph Cloud or LangGraph Studio, you __don't need__ to pass store to the entrypoint decorator, since it's done automatically.\n\n### Run the workflow!\n\nNow let's specify a user ID in the config and tell the model our name:\n\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n```\n```output\n================================== Ai Message ==================================\n\nHello Bob! Nice to meet you. I'll remember that your name is Bob. How can I help you today?\n```\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n```\n```output\n================================== Ai Message ==================================\n\nYour name is Bob.\n```\nWe can now inspect our in-memory store and verify that we have in fact saved the memories for the user:\n\n\n```python\nfor memory in in_memory_store.search((\"memories\", \"1\")):\n    print(memory.value)\n```\n```output\n{'data': 'User name is Bob'}\n```\nLet's now run the workflow for another user to verify that the memories about the first user are self contained:\n\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"2\"}}\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n```\n```output\n================================== Ai Message ==================================\n\nI don't have any information about your name. I can only see our current conversation without any prior context or personal details about you. If you'd like me to know your name, feel free to tell me!\n```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 7587,
    "word_count": 868
  },
  {
    "title": "How to build a multi-agent network (functional API)",
    "source": "how-tos/multi-agent-network-functional.ipynb",
    "content": "# How to build a multi-agent network (functional API)\n\n!!! info \"Prerequisites\" \n    This guide assumes familiarity with the following:\n\n    - <a href=\"../../concepts/multi_agent\">Multi-agent systems</a>\n    - <a href=\"../../concepts/functional_api\">Functional API</a>\n    - <a href=\"../../concepts/low_level/#command\">Command</a>\n    - <a href=\"../../concepts/low_level/\">LangGraph Glossary</a>\n\nIn this how-to guide we will demonstrate how to implement a <a href=\"../../concepts/multi_agent#network\">multi-agent network</a> architecture where each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. We will be using <a href=\"../../concepts/functional_api\">functional API</a> — individual agents will be defined as tasks and the agent handoffs will be defined in the main [entrypoint()][langgraph.func.entrypoint]:\n\n```python\nfrom langgraph.func import entrypoint\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.tools import tool\n\n\n# Define a tool to signal intent to hand off to a different agent\n@tool(return_direct=True)\ndef transfer_to_hotel_advisor():\n    \"\"\"Ask hotel advisor agent for help.\"\"\"\n    return \"Successfully transferred to hotel advisor\"\n\n\n# define an agent\ntravel_advisor_tools = [transfer_to_hotel_advisor, ...]\ntravel_advisor = create_react_agent(model, travel_advisor_tools)\n\n\n# define a task that calls an agent\n@task\ndef call_travel_advisor(messages):\n    response = travel_advisor.invoke({\"messages\": messages})\n    return response[\"messages\"]\n\n\n# define the multi-agent network workflow\n@entrypoint()\ndef workflow(messages):\n    call_active_agent = call_travel_advisor\n    while True:\n        agent_messages = call_active_agent(messages).result()\n        messages = messages + agent_messages\n        call_active_agent = get_next_agent(messages)\n    return messages\n```\n\n## Setup\n\nFirst, let's install the required packages\n\n\n```shell\npip install -U langgraph langchain-anthropic\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n```\n```output\nANTHROPIC_API_KEY:  ········\n```\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n    <p style=\"padding-top: 5px;\">\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n    </p>\n</div>\n\n## Travel agent example\n\nIn this example we will build a team of travel assistant agents that can communicate with each other.\n\nWe will create 2 agents:\n\n* `travel_advisor`: can help with travel destination recommendations. Can ask `hotel_advisor` for help.\n* `hotel_advisor`: can help with hotel recommendations. Can ask `travel_advisor` for help.\n\nThis is a fully-connected network - every agent can talk to any other agent. \n\nFirst, let's create some of the tools that the agents will be using:\n\n\n```python\nimport random\nfrom typing_extensions import Literal\nfrom langchain_core.tools import tool\n\n\n@tool\ndef get_travel_recommendations():\n    \"\"\"Get recommendation for travel destinations\"\"\"\n    return random.choice([\"aruba\", \"turks and caicos\"])\n\n\n@tool\ndef get_hotel_recommendations(location: Literal[\"aruba\", \"turks and caicos\"]):\n    \"\"\"Get hotel recommendations for a given destination.\"\"\"\n    return {\n        \"aruba\": [\n            \"The Ritz-Carlton, Aruba (Palm Beach)\"\n            \"Bucuti & Tara Beach Resort (Eagle Beach)\"\n        ],\n        \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"],\n    }[location]\n\n\n@tool(return_direct=True)\ndef transfer_to_hotel_advisor():\n    \"\"\"Ask hotel advisor agent for help.\"\"\"\n    return \"Successfully transferred to hotel advisor\"\n\n\n@tool(return_direct=True)\ndef transfer_to_travel_advisor():\n    \"\"\"Ask travel advisor agent for help.\"\"\"\n    return \"Successfully transferred to travel advisor\"\n```\n\n!!! note \"Transfer tools\"\n\n    You might have noticed that we're using `@tool(return_direct=True)` in the transfer tools. This is done so that individual agents (e.g., `travel_advisor`) can exit the ReAct loop early once these tools are called. This is the desired behavior, as we want to detect when the agent calls this tool and hand control off _immediately_ to a different agent. \n    \n    **NOTE**: This is meant to work with the prebuilt [`create_react_agent`][langgraph.prebuilt.chat_agent_executor.create_react_agent] -- if you are building a custom agent, make sure to manually add logic for handling early exit for tools that are marked with `return_direct`.\n\nNow let's define our agent tasks and combine them into a single multi-agent network workflow:\n\n\n```python\nfrom langchain_core.messages import AIMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n# Define travel advisor ReAct agent\ntravel_advisor_tools = [\n    get_travel_recommendations,\n    transfer_to_hotel_advisor,\n]\ntravel_advisor = create_react_agent(\n    model,\n    travel_advisor_tools,\n    prompt=(\n        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n        \"If you need hotel recommendations, ask 'hotel_advisor' for help. \"\n        \"You MUST include human-readable response before transferring to another agent.\"\n    ),\n)\n\n\n@task\ndef call_travel_advisor(messages):\n    # You can also add additional logic like changing the input to the agent / output from the agent, etc.\n    # NOTE: we're invoking the ReAct agent with the full history of messages in the state\n    response = travel_advisor.invoke({\"messages\": messages})\n    return response[\"messages\"]\n\n\n# Define hotel advisor ReAct agent\nhotel_advisor_tools = [get_hotel_recommendations, transfer_to_travel_advisor]\nhotel_advisor = create_react_agent(\n    model,\n    hotel_advisor_tools,\n    prompt=(\n        \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\n        \"If you need help picking travel destinations, ask 'travel_advisor' for help.\"\n        \"You MUST include human-readable response before transferring to another agent.\"\n    ),\n)\n\n\n@task\ndef call_hotel_advisor(messages):\n    response = hotel_advisor.invoke({\"messages\": messages})\n    return response[\"messages\"]\n\n\n@entrypoint()\ndef workflow(messages):\n    messages = add_messages([], messages)\n\n    call_active_agent = call_travel_advisor\n    while True:\n        agent_messages = call_active_agent(messages).result()\n        messages = add_messages(messages, agent_messages)\n        ai_msg = next(m for m in reversed(agent_messages) if isinstance(m, AIMessage))\n        if not ai_msg.tool_calls:\n            break\n\n        tool_call = ai_msg.tool_calls[-1]\n        if tool_call[\"name\"] == \"transfer_to_travel_advisor\":\n            call_active_agent = call_travel_advisor\n        elif tool_call[\"name\"] == \"transfer_to_hotel_advisor\":\n            call_active_agent = call_hotel_advisor\n        else:\n            raise ValueError(f\"Expected transfer tool, got '{tool_call['name']}'\")\n\n    return messages\n```\n\nLastly, let's define a helper to render the agent outputs:\n\n\n```python\nfrom langchain_core.messages import convert_to_messages\n\n\ndef pretty_print_messages(update):\n    if isinstance(update, tuple):\n        ns, update = update\n        # skip parent graph updates in the printouts\n        if len(ns) == 0:\n            return\n\n        graph_id = ns[-1].split(\":\")[0]\n        print(f\"Update from subgraph {graph_id}:\")\n        print(\"\\n\")\n\n    for node_name, node_update in update.items():\n        print(f\"Update from node {node_name}:\")\n        print(\"\\n\")\n\n        for m in convert_to_messages(node_update[\"messages\"]):\n            m.pretty_print()\n        print(\"\\n\")\n```\n\nLet's test it out using the same input as our original multi-agent system:\n\n\n```python\nfor chunk in workflow.stream(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": \"i wanna go somewhere warm in the caribbean. pick one destination and give me hotel recommendations\",\n        }\n    ],\n    subgraphs=True,\n):\n    pretty_print_messages(chunk)\n```\n```output\nUpdate from subgraph call_travel_advisor:\n\n\nUpdate from node agent:\n\n\n================================== Ai Message ==================================\n\n[{'text': \"I'll help you find a warm Caribbean destination and then get some hotel recommendations for you.\\n\\nLet me first get some destination recommendations for the Caribbean region.\", 'type': 'text'}, {'id': 'toolu_015vT8PkPq1VXvjrDvSpWUwJ', 'input': {}, 'name': 'get_travel_recommendations', 'type': 'tool_use'}]\nTool Calls:\n  get_travel_recommendations (toolu_015vT8PkPq1VXvjrDvSpWUwJ)\n Call ID: toolu_015vT8PkPq1VXvjrDvSpWUwJ\n  Args:\n\n\nUpdate from subgraph call_travel_advisor:\n\n\nUpdate from node tools:\n\n\n================================= Tool Message =================================\nName: get_travel_recommendations\n\nturks and caicos\n\n\nUpdate from subgraph call_travel_advisor:\n\n\nUpdate from node agent:\n\n\n================================== Ai Message ==================================\n\n[{'text': \"Based on the recommendation, I suggest Turks and Caicos! This beautiful British Overseas Territory is known for its stunning white-sand beaches, crystal-clear turquoise waters, and year-round warm weather. Grace Bay Beach in Providenciales is consistently ranked among the world's best beaches. The islands offer excellent snorkeling, diving, and water sports opportunities, plus a relaxed Caribbean atmosphere.\\n\\nNow, let me connect you with our hotel advisor to get some specific hotel recommendations for Turks and Caicos.\", 'type': 'text'}, {'id': 'toolu_01JY7pNNWFuaWoe9ymxFYiPV', 'input': {}, 'name': 'transfer_to_hotel_advisor', 'type': 'tool_use'}]\nTool Calls:\n  transfer_to_hotel_advisor (toolu_01JY7pNNWFuaWoe9ymxFYiPV)\n Call ID: toolu_01JY7pNNWFuaWoe9ymxFYiPV\n  Args:\n\n\nUpdate from subgraph call_travel_advisor:\n\n\nUpdate from node tools:\n\n\n================================= Tool Message =================================\nName: transfer_to_hotel_advisor\n\nSuccessfully transferred to hotel advisor\n\n\nUpdate from subgraph call_hotel_advisor:\n\n\nUpdate from node agent:\n\n\n================================== Ai Message ==================================\n\n[{'text': 'Let me get some hotel recommendations for Turks and Caicos:', 'type': 'text'}, {'id': 'toolu_0129ELa7jFocn16bowaGNapg', 'input': {'location': 'turks and caicos'}, 'name': 'get_hotel_recommendations', 'type': 'tool_use'}]\nTool Calls:\n  get_hotel_recommendations (toolu_0129ELa7jFocn16bowaGNapg)\n Call ID: toolu_0129ELa7jFocn16bowaGNapg\n  Args:\n    location: turks and caicos\n\n\nUpdate from subgraph call_hotel_advisor:\n\n\nUpdate from node tools:\n\n\n================================= Tool Message =================================\nName: get_hotel_recommendations\n\n[\"Grace Bay Club\", \"COMO Parrot Cay\"]\n\n\nUpdate from subgraph call_hotel_advisor:\n\n\nUpdate from node agent:\n\n\n================================== Ai Message ==================================\n\nHere are two excellent hotel options in Turks and Caicos:\n\n1. Grace Bay Club: This luxury resort is located on the world-famous Grace Bay Beach. It offers all-oceanfront suites, exceptional dining options, and personalized service. The resort features adult-only and family-friendly sections, making it perfect for any type of traveler.\n\n2. COMO Parrot Cay: This exclusive private island resort offers the ultimate luxury escape. It's known for its pristine beach, world-class spa, and holistic wellness programs. The resort provides an intimate, secluded experience with top-notch amenities and service.\n\nWould you like more specific information about either of these properties or would you like to explore hotels in another destination?\n```\nVoila - `travel_advisor` picks a destination and then makes a decision to call `hotel_advisor` for more info!",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 12269,
    "word_count": 1336
  },
  {
    "title": "How to add multi-turn conversation in a multi-agent application (functional API)",
    "source": "how-tos/multi-agent-multi-turn-convo-functional.ipynb",
    "content": "# How to add multi-turn conversation in a multi-agent application (functional API)\n\n!!! info \"Prerequisites\"\n    This guide assumes familiarity with the following:\n\n    - <a href=\"../../concepts/multi_agent\">Multi-agent systems</a>\n    - <a href=\"../../concepts/human_in_the_loop\">Human-in-the-loop</a>\n    - <a href=\"../../concepts/functional_api\">Functional API</a>\n    - <a href=\"../../concepts/low_level/#command\">Command</a>\n    - <a href=\"../../concepts/low_level/\">LangGraph Glossary</a>\n\n\nIn this how-to guide, we’ll build an application that allows an end-user to engage in a *multi-turn conversation* with one or more agents. We'll create a node that uses an <a href=\"../../reference/types/#langgraph.types.interrupt\">`interrupt`</a> to collect user input and routes back to the **active** agent.\n\nThe agents will be implemented as tasks in a workflow that executes agent steps and determines the next action:\n\n1. **Wait for user input** to continue the conversation, or\n2. **Route to another agent** (or back to itself, such as in a loop) via a <a href=\"../../concepts/multi_agent/#handoffs\">**handoff**</a>.\n\n```python\nfrom langgraph.func import entrypoint, task\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.tools import tool\nfrom langgraph.types import interrupt\n\n\n# Define a tool to signal intent to hand off to a different agent\n# Note: this is not using Command(goto) syntax for navigating to different agents:\n# `workflow()` below handles the handoffs explicitly\n@tool(return_direct=True)\ndef transfer_to_hotel_advisor():\n    \"\"\"Ask hotel advisor agent for help.\"\"\"\n    return \"Successfully transferred to hotel advisor\"\n\n\n# define an agent\ntravel_advisor_tools = [transfer_to_hotel_advisor, ...]\ntravel_advisor = create_react_agent(model, travel_advisor_tools)\n\n\n# define a task that calls an agent\n@task\ndef call_travel_advisor(messages):\n    response = travel_advisor.invoke({\"messages\": messages})\n    return response[\"messages\"]\n\n\n# define the multi-agent network workflow\n@entrypoint(checkpointer)\ndef workflow(messages):\n    call_active_agent = call_travel_advisor\n    while True:\n        agent_messages = call_active_agent(messages).result()\n        ai_msg = get_last_ai_msg(agent_messages)\n        if not ai_msg.tool_calls:\n            user_input = interrupt(value=\"Ready for user input.\")\n            messages = messages + [{\"role\": \"user\", \"content\": user_input}]\n            continue\n\n        messages = messages + agent_messages\n        call_active_agent = get_next_agent(messages)\n    return entrypoint.final(value=agent_messages[-1], save=messages)\n```\n\n## Setup\n\nFirst, let's install the required packages\n\n\n```python\n# %%capture --no-stderr\n# %pip install -U langgraph langchain-anthropic\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n```\n```output\nANTHROPIC_API_KEY:  ········\n```\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n    <p style=\"padding-top: 5px;\">\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n    </p>\n</div>\n\nIn this example we will build a team of travel assistant agents that can communicate with each other.\n\nWe will create 2 agents:\n\n* `travel_advisor`: can help with travel destination recommendations. Can ask `hotel_advisor` for help.\n* `hotel_advisor`: can help with hotel recommendations. Can ask `travel_advisor` for help.\n\nThis is a fully-connected network - every agent can talk to any other agent. \n\n\n```python\nimport random\nfrom typing_extensions import Literal\nfrom langchain_core.tools import tool\n\n\n@tool\ndef get_travel_recommendations():\n    \"\"\"Get recommendation for travel destinations\"\"\"\n    return random.choice([\"aruba\", \"turks and caicos\"])\n\n\n@tool\ndef get_hotel_recommendations(location: Literal[\"aruba\", \"turks and caicos\"]):\n    \"\"\"Get hotel recommendations for a given destination.\"\"\"\n    return {\n        \"aruba\": [\n            \"The Ritz-Carlton, Aruba (Palm Beach)\"\n            \"Bucuti & Tara Beach Resort (Eagle Beach)\"\n        ],\n        \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"],\n    }[location]\n\n\n@tool(return_direct=True)\ndef transfer_to_hotel_advisor():\n    \"\"\"Ask hotel advisor agent for help.\"\"\"\n    return \"Successfully transferred to hotel advisor\"\n\n\n@tool(return_direct=True)\ndef transfer_to_travel_advisor():\n    \"\"\"Ask travel advisor agent for help.\"\"\"\n    return \"Successfully transferred to travel advisor\"\n```\n\n!!! note \"Transfer tools\"\n\n    You might have noticed that we're using `@tool(return_direct=True)` in the transfer tools. This is done so that individual agents (e.g., `travel_advisor`) can exit the ReAct loop early once these tools are called. This is the desired behavior, as we want to detect when the agent calls this tool and hand control off _immediately_ to a different agent. \n    \n    **NOTE**: This is meant to work with the prebuilt [`create_react_agent`][langgraph.prebuilt.chat_agent_executor.create_react_agent] -- if you are building a custom agent, make sure to manually add logic for handling early exit for tools that are marked with `return_direct`.\n\nLet's now create our agents using the prebuilt [`create_react_agent`][langgraph.prebuilt.chat_agent_executor.create_react_agent] and our multi-agent workflow. Note that will be calling [`interrupt`][langgraph.types.interrupt] every time after we get the final response from each of the agents.\n\n\n```python\nimport uuid\n\nfrom langchain_core.messages import AIMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import interrupt, Command\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n# Define travel advisor ReAct agent\ntravel_advisor_tools = [\n    get_travel_recommendations,\n    transfer_to_hotel_advisor,\n]\ntravel_advisor = create_react_agent(\n    model,\n    travel_advisor_tools,\n    prompt=(\n        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n        \"If you need hotel recommendations, ask 'hotel_advisor' for help. \"\n        \"You MUST include human-readable response before transferring to another agent.\"\n    ),\n)\n\n\n@task\ndef call_travel_advisor(messages):\n    # You can also add additional logic like changing the input to the agent / output from the agent, etc.\n    # NOTE: we're invoking the ReAct agent with the full history of messages in the state\n    response = travel_advisor.invoke({\"messages\": messages})\n    return response[\"messages\"]\n\n\n# Define hotel advisor ReAct agent\nhotel_advisor_tools = [get_hotel_recommendations, transfer_to_travel_advisor]\nhotel_advisor = create_react_agent(\n    model,\n    hotel_advisor_tools,\n    prompt=(\n        \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\n        \"If you need help picking travel destinations, ask 'travel_advisor' for help.\"\n        \"You MUST include human-readable response before transferring to another agent.\"\n    ),\n)\n\n\n@task\ndef call_hotel_advisor(messages):\n    response = hotel_advisor.invoke({\"messages\": messages})\n    return response[\"messages\"]\n\n\ncheckpointer = InMemorySaver()\n\n\ndef string_to_uuid(input_string):\n    return str(uuid.uuid5(uuid.NAMESPACE_URL, input_string))\n\n\n@entrypoint(checkpointer=checkpointer)\ndef multi_turn_graph(messages, previous):\n    previous = previous or []\n    messages = add_messages(previous, messages)\n    call_active_agent = call_travel_advisor\n    while True:\n        agent_messages = call_active_agent(messages).result()\n        messages = add_messages(messages, agent_messages)\n        # Find the last AI message\n        # If one of the handoff tools is called, the last message returned\n        # by the agent will be a ToolMessage because we set them to have\n        # \"return_direct=True\". This means that the last AIMessage will\n        # have tool calls.\n        # Otherwise, the last returned message will be an AIMessage with\n        # no tool calls, which means we are ready for new input.\n        ai_msg = next(m for m in reversed(agent_messages) if isinstance(m, AIMessage))\n        if not ai_msg.tool_calls:\n            user_input = interrupt(value=\"Ready for user input.\")\n            # Add user input as a human message\n            # NOTE: we generate unique ID for the human message based on its content\n            # it's important, since on subsequent invocations previous user input (interrupt) values\n            # will be looked up again and we will attempt to add them again here\n            # `add_messages` deduplicates messages based on the ID, ensuring correct message history\n            human_message = {\n                \"role\": \"user\",\n                \"content\": user_input,\n                \"id\": string_to_uuid(user_input),\n            }\n            messages = add_messages(messages, [human_message])\n            continue\n\n        tool_call = ai_msg.tool_calls[-1]\n        if tool_call[\"name\"] == \"transfer_to_hotel_advisor\":\n            call_active_agent = call_hotel_advisor\n        elif tool_call[\"name\"] == \"transfer_to_travel_advisor\":\n            call_active_agent = call_travel_advisor\n        else:\n            raise ValueError(f\"Expected transfer tool, got '{tool_call['name']}'\")\n\n    return entrypoint.final(value=agent_messages[-1], save=messages)\n```\n\n## Test multi-turn conversation\n\nLet's test a multi turn conversation with this application.\n\n\n```python\nthread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n\ninputs = [\n    # 1st round of conversation,\n    {\n        \"role\": \"user\",\n        \"content\": \"i wanna go somewhere warm in the caribbean\",\n        \"id\": str(uuid.uuid4()),\n    },\n    # Since we're using `interrupt`, we'll need to resume using the Command primitive.\n    # 2nd round of conversation,\n    Command(\n        resume=\"could you recommend a nice hotel in one of the areas and tell me which area it is.\"\n    ),\n    # 3rd round of conversation,\n    Command(\n        resume=\"i like the first one. could you recommend something to do near the hotel?\"\n    ),\n]\n\nfor idx, user_input in enumerate(inputs):\n    print()\n    print(f\"--- Conversation Turn {idx + 1} ---\")\n    print()\n    print(f\"User: {user_input}\")\n    print()\n    for update in multi_turn_graph.stream(\n        user_input,\n        config=thread_config,\n        stream_mode=\"updates\",\n    ):\n        for node_id, value in update.items():\n            if isinstance(value, list) and value:\n                last_message = value[-1]\n                if isinstance(last_message, dict) or last_message.type != \"ai\":\n                    continue\n                print(f\"{node_id}: {last_message.content}\")\n```\n```output\n--- Conversation Turn 1 ---\n\nUser: {'role': 'user', 'content': 'i wanna go somewhere warm in the caribbean', 'id': 'f48d82a7-7efa-43f5-ad4c-541758c95f61'}\n\ncall_travel_advisor: Based on the recommendations, Aruba would be an excellent choice for your Caribbean getaway! Known as \"One Happy Island,\" Aruba offers:\n- Year-round warm weather with consistent temperatures around 82°F (28°C)\n- Beautiful white sand beaches like Eagle Beach and Palm Beach\n- Crystal clear waters perfect for swimming and snorkeling\n- Minimal rainfall and location outside the hurricane belt\n- Rich culture blending Dutch and Caribbean influences\n- Various activities from water sports to desert-like landscape exploration\n- Excellent dining and shopping options\n\nWould you like me to help you find suitable accommodations in Aruba? I can transfer you to our hotel advisor who can recommend specific hotels based on your preferences.\n\n--- Conversation Turn 2 ---\n\nUser: Command(resume='could you recommend a nice hotel in one of the areas and tell me which area it is.')\n\ncall_hotel_advisor: I can recommend two excellent options in different areas:\n\n1. The Ritz-Carlton, Aruba - Located in Palm Beach\n- Luxury beachfront resort\n- Located in the vibrant Palm Beach area, known for its lively atmosphere\n- Close to restaurants, shopping, and nightlife\n- Perfect for those who want a more active vacation with plenty of amenities nearby\n\n2. Bucuti & Tara Beach Resort - Located in Eagle Beach\n- Adults-only boutique resort\n- Situated on the quieter Eagle Beach\n- Known for its romantic atmosphere and excellent service\n- Ideal for couples seeking a more peaceful, intimate setting\n\nWould you like more specific information about either of these properties or their locations?\n\n--- Conversation Turn 3 ---\n\nUser: Command(resume='i like the first one. could you recommend something to do near the hotel?')\n\ncall_travel_advisor: Near The Ritz-Carlton in Palm Beach, here are some popular activities you can enjoy:\n\n1. Palm Beach Strip - Take a walk along this bustling strip filled with restaurants, shops, and bars\n2. Visit the Bubali Bird Sanctuary - Just a short distance away\n3. Try your luck at the Stellaris Casino - Located right in The Ritz-Carlton\n4. Water Sports at Palm Beach - Right in front of the hotel you can:\n   - Go parasailing\n   - Try jet skiing\n   - Take a sunset sailing cruise\n5. Visit the Palm Beach Plaza Mall - High-end shopping just a short walk away\n6. Enjoy dinner at Madame Janette's - One of Aruba's most famous restaurants nearby\n\nWould you like more specific information about any of these activities or other suggestions in the area?\n```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 13841,
    "word_count": 1662
  },
  {
    "title": "How to add thread-level persistence (functional API)",
    "source": "how-tos/persistence-functional.ipynb",
    "content": "# How to add thread-level persistence (functional API)\n\n!!! info \"Prerequisites\"\n\n    This guide assumes familiarity with the following:\n    \n    - <a href=\"../../concepts/functional_api/\">Functional API</a>\n    - <a href=\"../../concepts/persistence/\">Persistence</a>\n    - <a href=\"../../concepts/memory/\">Memory</a>\n    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n\n!!! info \"Not needed for LangGraph API users\"\n\n    If you're using the LangGraph API, you needn't manually implement a checkpointer. The API automatically handles checkpointing for you. This guide is relevant when implementing LangGraph in your own custom server.\n\nMany AI applications need memory to share context across multiple interactions on the same <a href=\"../../concepts/persistence#threads\">thread</a> (e.g., multiple turns of a conversation). In LangGraph functional API, this kind of memory can be added to any [entrypoint()][langgraph.func.entrypoint] workflow using [thread-level persistence](https://langchain-ai.github.io/langgraph/concepts/persistence).\n\nWhen creating a LangGraph workflow, you can set it up to persist its results by using a [checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#basecheckpointsaver):\n\n\n1. Create an instance of a checkpointer:\n\n    ```python\n    from langgraph.checkpoint.memory import InMemorySaver\n    \n    checkpointer = InMemorySaver()       \n    ```\n\n2. Pass `checkpointer` instance to the `entrypoint()` decorator:\n\n    ```python\n    from langgraph.func import entrypoint\n    \n    @entrypoint(checkpointer=checkpointer)\n    def workflow(inputs)\n        ...\n    ```\n\n3. Optionally expose `previous` parameter in the workflow function signature:\n\n    ```python\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(\n        inputs,\n        *,\n        # you can optionally specify `previous` in the workflow function signature\n        # to access the return value from the workflow as of the last execution\n        previous\n    ):\n        previous = previous or []\n        combined_inputs = previous + inputs\n        result = do_something(combined_inputs)\n        ...\n    ```\n\n4. Optionally choose which values will be returned from the workflow and which will be saved by the checkpointer as `previous`:\n\n    ```python\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(inputs, *, previous):\n        ...\n        result = do_something(...)\n        return entrypoint.final(value=result, save=combine(inputs, result))\n    ```\n\nThis guide shows how you can add thread-level persistence to your workflow.\n\n!!! tip \"Note\"\n\n    If you need memory that is __shared__ across multiple conversations or users (cross-thread persistence), check out this <a href=\"../cross-thread-persistence-functional\">how-to guide</a>.\n\n!!! tip \"Note\"\n\n    If you need to add thread-level persistence to a `StateGraph`, check out this <a href=\"../persistence\">how-to guide</a>.\n\n## Setup\n\nFirst we need to install the packages required\n\n\n```shell\npip install --quiet -U langgraph langchain_anthropic\n```\n\nNext, we need to set API key for Anthropic (the LLM we will use).\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n```\n\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n    <p style=\"padding-top: 5px;\">\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n    </p>\n</div>\n\n## Example: simple chatbot with short-term memory\n\nWe will be using a workflow with a single task that calls a [chat model](https://python.langchain.com/docs/concepts/chat_models/).\n\nLet's first define the model we'll be using:\n\n\n```python\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n```\n\nNow we can define our task and workflow. To add in persistence, we need to pass in a [Checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) to the [entrypoint()][langgraph.func.entrypoint] decorator.\n\n\n```python\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\n@task\ndef call_model(messages: list[BaseMessage]):\n    response = model.invoke(messages)\n    return response\n\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):\n    if previous:\n        inputs = add_messages(previous, inputs)\n\n    response = call_model(inputs).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n```\n\nIf we try to use this workflow, the context of the conversation will be persisted across interactions:\n\n!!! note Note\n\n    If you're using LangGraph Platform or LangGraph Studio, you __don't need__ to pass checkpointer to the entrypoint decorator, since it's done automatically.\n\nWe can now interact with the agent and see that it remembers previous messages!\n\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n```\n```output\n================================== Ai Message ==================================\n\nHi Bob! I'm Claude. Nice to meet you! How are you today?\n```\nYou can always resume previous threads:\n\n\n```python\ninput_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n```\n```output\n================================== Ai Message ==================================\n\nYour name is Bob.\n```\nIf we want to start a new conversation, we can pass in a different `thread_id`. Poof! All the memories are gone!\n\n\n```python\ninput_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in workflow.stream(\n    [input_message],\n    {\"configurable\": {\"thread_id\": \"2\"}},\n    stream_mode=\"values\",\n):\n    chunk.pretty_print()\n```\n```output\n================================== Ai Message ==================================\n\nI don't know your name unless you tell me. Each conversation I have starts fresh, so I don't have access to any previous interactions or personal information unless you share it with me.\n```\n!!! tip \"Streaming tokens\"\n\n    If you would like to stream LLM tokens from your chatbot, you can use `stream_mode=\"messages\"`. Check out this <a href=\"../streaming-tokens\">how-to guide</a> to learn more.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 6987,
    "word_count": 769
  },
  {
    "title": "How to manage conversation history in a ReAct Agent",
    "source": "how-tos/create-react-agent-manage-message-history.ipynb",
    "content": "# How to manage conversation history in a ReAct Agent\n\n!!! info \"Prerequisites\"\n    This guide assumes familiarity with the following:\n\n    - <a href=\"../create-react-agent\">Prebuilt create_react_agent</a>\n    - <a href=\"../../concepts/persistence\">Persistence</a>\n    - <a href=\"../../concepts/memory/#short-term-memory\">Short-term Memory</a>\n    - [Trimming Messages](https://python.langchain.com/docs/how_to/trim_messages/)\n\nMessage history can grow quickly and exceed LLM context window size, whether you're building chatbots with many conversation turns or agentic systems with numerous tool calls. There are several strategies for managing the message history:\n\n* <a href=\"#keep-the-original-message-history-unmodified\">message trimming</a> — remove first or last N messages in the history\n* <a href=\"#summarizing-message-history\">summarization</a> — summarize earlier messages in the history and replace them with a summary\n* custom strategies (e.g., message filtering, etc.)\n\nTo manage message history in `create_react_agent`, you need to define a `pre_model_hook` function or [runnable](https://python.langchain.com/docs/concepts/runnables/) that takes graph state an returns a state update:\n\n\n* Trimming example:\n    ```python hl_lines=\"1 2 3 4 19 25\"\n    from langchain_core.messages.utils import (\n        trim_messages, \n        count_tokens_approximately\n    )\n    from langgraph.prebuilt import create_react_agent\n    \n    # This function will be called every time before the node that calls LLM\n    def pre_model_hook(state):\n        trimmed_messages = trim_messages(\n            state[\"messages\"],\n            strategy=\"last\",\n            token_counter=count_tokens_approximately,\n            max_tokens=384,\n            start_on=\"human\",\n            end_on=(\"human\", \"tool\"),\n        )\n        # You can return updated messages either under `llm_input_messages` or \n        # `messages` key (see the note below)\n        return {\"llm_input_messages\": trimmed_messages}\n\n    checkpointer = InMemorySaver()\n    agent = create_react_agent(\n        model,\n        tools,\n        pre_model_hook=pre_model_hook,\n        checkpointer=checkpointer,\n    )\n    ```\n\n* Summarization example:\n    ```python hl_lines=\"1 20 27 28\"\n    from langmem.short_term import SummarizationNode\n    from langchain_core.messages.utils import count_tokens_approximately\n    from langgraph.prebuilt.chat_agent_executor import AgentState\n    from langgraph.checkpoint.memory import InMemorySaver\n    from typing import Any\n    \n    model = ChatOpenAI(model=\"gpt-4o\")\n    \n    summarization_node = SummarizationNode(\n        token_counter=count_tokens_approximately,\n        model=model,\n        max_tokens=384,\n        max_summary_tokens=128,\n        output_messages_key=\"llm_input_messages\",\n    )\n\n    class State(AgentState):\n        # NOTE: we're adding this key to keep track of previous summary information\n        # to make sure we're not summarizing on every LLM call\n        context: dict[str, Any]\n    \n    \n    checkpointer = InMemorySaver()\n    graph = create_react_agent(\n        model,\n        tools,\n        pre_model_hook=summarization_node,\n        state_schema=State,\n        checkpointer=checkpointer,\n    )\n    ```\n\n!!! Important\n    \n    * To **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**, return updated messages under `llm_input_messages` key\n    * To **overwrite the original message history** in the graph state with the updated history, return updated messages under `messages` key\n    \n    To overwrite the `messages` key, you need to do the following:\n\n    ```python\n    from langchain_core.messages import RemoveMessage\n    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n\n    def pre_model_hook(state):\n        updated_messages = ...\n        return {\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *updated_messages]\n            ...\n        }\n    ```\n\n## Setup\n\nFirst, let's install the required packages and set our API keys\n\n\n```shell\npip install -U langgraph langchain-openai langmem\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n    <p style=\"padding-top: 5px;\">\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n    </p>\n</div>\n\n## Keep the original message history unmodified\n\nLet's build a ReAct agent with a step that manages the conversation history: when the length of the history exceeds a specified number of tokens, we will call [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) utility that that will reduce the history while satisfying LLM provider constraints.\n\nThere are two ways that the updated message history can be applied inside ReAct agent:\n\n  * <a href=\"#keep-the-original-message-history-unmodified\">**Keep the original message history unmodified**</a> in the graph state and pass the updated history **only as the input to the LLM**\n  * <a href=\"#overwrite-the-original-message-history\">**Overwrite the original message history**</a> in the graph state with the updated history\n\nLet's start by implementing the first one. We'll need to first define model and tools for our agent:\n\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n\ndef get_weather(location: str) -> str:\n    \"\"\"Use this to get weather information.\"\"\"\n    if any([city in location.lower() for city in [\"nyc\", \"new york city\"]]):\n        return \"It might be cloudy in nyc, with a chance of rain and temperatures up to 80 degrees.\"\n    elif any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n        return \"It's always sunny in sf\"\n    else:\n        return f\"I am not sure what the weather is in {location}\"\n\n\ntools = [get_weather]\n```\n\nNow let's implement `pre_model_hook` — a function that will be added as a new node and called every time **before** the node that calls the LLM (the `agent` node).\n\nOur implementation will wrap the `trim_messages` call and return the trimmed messages under `llm_input_messages`. This will **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**\n\n\n```python hl_lines=\"4 5 6 7 22 29\"\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nfrom langchain_core.messages.utils import (\n    trim_messages,\n    count_tokens_approximately,\n)\n\n\n# This function will be added as a new node in ReAct agent graph\n# that will run every time before the node that calls the LLM.\n# The messages returned by this function will be the input to the LLM.\ndef pre_model_hook(state):\n    trimmed_messages = trim_messages(\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=384,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    return {\"llm_input_messages\": trimmed_messages}\n\n\ncheckpointer = InMemorySaver()\ngraph = create_react_agent(\n    model,\n    tools,\n    pre_model_hook=pre_model_hook,\n    checkpointer=checkpointer,\n)\n```\n\n\n```python\nfrom IPython.display import display, Image\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<p>\n\n</p>\n\nWe'll also define a utility to render the agent outputs nicely:\n\n\n```python\ndef print_stream(stream, output_messages_key=\"llm_input_messages\"):\n    for chunk in stream:\n        for node, update in chunk.items():\n            print(f\"Update from node: {node}\")\n            messages_key = (\n                output_messages_key if node == \"pre_model_hook\" else \"messages\"\n            )\n            for message in update[messages_key]:\n                if isinstance(message, tuple):\n                    print(message)\n                else:\n                    message.pretty_print()\n\n        print(\"\\n\\n\")\n```\n\nNow let's run the agent with a few different queries to reach the specified max tokens limit:\n\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\ninputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\nresult = graph.invoke(inputs, config=config)\n\ninputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\nresult = graph.invoke(inputs, config=config)\n```\n\nLet's see how many tokens we have in the message history so far:\n\n\n```python\nmessages = result[\"messages\"]\ncount_tokens_approximately(messages)\n```\n\n\n\n```output\n415\n```\n\n\nYou can see that we are close to the `max_tokens` threshold, so on the next invocation we should see `pre_model_hook` kick-in and trim the message history. Let's run it again:\n\n\n```python\ninputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\nprint_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))\n```\n```output\nUpdate from node: pre_model_hook\n================================ Human Message =================================\n\nWhat's it known for?\n================================== Ai Message ==================================\n\nNew York City is known for a variety of iconic landmarks, cultural institutions, and vibrant neighborhoods. Some of the most notable features include:\n\n1. **Statue of Liberty**: A symbol of freedom and democracy, located on Liberty Island.\n2. **Times Square**: Known for its bright lights, Broadway theaters, and bustling atmosphere.\n3. **Central Park**: A large public park offering a natural retreat in the middle of the city.\n4. **Empire State Building**: An iconic skyscraper offering panoramic views of the city.\n5. **Broadway**: Famous for its world-class theater productions.\n6. **Wall Street**: The financial hub of the United States.\n7. **Museums**: Including the Metropolitan Museum of Art, Museum of Modern Art (MoMA), and the American Museum of Natural History.\n8. **Diverse Cuisine**: A melting pot of cultures offering a wide range of culinary experiences.\n9. **Cultural Diversity**: A rich tapestry of cultures and communities from around the world.\n10. **Fashion**: A global fashion capital, hosting events like New York Fashion Week.\n\nThese are just a few highlights of what makes New York City a unique and vibrant place.\n================================ Human Message =================================\n\nwhere can i find the best bagel?\n\n\n\nUpdate from node: agent\n================================== Ai Message ==================================\n\nNew York City is famous for its bagels, and there are several places renowned for serving some of the best. Here are a few top spots where you can find excellent bagels in NYC:\n\n1. **Ess-a-Bagel**: Known for their large, chewy bagels with a variety of spreads and toppings.\n2. **Russ & Daughters**: A classic spot offering traditional bagels with high-quality smoked fish and cream cheese.\n3. **H&H Bagels**: Famous for their fresh, hand-rolled bagels.\n4. **Murray’s Bagels**: Offers a wide selection of bagels and spreads, with a no-toasting policy to preserve freshness.\n5. **Absolute Bagels**: Known for their authentic, fluffy bagels and a variety of cream cheese options.\n6. **Tompkins Square Bagels**: Offers creative bagel sandwiches and a wide range of spreads.\n7. **Bagel Hole**: Known for their smaller, denser bagels with a crispy crust.\n\nEach of these places has its own unique style and flavor, so it might be worth trying a few to find your personal favorite!\n```\nYou can see that the `pre_model_hook` node now only returned the last 3 messages, as expected. However, the existing message history is untouched:\n\n\n```python\nupdated_messages = graph.get_state(config).values[\"messages\"]\nassert [(m.type, m.content) for m in updated_messages[: len(messages)]] == [\n    (m.type, m.content) for m in messages\n]\n```\n\n## Overwrite the original message history\n\nLet's now change the `pre_model_hook` to **overwrite** the message history in the graph state. To do this, we’ll return the updated messages under `messages` key. We’ll also include a special `RemoveMessage(REMOVE_ALL_MESSAGES)` object, which tells `create_react_agent` to remove previous messages from the graph state:\n\n\n```python hl_lines=\"16 23\"\nfrom langchain_core.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\n\n\ndef pre_model_hook(state):\n    trimmed_messages = trim_messages(\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=384,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    # NOTE that we're now returning the messages under the `messages` key\n    # We also remove the existing messages in the history to ensure we're overwriting the history\n    return {\"messages\": [RemoveMessage(REMOVE_ALL_MESSAGES)] + trimmed_messages}\n\n\ncheckpointer = InMemorySaver()\ngraph = create_react_agent(\n    model,\n    tools,\n    pre_model_hook=pre_model_hook,\n    checkpointer=checkpointer,\n)\n```\n\nNow let's run the agent with the same queries as before:\n\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\ninputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\nresult = graph.invoke(inputs, config=config)\n\ninputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\nresult = graph.invoke(inputs, config=config)\nmessages = result[\"messages\"]\n\ninputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\nprint_stream(\n    graph.stream(inputs, config=config, stream_mode=\"updates\"),\n    output_messages_key=\"messages\",\n)\n```\n```output\nUpdate from node: pre_model_hook\n================================ Remove Message ================================\n\n\n================================ Human Message =================================\n\nWhat's it known for?\n================================== Ai Message ==================================\n\nNew York City is known for a variety of iconic landmarks, cultural institutions, and vibrant neighborhoods. Some of the most notable features include:\n\n1. **Statue of Liberty**: A symbol of freedom and democracy, located on Liberty Island.\n2. **Times Square**: Known for its bright lights, Broadway theaters, and bustling atmosphere.\n3. **Central Park**: A large public park offering a natural oasis amidst the urban environment.\n4. **Empire State Building**: An iconic skyscraper offering panoramic views of the city.\n5. **Broadway**: Famous for its world-class theater productions and musicals.\n6. **Wall Street**: The financial hub of the United States, located in the Financial District.\n7. **Museums**: Including the Metropolitan Museum of Art, Museum of Modern Art (MoMA), and the American Museum of Natural History.\n8. **Diverse Cuisine**: A melting pot of cultures, offering a wide range of international foods.\n9. **Cultural Diversity**: Known for its diverse population and vibrant cultural scene.\n10. **Brooklyn Bridge**: An iconic suspension bridge connecting Manhattan and Brooklyn.\n\nThese are just a few highlights, as NYC is a city with endless attractions and activities.\n================================ Human Message =================================\n\nwhere can i find the best bagel?\n\n\n\nUpdate from node: agent\n================================== Ai Message ==================================\n\nNew York City is famous for its bagels, and there are several places renowned for serving some of the best. Here are a few top spots where you can find delicious bagels in NYC:\n\n1. **Ess-a-Bagel**: Known for its large, chewy bagels and a wide variety of spreads and toppings. Locations in Midtown and the East Village.\n\n2. **Russ & Daughters**: A historic appetizing store on the Lower East Side, famous for its bagels with lox and cream cheese.\n\n3. **Absolute Bagels**: Located on the Upper West Side, this spot is popular for its fresh, fluffy bagels.\n\n4. **Murray’s Bagels**: Known for its traditional, hand-rolled bagels. Located in Greenwich Village.\n\n5. **Tompkins Square Bagels**: Offers a wide selection of bagels and creative cream cheese flavors. Located in the East Village.\n\n6. **Bagel Hole**: A small shop in Park Slope, Brooklyn, known for its classic, no-frills bagels.\n\n7. **Leo’s Bagels**: Located in the Financial District, known for its authentic New York-style bagels.\n\nEach of these places has its own unique style and flavor, so it might be worth trying a few to find your personal favorite!\n```\nYou can see that the `pre_model_hook` node returned the last 3 messages again. However, this time, the message history is modified in the graph state as well:\n\n\n```python\nupdated_messages = graph.get_state(config).values[\"messages\"]\nassert (\n    # First 2 messages in the new history are the same as last 2 messages in the old\n    [(m.type, m.content) for m in updated_messages[:2]]\n    == [(m.type, m.content) for m in messages[-2:]]\n)\n```\n\n## Summarizing message history\n\nFinally, let's apply a different strategy for managing message history — summarization. Just as with trimming, you can choose to keep original message history unmodified or overwrite it. The example below will only show the former.\n\nWe will use the [`SummarizationNode`](https://langchain-ai.github.io/langmem/guides/summarization/#using-summarizationnode) from the prebuilt `langmem` library. Once the message history reaches the token limit, the summarization node will summarize earlier messages to make sure they fit into `max_tokens`.\n\n\n```python hl_lines=\"1 20 28 29\"\nfrom langmem.short_term import SummarizationNode\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom typing import Any\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nsummarization_model = model.bind(max_tokens=128)\n\nsummarization_node = SummarizationNode(\n    token_counter=count_tokens_approximately,\n    model=summarization_model,\n    max_tokens=384,\n    max_summary_tokens=128,\n    output_messages_key=\"llm_input_messages\",\n)\n\n\nclass State(AgentState):\n    # NOTE: we're adding this key to keep track of previous summary information\n    # to make sure we're not summarizing on every LLM call\n    context: dict[str, Any]\n\n\ncheckpointer = InMemorySaver()\ngraph = create_react_agent(\n    # limit the output size to ensure consistent behavior\n    model.bind(max_tokens=256),\n    tools,\n    pre_model_hook=summarization_node,\n    state_schema=State,\n    checkpointer=checkpointer,\n)\n```\n\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ninputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n\nresult = graph.invoke(inputs, config=config)\n\ninputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\nresult = graph.invoke(inputs, config=config)\n\ninputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\nprint_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))\n```\n```output\nUpdate from node: pre_model_hook\n================================ System Message ================================\n\nSummary of the conversation so far: The user asked about the current weather in New York City. In response, the assistant provided information that it might be cloudy, with a chance of rain, and temperatures reaching up to 80 degrees.\n================================ Human Message =================================\n\nWhat's it known for?\n================================== Ai Message ==================================\n\nNew York City, often referred to as NYC, is known for its:\n\n1. **Landmarks and Iconic Sites**:\n   - **Statue of Liberty**: A symbol of freedom and democracy.\n   - **Central Park**: A vast green oasis in the middle of the city.\n   - **Empire State Building**: Once the tallest building in the world, offering stunning views of the city.\n   - **Times Square**: Known for its bright lights and bustling atmosphere.\n\n2. **Cultural Institutions**:\n   - **Broadway**: Renowned for theatrical performances and musicals.\n   - **Metropolitan Museum of Art** and **Museum of Modern Art (MoMA)**: World-class art collections.\n   - **American Museum of Natural History**: Known for its extensive exhibits ranging from dinosaurs to space exploration.\n   \n3. **Diverse Neighborhoods and Cuisine**:\n   - NYC is famous for having a melting pot of cultures, reflected in neighborhoods like Chinatown, Little Italy, and Harlem.\n   - The city offers a wide range of international cuisines, from street food to high-end dining.\n\n4. **Financial District**:\n   - Home to Wall Street, the New York Stock Exchange (NYSE), and other major financial institutions.\n\n5. **Media and Entertainment**:\n   - Major hub for television, film, and media, with numerous studios and networks based there.\n\n6. **Fashion**:\n   - Often referred to as one of the \"Big Four\" fashion capitals, hosting events like New York Fashion Week.\n\n7. **Sports**:\n   - Known for its passionate sports culture with teams like the Yankees (MLB), Mets (MLB), Knicks (NBA), and Rangers (NHL).\n\nThese elements, among others, contribute to NYC's reputation as a vibrant and dynamic city.\n================================ Human Message =================================\n\nwhere can i find the best bagel?\n\n\n\nUpdate from node: agent\n================================== Ai Message ==================================\n\nFinding the best bagel in New York City can be subjective, as there are many beloved spots across the city. However, here are some renowned bagel shops you might want to try:\n\n1. **Ess-a-Bagel**: Known for its chewy and flavorful bagels, located in Midtown and Stuyvesant Town.\n\n2. **Bagel Hole**: A favorite for traditionalists, offering classic and dense bagels, located in Park Slope, Brooklyn.\n\n3. **Russ & Daughters**: A legendary appetizing store on the Lower East Side, famous for their bagels with lox.\n\n4. **Murray’s Bagels**: Located in Greenwich Village, known for their fresh and authentic New York bagels.\n\n5. **Absolute Bagels**: Located on the Upper West Side, they’re known for their fresh, fluffy bagels with a variety of spreads.\n\n6. **Tompkins Square Bagels**: In the East Village, famous for their creative cream cheese options and fresh bagels.\n\n7. **Zabar’s**: A landmark on the Upper West Side known for their classic bagels and smoked fish.\n\nEach of these spots offers a unique take on the classic New York bagel experience, and trying several might be the best way to discover your personal favorite!\n```\nYou can see that the earlier messages have now been replaced with the summary of the earlier conversation!",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 22734,
    "word_count": 2810
  },
  {
    "title": "How to integrate LangGraph (functional API) with AutoGen, CrewAI, and other frameworks",
    "source": "how-tos/autogen-integration-functional.ipynb",
    "content": "# How to integrate LangGraph (functional API) with AutoGen, CrewAI, and other frameworks\n\nLangGraph is a framework for building agentic and multi-agent applications. LangGraph can be easily integrated with other agent frameworks. \n\nThe primary reasons you might want to integrate LangGraph with other agent frameworks:\n\n- create <a href=\"../../concepts/multi_agent\">multi-agent systems</a> where individual agents are built with different frameworks\n- leverage LangGraph to add features like <a href=\"../../concepts/persistence\">persistence</a>, <a href=\"../../concepts/streaming\">streaming</a>, <a href=\"../../concepts/memory\">short and long-term memory</a> and more\n\nThe simplest way to integrate agents from other frameworks is by calling those agents inside a LangGraph <a href=\"../../concepts/low_level/#nodes\">node</a>:\n\n```python\nimport autogen\nfrom langgraph.func import entrypoint, task\n\nautogen_agent = autogen.AssistantAgent(name=\"assistant\", ...)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\", ...)\n\n@task\ndef call_autogen_agent(messages):\n    response = user_proxy.initiate_chat(\n        autogen_agent,\n        message=messages[-1],\n        ...\n    )\n    ...\n\n\n@entrypoint()\ndef workflow(messages):\n    response = call_autogen_agent(messages).result()\n    return response\n\n\nworkflow.invoke(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\",\n        }\n    ]\n)\n```\n\nIn this guide we show how to build a LangGraph chatbot that integrates with AutoGen, but you can follow the same approach with other frameworks.\n\n## Setup\n\n\n```python\n%pip install autogen langgraph\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n```output\nOPENAI_API_KEY:  ········\n```\n## Define AutoGen agent\n\nHere we define our AutoGen agent. Adapted from official tutorial [here](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb).\n\n\n```python\nimport autogen\nimport os\n\nconfig_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n\nllm_config = {\n    \"timeout\": 600,\n    \"cache_seed\": 42,\n    \"config_list\": config_list,\n    \"temperature\": 0,\n}\n\nautogen_agent = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config,\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n    code_execution_config={\n        \"work_dir\": \"web\",\n        \"use_docker\": False,\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n    llm_config=llm_config,\n    system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\n)\n```\n\n---\n\n## Create the workflow\n\nWe will now create a LangGraph chatbot graph that calls AutoGen agent.\n\n\n```python\nfrom langchain_core.messages import convert_to_openai_messages, BaseMessage\nfrom langgraph.func import entrypoint, task\nfrom langgraph.graph import add_messages\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\n@task\ndef call_autogen_agent(messages: list[BaseMessage]):\n    # convert to openai-style messages\n    messages = convert_to_openai_messages(messages)\n    response = user_proxy.initiate_chat(\n        autogen_agent,\n        message=messages[-1],\n        # pass previous message history as context\n        carryover=messages[:-1],\n    )\n    # get the final response from the agent\n    content = response.chat_history[-1][\"content\"]\n    return {\"role\": \"assistant\", \"content\": content}\n\n\n# add short-term memory for storing conversation history\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(messages: list[BaseMessage], previous: list[BaseMessage]):\n    messages = add_messages(previous or [], messages)\n    response = call_autogen_agent(messages).result()\n    return entrypoint.final(value=response, save=add_messages(messages, response))\n```\n\n## Run the graph\n\nWe can now run the graph.\n\n\n```python hl_lines=\"2 11\"\n# pass the thread ID to persist agent outputs for future interactions\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor chunk in workflow.stream(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\",\n        }\n    ],\n    config,\n):\n    print(chunk)\n```\n```output\nuser_proxy (to assistant):\n\nFind numbers between 10 and 30 in fibonacci sequence\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nTo find numbers between 10 and 30 in the Fibonacci sequence, we can generate the Fibonacci sequence and check which numbers fall within this range. Here's a plan:\n\n1. Generate Fibonacci numbers starting from 0.\n2. Continue generating until the numbers exceed 30.\n3. Collect and print the numbers that are between 10 and 30.\n\nLet's implement this in Python:\n\n\\`\\`\\`python\n# filename: fibonacci_range.py\n\ndef fibonacci_sequence():\n    a, b = 0, 1\n    while a <= 30:\n        if 10 <= a <= 30:\n            print(a)\n        a, b = b, a + b\n\nfibonacci_sequence()\n\\`\\`\\`\n\nThis script will print the Fibonacci numbers between 10 and 30. Please execute the code to see the result.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\nuser_proxy (to assistant):\n\nexitcode: 0 (execution succeeded)\nCode output: \n13\n21\n\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nThe Fibonacci numbers between 10 and 30 are 13 and 21. \n\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \n\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n{'call_autogen_agent': {'role': 'assistant', 'content': 'The Fibonacci numbers between 10 and 30 are 13 and 21. \\n\\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \\n\\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\n\\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\\n\\nTERMINATE'}}\n{'workflow': {'role': 'assistant', 'content': 'The Fibonacci numbers between 10 and 30 are 13 and 21. \\n\\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \\n\\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\n\\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\\n\\nTERMINATE'}}\n```\nSince we're leveraging LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) features we can now continue the conversation using the same thread ID -- LangGraph will automatically pass previous history to the AutoGen agent:\n\n\n```python hl_lines=\"8\"\nfor chunk in workflow.stream(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": \"Multiply the last number by 3\",\n        }\n    ],\n    config,\n):\n    print(chunk)\n```\n```output\nuser_proxy (to assistant):\n\nMultiply the last number by 3\nContext: \nFind numbers between 10 and 30 in fibonacci sequence\nThe Fibonacci numbers between 10 and 30 are 13 and 21. \n\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \n\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nThe last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\n\n21 * 3 = 63\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n{'call_autogen_agent': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}\n{'workflow': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}\n```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 8808,
    "word_count": 1079
  },
  {
    "title": "How to handle large numbers of tools",
    "source": "how-tos/many-tools.ipynb",
    "content": "# How to handle large numbers of tools\n\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Prerequisites</p>\n    <p>\n        This guide assumes familiarity with the following:\n        <ul>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#tools\">\n                    Tools\n                </a>\n            </li>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#chat-models/\">\n                    Chat Models\n                </a>\n            </li>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#embedding-models\">\n                    Embedding Models\n                </a>\n            </li>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#vector-stores\">\n                    Vectorstores\n                </a>\n            </li>   \n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#documents\">\n                    Document\n                </a>\n            </li>\n        </ul>\n    </p>\n</div> \n\n\nThe subset of available tools to call is generally at the discretion of the model (although many providers also enable the user to [specify or constrain the choice of tool](https://python.langchain.com/docs/how_to/tool_choice/)). As the number of available tools grows, you may want to limit the scope of the LLM's selection, to decrease token consumption and to help manage sources of error in LLM reasoning.\n\nHere we will demonstrate how to dynamically adjust the tools available to a model. Bottom line up front: like [RAG](https://python.langchain.com/docs/concepts/#retrieval) and similar methods, we prefix the model invocation by retrieving over available tools. Although we demonstrate one implementation that searches over tool descriptions, the details of the tool selection can be customized as needed.\n\n## Setup\n\nFirst, let's install the required packages and set our API keys\n\n\n```shell\npip install --quiet -U langgraph langchain_openai numpy\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n    <p style=\"padding-top: 5px;\">\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n    </p>\n</div>\n\n## Define the tools\n\nLet's consider a toy example in which we have one tool for each publicly traded company in the [S&P 500 index](https://en.wikipedia.org/wiki/S%26P_500). Each tool fetches company-specific information based on the year provided as a parameter.\n\nWe first construct a registry that associates a unique identifier with a schema for each tool. We will represent the tools using JSON schema, which can be bound directly to chat models supporting tool calling.\n\n\n```python\nimport re\nimport uuid\n\nfrom langchain_core.tools import StructuredTool\n\n\ndef create_tool(company: str) -> dict:\n    \"\"\"Create schema for a placeholder tool.\"\"\"\n    # Remove non-alphanumeric characters and replace spaces with underscores for the tool name\n    formatted_company = re.sub(r\"[^\\w\\s]\", \"\", company).replace(\" \", \"_\")\n\n    def company_tool(year: int) -> str:\n        # Placeholder function returning static revenue information for the company and year\n        return f\"{company} had revenues of $100 in {year}.\"\n\n    return StructuredTool.from_function(\n        company_tool,\n        name=formatted_company,\n        description=f\"Information about {company}\",\n    )\n\n\n# Abbreviated list of S&P 500 companies for demonstration\ns_and_p_500_companies = [\n    \"3M\",\n    \"A.O. Smith\",\n    \"Abbott\",\n    \"Accenture\",\n    \"Advanced Micro Devices\",\n    \"Yum! Brands\",\n    \"Zebra Technologies\",\n    \"Zimmer Biomet\",\n    \"Zoetis\",\n]\n\n# Create a tool for each company and store it in a registry with a unique UUID as the key\ntool_registry = {\n    str(uuid.uuid4()): create_tool(company) for company in s_and_p_500_companies\n}\n```\n\n## Define the graph\n\n### Tool selection\n\nWe will construct a node that retrieves a subset of available tools given the information in the state-- such as a recent user message. In general, the full scope of [retrieval solutions](https://python.langchain.com/docs/concepts/#retrieval) are available for this step. As a simple solution, we index embeddings of tool descriptions in a vector store, and associate user queries to tools via semantic search.\n\n\n```python\nfrom langchain_core.documents import Document\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\ntool_documents = [\n    Document(\n        page_content=tool.description,\n        id=id,\n        metadata={\"tool_name\": tool.name},\n    )\n    for id, tool in tool_registry.items()\n]\n\nvector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\ndocument_ids = vector_store.add_documents(tool_documents)\n```\n\n### Incorporating with an agent\n\nWe will use a typical React agent graph (e.g., as used in the [quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-2-enhancing-the-chatbot-with-tools)), with some modifications:\n\n- We add a `selected_tools` key to the state, which stores our selected subset of tools;\n- We set the entry point of the graph to be a `select_tools` node, which populates this element of the state;\n- We bind the selected subset of tools to the chat model within the `agent` node.\n\n\n```python\nfrom typing import Annotated\n\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\n\n# Define the state structure using TypedDict.\n# It includes a list of messages (processed by add_messages)\n# and a list of selected tool IDs.\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n    selected_tools: list[str]\n\n\nbuilder = StateGraph(State)\n\n# Retrieve all available tools from the tool registry.\ntools = list(tool_registry.values())\nllm = ChatOpenAI()\n\n\n# The agent function processes the current state\n# by binding selected tools to the LLM.\ndef agent(state: State):\n    # Map tool IDs to actual tools\n    # based on the state's selected_tools list.\n    selected_tools = [tool_registry[id] for id in state[\"selected_tools\"]]\n    # Bind the selected tools to the LLM for the current interaction.\n    llm_with_tools = llm.bind_tools(selected_tools)\n    # Invoke the LLM with the current messages and return the updated message list.\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\n\n# The select_tools function selects tools based on the user's last message content.\ndef select_tools(state: State):\n    last_user_message = state[\"messages\"][-1]\n    query = last_user_message.content\n    tool_documents = vector_store.similarity_search(query)\n    return {\"selected_tools\": [document.id for document in tool_documents]}\n\n\nbuilder.add_node(\"agent\", agent)\nbuilder.add_node(\"select_tools\", select_tools)\n\ntool_node = ToolNode(tools=tools)\nbuilder.add_node(\"tools\", tool_node)\n\nbuilder.add_conditional_edges(\"agent\", tools_condition, path_map=[\"tools\", \"__end__\"])\nbuilder.add_edge(\"tools\", \"agent\")\nbuilder.add_edge(\"select_tools\", \"agent\")\nbuilder.add_edge(START, \"select_tools\")\ngraph = builder.compile()\n```\n\n\n```python\nfrom IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n```\n\n<p>\n\n</p>\n\n\n```python\nuser_input = \"Can you give me some information about AMD in 2022?\"\n\nresult = graph.invoke({\"messages\": [(\"user\", user_input)]})\n```\n\n\n```python\nprint(result[\"selected_tools\"])\n```\n```output\n['ab9c0d59-3d16-448d-910c-73cf10a26020', 'f5eff8f6-7fb9-47b6-b54f-19872a52db84', '2962e168-9ef4-48dc-8b7c-9227e7956d39', '24a9fb82-19fe-4a88-944e-47bc4032e94a']\n```\n\n```python\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n```output\n================================ Human Message =================================\n\nCan you give me some information about AMD in 2022?\n================================== Ai Message ==================================\nTool Calls:\n  Advanced_Micro_Devices (call_CRxQ0oT7NY7lqf35DaRNTJ35)\n Call ID: call_CRxQ0oT7NY7lqf35DaRNTJ35\n  Args:\n    year: 2022\n================================= Tool Message =================================\nName: Advanced_Micro_Devices\n\nAdvanced Micro Devices had revenues of $100 in 2022.\n================================== Ai Message ==================================\n\nIn 2022, Advanced Micro Devices (AMD) had revenues of $100.\n```\n## Repeating tool selection\n\nTo manage errors from incorrect tool selection, we could revisit the `select_tools` node. One option for implementing this is to modify `select_tools` to generate the vector store query using all messages in the state (e.g., with a chat model) and add an edge routing from `tools` to `select_tools`.\n\nWe implement this change below. For demonstration purposes, we simulate an error in the initial tool selection by adding a `hack_remove_tool_condition` to the `select_tools` node, which removes the correct tool on the first iteration of the node. Note that on the second iteration, the agent finishes the run as it has access to the correct tool.\n\n<div class=\"admonition note\">\n    <p class=\"admonition-title\">Using Pydantic with LangChain</p>\n    <p>\n        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.\n    </p>\n</div>  \n\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\nfrom langgraph.pregel.retry import RetryPolicy\n\nfrom pydantic import BaseModel, Field\n\n\nclass QueryForTools(BaseModel):\n    \"\"\"Generate a query for additional tools.\"\"\"\n\n    query: str = Field(..., description=\"Query for additional tools.\")\n\n\ndef select_tools(state: State):\n    \"\"\"Selects tools based on the last message in the conversation state.\n\n    If the last message is from a human, directly uses the content of the message\n    as the query. Otherwise, constructs a query using a system message and invokes\n    the LLM to generate tool suggestions.\n    \"\"\"\n    last_message = state[\"messages\"][-1]\n    hack_remove_tool_condition = False  # Simulate an error in the first tool selection\n\n    if isinstance(last_message, HumanMessage):\n        query = last_message.content\n        hack_remove_tool_condition = True  # Simulate wrong tool selection\n    else:\n        assert isinstance(last_message, ToolMessage)\n        system = SystemMessage(\n            \"Given this conversation, generate a query for additional tools. \"\n            \"The query should be a short string containing what type of information \"\n            \"is needed. If no further information is needed, \"\n            \"set more_information_needed False and populate a blank string for the query.\"\n        )\n        input_messages = [system] + state[\"messages\"]\n        response = llm.bind_tools([QueryForTools], tool_choice=True).invoke(\n            input_messages\n        )\n        query = response.tool_calls[0][\"args\"][\"query\"]\n\n    # Search the tool vector store using the generated query\n    tool_documents = vector_store.similarity_search(query)\n    if hack_remove_tool_condition:\n        # Simulate error by removing the correct tool from the selection\n        selected_tools = [\n            document.id\n            for document in tool_documents\n            if document.metadata[\"tool_name\"] != \"Advanced_Micro_Devices\"\n        ]\n    else:\n        selected_tools = [document.id for document in tool_documents]\n    return {\"selected_tools\": selected_tools}\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"agent\", agent)\ngraph_builder.add_node(\n    \"select_tools\", select_tools, retry_policy=RetryPolicy(max_attempts=3)\n)\n\ntool_node = ToolNode(tools=tools)\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"agent\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"select_tools\")\ngraph_builder.add_edge(\"select_tools\", \"agent\")\ngraph_builder.add_edge(START, \"select_tools\")\ngraph = graph_builder.compile()\n```\n\n\n```python\nfrom IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n```\n\n<p>\n\n</p>\n\n\n```python\nuser_input = \"Can you give me some information about AMD in 2022?\"\n\nresult = graph.invoke({\"messages\": [(\"user\", user_input)]})\n```\n\n\n```python\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n```output\n================================ Human Message =================================\n\nCan you give me some information about AMD in 2022?\n================================== Ai Message ==================================\nTool Calls:\n  Accenture (call_qGmwFnENwwzHOYJXiCAaY5Mx)\n Call ID: call_qGmwFnENwwzHOYJXiCAaY5Mx\n  Args:\n    year: 2022\n================================= Tool Message =================================\nName: Accenture\n\nAccenture had revenues of $100 in 2022.\n================================== Ai Message ==================================\nTool Calls:\n  Advanced_Micro_Devices (call_u9e5UIJtiieXVYi7Y9GgyDpn)\n Call ID: call_u9e5UIJtiieXVYi7Y9GgyDpn\n  Args:\n    year: 2022\n================================= Tool Message =================================\nName: Advanced_Micro_Devices\n\nAdvanced Micro Devices had revenues of $100 in 2022.\n================================== Ai Message ==================================\n\nIn 2022, AMD had revenues of $100.\n```\n## Next steps\n\nThis guide provides a minimal implementation for dynamically selecting tools. There is a host of possible improvements and optimizations:\n\n- **Repeating tool selection**: Here, we repeated tool selection by modifying the `select_tools` node. Another option is to equip the agent with a `reselect_tools` tool, allowing it to re-select tools at its discretion.\n- **Optimizing tool selection**: In general, the full scope of [retrieval solutions](https://python.langchain.com/docs/concepts/#retrieval) are available for tool selection. Additional options include:\n  - Group tools and retrieve over groups;\n  - Use a chat model to select tools or groups of tool.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 14821,
    "word_count": 1601
  },
  {
    "title": "How to create a ReAct agent from scratch",
    "source": "how-tos/react-agent-from-scratch.ipynb",
    "content": "# How to create a ReAct agent from scratch\n\n!!! info \"Prerequisites\"\n    This guide assumes familiarity with the following:\n    \n    - <a href=\"../../concepts/agentic_concepts/#tool-calling-agent\">Tool calling agent</a>\n    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n    - [Messages](https://python.langchain.com/docs/concepts/messages/)\n    - <a href=\"../../concepts/low_level/\">LangGraph Glossary</a>\n\nUsing the prebuilt ReAct agent [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] is a great way to get started, but sometimes you might want more control and customization. In those cases, you can create a custom ReAct agent. This guide shows how to implement ReAct agent from scratch using LangGraph.\n\n## Setup\n\nFirst, let's install the required packages and set our API keys:\n\n\n```shell\npip install -U langgraph langchain-openai\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n<div class=\"admonition tip\">\n     <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for better debugging</p>\n     <p style=\"padding-top: 5px;\">\n         Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM aps built with LangGraph — read more about how to get started in the <a href=\"https://docs.smith.langchain.com\">docs</a>. \n     </p>\n </div>\n\n## Create ReAct agent\n\nNow that you have installed the required packages and set your environment variables, we can code our ReAct agent!\n\n### Define graph state\n\nWe are going to define the most basic ReAct state in this example, which will just contain a list of messages.\n\nFor your specific use case, feel free to add any other state keys that you need.\n\n\n```python\nfrom typing import (\n    Annotated,\n    Sequence,\n    TypedDict,\n)\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph.message import add_messages\n\n\nclass AgentState(TypedDict):\n    \"\"\"The state of the agent.\"\"\"\n\n    # add_messages is a reducer\n    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n```\n\n### Define model and tools\n\nNext, let's define the tools and model we will use for our example.\n\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\n@tool\ndef get_weather(location: str):\n    \"\"\"Call to get the weather from a specific location.\"\"\"\n    # This is a placeholder for the actual implementation\n    # Don't let the LLM know this though 😊\n    if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n        return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\"\n    else:\n        return f\"I am not sure what the weather is in {location}\"\n\n\ntools = [get_weather]\n\nmodel = model.bind_tools(tools)\n```\n\n### Define nodes and edges\n\nNext let's define our nodes and edges. In our basic ReAct agent there are only two nodes, one for calling the model and one for using tools, however you can modify this basic structure to work better for your use case. The tool node we define here is a simplified version of the prebuilt [`ToolNode`](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/), which has some additional features.\n\nPerhaps you want to add a node for [adding structured output](https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output/) or a node for executing some external action (sending an email, adding a calendar event, etc.). Maybe you just want to change the way the `call_model` node works and how `should_continue` decides whether to call tools - the possibilities are endless and LangGraph makes it easy to customize this basic structure for your specific use case.\n\n\n```python\nimport json\nfrom langchain_core.messages import ToolMessage, SystemMessage\nfrom langchain_core.runnables import RunnableConfig\n\ntools_by_name = {tool.name: tool for tool in tools}\n\n\n# Define our tool node\ndef tool_node(state: AgentState):\n    outputs = []\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n        outputs.append(\n            ToolMessage(\n                content=json.dumps(tool_result),\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n    return {\"messages\": outputs}\n\n\n# Define the node that calls the model\ndef call_model(\n    state: AgentState,\n    config: RunnableConfig,\n):\n    # this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n    system_prompt = SystemMessage(\n        \"You are a helpful AI assistant, please respond to the users query to the best of your ability!\"\n    )\n    response = model.invoke([system_prompt] + state[\"messages\"], config)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\n# Define the conditional edge that determines whether to continue or not\ndef should_continue(state: AgentState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we finish\n    if not last_message.tool_calls:\n        return \"end\"\n    # Otherwise if there is, we continue\n    else:\n        return \"continue\"\n```\n\n### Define the graph\n\nNow that we have defined all of our nodes and edges, we can define and compile our graph. Depending on if you have added more nodes or different edges, you will need to edit this to fit your specific use case.\n\n\n```python\nfrom langgraph.graph import StateGraph, END\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n    # Finally we pass in a mapping.\n    # The keys are strings, and the values are other nodes.\n    # END is a special node marking that the graph should finish.\n    # What will happen is we will call `should_continue`, and then the output of that\n    # will be matched against the keys in this mapping.\n    # Based on which one it matches, that node will then be called.\n    {\n        # If `tools`, then we call the tool node.\n        \"continue\": \"tools\",\n        # Otherwise we finish.\n        \"end\": END,\n    },\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", \"agent\")\n\n# Now we can compile and visualize our graph\ngraph = workflow.compile()\n\nfrom IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n```\n\n<p>\n\n</p>\n\n## Use ReAct agent\n\nNow that we have created our react agent, let's actually put it to the test!\n\n\n```python\n# Helper function for formatting the stream nicely\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\n\ninputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\nprint_stream(graph.stream(inputs, stream_mode=\"values\"))\n```\n```output\n================================ Human Message =================================\n\nwhat is the weather in sf\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_azW0cQ4XjWWj0IAkWAxq9nLB)\n Call ID: call_azW0cQ4XjWWj0IAkWAxq9nLB\n  Args:\n    location: San Francisco\n================================= Tool Message =================================\nName: get_weather\n\n\"It's sunny in San Francisco, but you better look out if you're a Gemini \\ud83d\\ude08.\"\n================================== Ai Message ==================================\n\nThe weather in San Francisco is sunny! However, it seems there's a playful warning for Geminis. Enjoy the sunshine!\n```\nPerfect! The graph correctly calls the `get_weather` tool and responds to the user after receiving the information from the tool.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 8767,
    "word_count": 1117
  },
  {
    "title": "How to force tool-calling agent to structure output",
    "source": "how-tos/react-agent-structured-output.ipynb",
    "content": "# How to force tool-calling agent to structure output\n\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Prerequisites</p>\n    <p>\n        This guide assumes familiarity with the following:\n        <ul>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#structured-output\">\n                    Structured Output\n                </a>\n            </li>            \n            <li>\n                <a href=\"https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#tool-calling-agent\">\n                    Tool calling agent\n                </a>\n            </li>                \n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#chat-models\">\n                    Chat Models\n                </a>\n            </li>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#messages\">\n                    Messages\n                </a>\n            </li>\n            <li>\n                <a href=\"https://langchain-ai.github.io/langgraph/concepts/low_level/\">\n                    LangGraph Glossary\n                </a>\n            </li>\n        </ul>\n    </p>\n</div> \n\nYou might want your agent to return its output in a structured format. For example, if the output of the agent is used by some other downstream software, you may want the output to be in the same structured format every time the agent is invoked to ensure consistency.\n\nThis notebook will walk through two different options for forcing a tool calling agent to structure its output. We will be using a basic [ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) (a model node and a tool-calling node) together with a third node at the end that will format response for the user. Both of the options will use the same graph structure as shown in the diagram below, but will have different mechanisms under the hood.\n\n\n\n**Option 1**\n\n\n\nThe first way you can force your tool calling agent to have structured output is to bind the output you would like as an additional tool for the `agent` node to use. In contrast to the basic ReAct agent, the `agent` node in this case is not selecting between `tools` and `END` but rather selecting between the specific tools it calls. The expected flow in this case is that the LLM in the `agent` node will first select the action tool, and after receiving the action tool output it will call the response tool, which will then route to the `respond` node which simply structures the arguments from the `agent` node tool call.\n\n**Pros and Cons**\n\nThe benefit to this format is that you only need one LLM, and can save money and latency because of this. The downside to this option is that it isn't guaranteed that the single LLM will call the correct tool when you want it to. We can help the LLM by setting `tool_choice` to `any` when we use `bind_tools` which forces the LLM to select at least one tool at every turn, but this is far from a foolproof strategy. In addition, another downside is that the agent might call *multiple* tools, so we need to check for this explicitly in our routing function (or if we are using OpenAI we can set `parallell_tool_calling=False` to ensure only one tool is called at a time).\n\n**Option 2**\n\n\n\nThe second way you can force your tool calling agent to have structured output is to use a second LLM (in this case `model_with_structured_output`) to respond to the user. \n\nIn this case, you will define a basic ReAct agent normally, but instead of having the `agent` node choose between the `tools` node and ending the conversation, the `agent` node will choose between the `tools` node and the `respond` node. The `respond` node will contain a second LLM that uses structured output, and once called will return directly to the user. You can think of this method as basic ReAct with one extra step before responding to the user. \n\n**Pros and Cons**\n\nThe benefit of this method is that it guarantees structured output (as long as `.with_structured_output` works as expected with the LLM). The downside to using this approach is that it requires making an additional LLM call before responding to the user, which can increase costs as well as latency. In addition, by not providing the `agent` node LLM with information about the desired output schema there is a risk that the `agent` LLM will fail to call the correct tools required to answer in the correct output schema.\n\nNote that both of these options will follow the exact same graph structure (see the diagram above), in that they are both exact replicas of the basic ReAct architecture but with a `respond` node before the end.\n\n## Setup\n\nFirst, let's install the required packages and set our API keys\n\n\n```shell\npip install -U langgraph langchain_anthropic\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n```\n\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n    <p style=\"padding-top: 5px;\">\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n    </p>\n</div>\n\n## Define model, tools, and graph state\n\nNow we can define how we want to structure our output, define our graph state, and also our tools and the models we are going to use.\n\nTo use structured output, we will use the `with_structured_output` method from LangChain, which you can read more about [here](https://python.langchain.com/docs/how_to/structured_output/).\n\nWe are going to use a single tool in this example for finding the weather, and will return a structured weather response to the user.\n\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain_core.tools import tool\nfrom langchain_anthropic import ChatAnthropic\nfrom langgraph.graph import MessagesState\n\n\nclass WeatherResponse(BaseModel):\n    \"\"\"Respond to the user with this\"\"\"\n\n    temperature: float = Field(description=\"The temperature in fahrenheit\")\n    wind_directon: str = Field(\n        description=\"The direction of the wind in abbreviated form\"\n    )\n    wind_speed: float = Field(description=\"The speed of the wind in km/h\")\n\n\n# Inherit 'messages' key from MessagesState, which is a list of chat messages\nclass AgentState(MessagesState):\n    # Final structured response from the agent\n    final_response: WeatherResponse\n\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees\"\n    elif city == \"sf\":\n        return \"It is 75 degrees and sunny in SF, with 3 mph winds in the South-East direction\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\ntools = [get_weather]\n\nmodel = ChatAnthropic(model=\"claude-3-opus-20240229\")\n\nmodel_with_tools = model.bind_tools(tools)\nmodel_with_structured_output = model.with_structured_output(WeatherResponse)\n```\n\n## Option 1: Bind output as tool\n\nLet's now examine how we would use the single LLM option.\n\n### Define Graph\n\nThe graph definition is very similar to the one above, the only difference is we no longer call an LLM in the `response` node, and instead bind the `WeatherResponse` tool to our LLM that already contains the `get_weather` tool.\n\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import ToolNode\n\ntools = [get_weather, WeatherResponse]\n\n# Force the model to use tools by passing tool_choice=\"any\"\nmodel_with_response_tool = model.bind_tools(tools, tool_choice=\"any\")\n\n\n# Define the function that calls the model\ndef call_model(state: AgentState):\n    response = model_with_response_tool.invoke(state[\"messages\"])\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\n# Define the function that responds to the user\ndef respond(state: AgentState):\n    # Construct the final answer from the arguments of the last tool call\n    weather_tool_call = state[\"messages\"][-1].tool_calls[0]\n    response = WeatherResponse(**weather_tool_call[\"args\"])\n    # Since we're using tool calling to return structured output,\n    # we need to add  a tool message corresponding to the WeatherResponse tool call,\n    # This is due to LLM providers' requirement that AI messages with tool calls\n    # need to be followed by a tool message for each tool call\n    tool_message = {\n        \"type\": \"tool\",\n        \"content\": \"Here is your structured response\",\n        \"tool_call_id\": weather_tool_call[\"id\"],\n    }\n    # We return the final answer\n    return {\"final_response\": response, \"messages\": [tool_message]}\n\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: AgentState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is only one tool call and it is the response tool call we respond to the user\n    if (\n        len(last_message.tool_calls) == 1\n        and last_message.tool_calls[0][\"name\"] == \"WeatherResponse\"\n    ):\n        return \"respond\"\n    # Otherwise we will use the tool node again\n    else:\n        return \"continue\"\n\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"respond\", respond)\nworkflow.add_node(\"tools\", ToolNode(tools))\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"tools\",\n        \"respond\": \"respond\",\n    },\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\nworkflow.add_edge(\"respond\", END)\ngraph = workflow.compile()\n```\n\n### Usage\n\nNow we can run our graph to check that it worked as intended:\n\n\n```python\nanswer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n    \"final_response\"\n]\n```\n\n\n```python\nanswer\n```\n\n\n\n```output\nWeatherResponse(temperature=75.0, wind_directon='SE', wind_speed=3.0)\n```\n\n\nAgain, the agent returned a `WeatherResponse` object as we expected.\n\n## Option 2: 2 LLMs\n\nLet's now dive into how we would use a second LLM to force structured output.\n\n### Define Graph\n\nWe can now define our graph:\n\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import ToolNode\nfrom langchain_core.messages import HumanMessage\n\n\n# Define the function that calls the model\ndef call_model(state: AgentState):\n    response = model_with_tools.invoke(state[\"messages\"])\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\n# Define the function that responds to the user\ndef respond(state: AgentState):\n    # We call the model with structured output in order to return the same format to the user every time\n    # state['messages'][-2] is the last ToolMessage in the convo, which we convert to a HumanMessage for the model to use\n    # We could also pass the entire chat history, but this saves tokens since all we care to structure is the output of the tool\n    response = model_with_structured_output.invoke(\n        [HumanMessage(content=state[\"messages\"][-2].content)]\n    )\n    # We return the final answer\n    return {\"final_response\": response}\n\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: AgentState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we respond to the user\n    if not last_message.tool_calls:\n        return \"respond\"\n    # Otherwise if there is, we continue\n    else:\n        return \"continue\"\n\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"respond\", respond)\nworkflow.add_node(\"tools\", ToolNode(tools))\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"tools\",\n        \"respond\": \"respond\",\n    },\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\nworkflow.add_edge(\"respond\", END)\ngraph = workflow.compile()\n```\n\n\n### Usage\n\nWe can now invoke our graph to verify that the output is being structured as desired:\n\n\n```python\nanswer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n    \"final_response\"\n]\n```\n\n\n```python\nanswer\n```\n\n\n\n```output\nWeatherResponse(temperature=75.0, wind_directon='SE', wind_speed=4.83)\n```\n\n\nAs we can see, the agent returned a `WeatherResponse` object as we expected. If would now be easy to use this agent in a more complex software stack without having to worry about the output of the agent not matching the format expected from the next step in the stack.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 13290,
    "word_count": 1763
  },
  {
    "title": "How to disable streaming for models that don't support it",
    "source": "how-tos/disable-streaming.ipynb",
    "content": "# How to disable streaming for models that don't support it\n\n<div class=\"admonition tip\">\n    <p class=\"admonition-title\">Prerequisites</p>\n    <p>\n        This guide assumes familiarity with the following:\n        <ul>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#streaming\">\n                    streaming\n                </a>                \n            </li>\n            <li>\n                <a href=\"https://python.langchain.com/docs/concepts/#chat-models/\">\n                    Chat Models\n                </a>\n            </li>\n        </ul>\n    </p>\n</div> \n\nSome chat models, including the new O1 models from OpenAI (depending on when you're reading this), do not support streaming. This can lead to issues when using the [astream_events API](https://python.langchain.com/docs/concepts/#astream_events), as it calls models in streaming mode, expecting streaming to function properly.\n\nIn this guide, we’ll show you how to disable streaming for models that don’t support it, ensuring they they're never called in streaming mode, even when invoked through the astream_events API.\n\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import MessagesState\nfrom langgraph.graph import StateGraph, START, END\n\nllm = ChatOpenAI(model=\"o1-preview\", temperature=1)\n\ngraph_builder = StateGraph(MessagesState)\n\n\ndef chatbot(state: MessagesState):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\ngraph = graph_builder.compile()\n```\n\n\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<p>\n\n</p>\n\n## Without disabling streaming\n\nNow that we've defined our graph, let's try to call `astream_events` without disabling streaming. This should throw an error because the `o1` model does not support streaming natively:\n\n\n```python\ninput = {\"messages\": {\"role\": \"user\", \"content\": \"how many r's are in strawberry?\"}}\ntry:\n    async for event in graph.astream_events(input, version=\"v2\"):\n        if event[\"event\"] == \"on_chat_model_end\":\n            print(event[\"data\"][\"output\"].content, end=\"\", flush=True)\nexcept:\n    print(\"Streaming not supported!\")\n```\n```output\nStreaming not supported!\n```\nAn error occurred as we expected, luckily there is an easy fix!\n\n## Disabling streaming\n\nNow without making any changes to our graph, let's set the [disable_streaming](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.disable_streaming) parameter on our model to be `True` which will solve the problem:\n\n\n```python\nllm = ChatOpenAI(model=\"o1-preview\", temperature=1, disable_streaming=True)\n\ngraph_builder = StateGraph(MessagesState)\n\n\ndef chatbot(state: MessagesState):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\ngraph = graph_builder.compile()\n```\n\nAnd now, rerunning with the same input, we should see no errors:\n\n\n```python\ninput = {\"messages\": {\"role\": \"user\", \"content\": \"how many r's are in strawberry?\"}}\nasync for event in graph.astream_events(input, version=\"v2\"):\n    if event[\"event\"] == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"].content, end=\"\", flush=True)\n```\n```output\nThere are three \"r\"s in the word \"strawberry\".\n```",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 3567,
    "word_count": 344
  },
  {
    "title": "How to create a ReAct agent from scratch (Functional API)",
    "source": "how-tos/react-agent-from-scratch-functional.ipynb",
    "content": "# How to create a ReAct agent from scratch (Functional API)\n\n!!! info \"Prerequisites\"\n    This guide assumes familiarity with the following:\n    \n    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models)\n    - [Messages](https://python.langchain.com/docs/concepts/messages)\n    - [Tool Calling](https://python.langchain.com/docs/concepts/tool_calling/)\n    - <a href=\"../../concepts/functional_api/#entrypoint\">Entrypoints</a> and <a href=\"../../concepts/functional_api/#task\">Tasks</a>\n\nThis guide demonstrates how to implement a ReAct agent using the LangGraph <a href=\"../../concepts/functional_api\">Functional API</a>.\n\nThe ReAct agent is a <a href=\"../../concepts/agentic_concepts/#tool-calling-agent\">tool-calling agent</a> that operates as follows:\n\n1. Queries are issued to a chat model;\n2. If the model generates no <a href=\"../../concepts/agentic_concepts/#tool-calling\">tool calls</a>, we return the model response.\n3. If the model generates tool calls, we execute the tool calls with available tools, append them as [tool messages](https://python.langchain.com/docs/concepts/messages/) to our message list, and repeat the process.\n\nThis is a simple and versatile set-up that can be extended with memory, human-in-the-loop capabilities, and other features. See the dedicated <a href=\"../../how-tos/#prebuilt-react-agent\">how-to guides</a> for examples.\n\n## Setup\n\nFirst, let's install the required packages and set our API keys:\n\n\n```shell\npip install -U langgraph langchain-openai\n```\n\n\n```python\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n<div class=\"admonition tip\">\n     <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for better debugging</p>\n     <p style=\"padding-top: 5px;\">\n         Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM aps built with LangGraph — read more about how to get started in the <a href=\"https://docs.smith.langchain.com\">docs</a>. \n     </p>\n </div>\n\n## Create ReAct agent\n\nNow that you have installed the required packages and set your environment variables, we can create our agent.\n\n### Define model and tools\n\nLet's first define the tools and model we will use for our example. Here we will use a single place-holder tool that gets a description of the weather for a location.\n\nWe will use an [OpenAI](https://python.langchain.com/docs/integrations/providers/openai/) chat model for this example, but any model [supporting tool-calling](https://python.langchain.com/docs/integrations/chat/) will suffice.\n\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\n@tool\ndef get_weather(location: str):\n    \"\"\"Call to get the weather from a specific location.\"\"\"\n    # This is a placeholder for the actual implementation\n    if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n        return \"It's sunny!\"\n    elif \"boston\" in location.lower():\n        return \"It's rainy!\"\n    else:\n        return f\"I am not sure what the weather is in {location}\"\n\n\ntools = [get_weather]\n```\n\n### Define tasks\n\nWe next define the <a href=\"../../concepts/functional_api/#task\">tasks</a> we will execute. Here there are two different tasks:\n\n1. **Call model**: We want to query our chat model with a list of messages.\n2. **Call tool**: If our model generates tool calls, we want to execute them.\n\n\n```python\nfrom langchain_core.messages import ToolMessage\nfrom langgraph.func import entrypoint, task\n\ntools_by_name = {tool.name: tool for tool in tools}\n\n\n@task\ndef call_model(messages):\n    \"\"\"Call model with a sequence of messages.\"\"\"\n    response = model.bind_tools(tools).invoke(messages)\n    return response\n\n\n@task\ndef call_tool(tool_call):\n    tool = tools_by_name[tool_call[\"name\"]]\n    observation = tool.invoke(tool_call[\"args\"])\n    return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])\n```\n\n### Define entrypoint\n\nOur <a href=\"../../concepts/functional_api/#entrypoint\">entrypoint</a> will handle the orchestration of these two tasks. As described above, when our `call_model` task generates tool calls, the `call_tool` task will generate responses for each. We append all messages to a single messages list.\n\n!!! tip\n    Note that because tasks return future-like objects, the below implementation executes tools in parallel.\n\n\n```python\nfrom langgraph.graph.message import add_messages\n\n\n@entrypoint()\ndef agent(messages):\n    llm_response = call_model(messages).result()\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Execute tools\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n\n        # Append to message list\n        messages = add_messages(messages, [llm_response, *tool_results])\n\n        # Call model again\n        llm_response = call_model(messages).result()\n\n    return llm_response\n```\n\n## Usage\n\nTo use our agent, we invoke it with a messages list. Based on our implementation, these can be LangChain [message](https://python.langchain.com/docs/concepts/messages/) objects or OpenAI-style dicts:\n\n\n```python\nuser_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\nprint(user_message)\n\nfor step in agent.stream([user_message]):\n    for task_name, message in step.items():\n        if task_name == \"agent\":\n            continue  # Just print task updates\n        print(f\"\\n{task_name}:\")\n        message.pretty_print()\n```\n```output\n{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n\ncall_model:\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_tNnkrjnoz6MNfCHJpwfuEQ0v)\n Call ID: call_tNnkrjnoz6MNfCHJpwfuEQ0v\n  Args:\n    location: san francisco\n\ncall_tool:\n================================= Tool Message =================================\n\nIt's sunny!\n\ncall_model:\n================================== Ai Message ==================================\n\nThe weather in San Francisco is sunny!\n```\nPerfect! The graph correctly calls the `get_weather` tool and responds to the user after receiving the information from the tool. Check out the LangSmith trace [here](https://smith.langchain.com/public/d5a0d5ea-bdaa-4032-911e-7db177c8141b/r).\n\n## Add thread-level persistence\n\nAdding <a href=\"../../concepts/persistence#threads\">thread-level persistence</a> lets us support conversational experiences with our agent: subsequent invocations will append to the prior messages list, retaining the full conversational context.\n\nTo add thread-level persistence to our agent:\n\n1. Select a <a href=\"../../concepts/persistence#checkpointer-libraries\">checkpointer</a>: here we will use <a href=\"../../reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver\">InMemorySaver</a>, a simple in-memory checkpointer.\n2. Update our entrypoint to accept the previous messages state as a second argument. Here, we simply append the message updates to the previous sequence of messages.\n3. Choose which values will be returned from the workflow and which will be saved by the checkpointer as `previous` using `entrypoint.final` (optional)\n\n\n```python hl_lines=\"3 6 7 8 9 30\"\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef agent(messages, previous):\n    if previous is not None:\n        messages = add_messages(previous, messages)\n\n    llm_response = call_model(messages).result()\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Execute tools\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n\n        # Append to message list\n        messages = add_messages(messages, [llm_response, *tool_results])\n\n        # Call model again\n        llm_response = call_model(messages).result()\n\n    # Generate final response\n    messages = add_messages(messages, llm_response)\n    return entrypoint.final(value=llm_response, save=messages)\n```\n\nWe will now need to pass in a config when running our application. The config will specify an identifier for the conversational thread.\n\n!!! tip\n\n    Read more about thread-level persistence in our <a href=\"../../concepts/persistence/\">concepts page</a> and <a href=\"../../how-tos/#persistence\">how-to guides</a>.\n\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n```\n\nWe start a thread the same way as before, this time passing in the config:\n\n\n```python hl_lines=\"4\"\nuser_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\nprint(user_message)\n\nfor step in agent.stream([user_message], config):\n    for task_name, message in step.items():\n        if task_name == \"agent\":\n            continue  # Just print task updates\n        print(f\"\\n{task_name}:\")\n        message.pretty_print()\n```\n```output\n{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n\ncall_model:\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_lubbUSdDofmOhFunPEZLBz3g)\n Call ID: call_lubbUSdDofmOhFunPEZLBz3g\n  Args:\n    location: San Francisco\n\ncall_tool:\n================================= Tool Message =================================\n\nIt's sunny!\n\ncall_model:\n================================== Ai Message ==================================\n\nThe weather in San Francisco is sunny!\n```\nWhen we ask a follow-up conversation, the model uses the prior context to infer that we are asking about the weather:\n\n\n```python\nuser_message = {\"role\": \"user\", \"content\": \"How does it compare to Boston, MA?\"}\nprint(user_message)\n\nfor step in agent.stream([user_message], config):\n    for task_name, message in step.items():\n        if task_name == \"agent\":\n            continue  # Just print task updates\n        print(f\"\\n{task_name}:\")\n        message.pretty_print()\n```\n```output\n{'role': 'user', 'content': 'How does it compare to Boston, MA?'}\n\ncall_model:\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_8sTKYAhSIHOdjLD5d6gaswuV)\n Call ID: call_8sTKYAhSIHOdjLD5d6gaswuV\n  Args:\n    location: Boston, MA\n\ncall_tool:\n================================= Tool Message =================================\n\nIt's rainy!\n\ncall_model:\n================================== Ai Message ==================================\n\nCompared to San Francisco, which is sunny, Boston, MA is experiencing rainy weather.\n```\nIn the [LangSmith trace](https://smith.langchain.com/public/20a1116b-bb3b-44c1-8765-7a28663439d9/r), we can see that the full conversational context is retained in each model call.",
    "source_file": "langgraph",
    "category": "how-tos",
    "char_count": 11044,
    "word_count": 1179
  },
  {
    "title": "Assistants",
    "source": "concepts/assistants.md",
    "content": "# Assistants\n\n**Assistants** allow you to manage configurations (like prompts, LLM selection, tools) separately from your graph's core logic, enabling rapid changes that don't alter the graph architecture. It is a way to create multiple specialized versions of the same graph architecture, each optimized for different use cases through context/configuration variations rather than structural changes.\n\nFor example, imagine a general-purpose writing agent built on a common graph architecture. While the structure remains the same, different writing styles—such as blog posts and tweets—require tailored configurations to optimize performance. To support these variations, you can create multiple assistants (e.g., one for blogs and another for tweets) that share the underlying graph but differ in model selection and system prompt.\n\n![assistant versions](img/assistants.png)\n\nThe LangGraph Cloud API provides several endpoints for creating and managing assistants and their versions. See the [API reference](../cloud/reference/api/api_ref.html#tag/assistants) for more details.\n\n!!! info\n\n    Assistants are a [LangGraph Platform](langgraph_platform.md) concept. They are not available in the open source LangGraph library.\n\n## Configuration\n\nAssistants build on the LangGraph open source concepts of configuration and [runtime context](low_level.md#runtime-context).\n\n\nWhile these features are available in the open source LangGraph library, assistants are only present in [LangGraph Platform](langgraph_platform.md). This is due to the fact that assistants are tightly coupled to your deployed graph. Upon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph's default context and configuration settings.\n\nIn practice, an assistant is just an _instance_ of a graph with a specific configuration. Therefore, multiple assistants can reference the same graph but can contain different configurations (e.g. prompts, models, tools). The LangGraph Server API provides several endpoints for creating and managing assistants. See the [API reference](../cloud/reference/api/api_ref.html) and [this how-to](../cloud/how-tos/configuration_cloud.md) for more details on how to create assistants.\n\n## Versioning\n\nAssistants support versioning to track changes over time.\nOnce you've created an assistant, subsequent edits to that assistant will create new versions. See [this how-to](../cloud/how-tos/configuration_cloud.md#create-a-new-version-for-your-assistant) for more details on how to manage assistant versions.\n\n## Execution\n\nA **run** is an invocation of an assistant. Each run may have its own input, configuration, context, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a [thread](./persistence.md#threads).\n\nThe LangGraph Platform API provides several endpoints for creating and managing runs. See the [API reference](../cloud/reference/api/api_ref.html#tag/thread-runs/) for more details.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3012,
    "word_count": 382
  },
  {
    "title": "LangGraph Studio",
    "source": "concepts/langgraph_studio.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph Studio\n\n!!! info \"Prerequisites\"\n\n    - [LangGraph Platform](./langgraph_platform.md)\n    - [LangGraph Server](./langgraph_server.md)\n    - [LangGraph CLI](./langgraph_cli.md)\n\nLangGraph Studio is a specialized agent IDE that enables visualization, interaction, and debugging of agentic systems that implement the LangGraph Server API protocol. Studio also integrates with LangSmith to enable tracing, evaluation, and prompt engineering.\n\n![](img/lg_studio.png)\n\n## Features\n\nKey features of LangGraph Studio:\n\n- Visualize your graph architecture\n- [Run and interact with your agent](../cloud/how-tos/invoke_studio.md)\n- [Manage assistants](../cloud/how-tos/studio/manage_assistants.md)\n- [Manage threads](../cloud/how-tos/threads_studio.md)\n- [Iterate on prompts](../cloud/how-tos/iterate_graph_studio.md)\n- [Run experiments over a dataset](../cloud/how-tos/studio/run_evals.md)\n- Manage [long term memory](memory.md)\n- Debug agent state via [time travel](time-travel.md)\n\nLangGraph Studio works for graphs that are deployed on [LangGraph Platform](../cloud/quick_start.md) or for graphs that are running locally via the [LangGraph Server](../tutorials/langgraph-platform/local-server.md).\n\nStudio supports two modes:\n\n### Graph mode\n\nGraph mode exposes the full feature-set of Studio and is useful when you would like as many details about the execution of your agent, including the nodes traversed, intermediate states, and LangSmith integrations (such as adding to datasets and playground).\n\n### Chat mode\n\nChat mode is a simpler UI for iterating on and testing chat-specific agents. It is useful for business users and those who want to test overall agent behavior. Chat mode is only supported for graph's whose state includes or extends [`MessagesState`](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#messagesstate).\n\n## Learn more\n\n- See this guide on how to [get started](../cloud/how-tos/studio/quick_start.md) with LangGraph Studio.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 1996,
    "word_count": 231
  },
  {
    "title": "Double Texting",
    "source": "concepts/double_texting.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Double Texting\n\n!!! info \"Prerequisites\"\n    - [LangGraph Server](./langgraph_server.md)\n\nMany times users might interact with your graph in unintended ways. \nFor instance, a user may send one message and before the graph has finished running send a second message. \nMore generally, users may invoke the graph a second time before the first run has finished.\nWe call this \"double texting\".\n\nCurrently, LangGraph only addresses this as part of [LangGraph Platform](langgraph_platform.md), not in the open source.\nThe reason for this is that in order to handle this we need to know how the graph is deployed, and since LangGraph Platform deals with deployment the logic needs to live there.\nIf you do not want to use LangGraph Platform, we describe the options we have implemented in detail below.\n\n![](img/double_texting.png)\n\n## Reject\n\nThis is the simplest option, this just rejects any follow-up runs and does not allow double texting. \nSee the [how-to guide](../cloud/how-tos/reject_concurrent.md) for configuring the reject double text option.\n\n## Enqueue\n\nThis is a relatively simple option which continues the first run until it completes the whole run, then sends the new input as a separate run. \nSee the [how-to guide](../cloud/how-tos/enqueue_concurrent.md) for configuring the enqueue double text option.\n\n## Interrupt\n\nThis option interrupts the current execution but saves all the work done up until that point. \nIt then inserts the user input and continues from there. \n\nIf you enable this option, your graph should be able to handle weird edge cases that may arise. \nFor example, you could have called a tool but not yet gotten back a result from running that tool.\nYou may need to remove that tool call in order to not have a dangling tool call.\n\nSee the [how-to guide](../cloud/how-tos/interrupt_concurrent.md) for configuring the interrupt double text option.\n\n## Rollback\n\nThis option interrupts the current execution AND rolls back all work done up until that point, including the original run input. It then sends the new user input in, basically as if it was the original input.\n\nSee the [how-to guide](../cloud/how-tos/rollback_concurrent.md) for configuring the rollback double text option.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 2244,
    "word_count": 345
  },
  {
    "title": "LangGraph runtime",
    "source": "concepts/pregel.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph runtime\n\n[Pregel](https://langchain-ai.github.io/langgraph/reference/pregel/) implements LangGraph's runtime, managing the execution of LangGraph applications.\n\nCompiling a [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph) or creating an [entrypoint](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint) produces a [Pregel](https://langchain-ai.github.io/langgraph/reference/pregel/) instance that can be invoked with input.\n\n\n\n\nThis guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel.\n\n> **Note:** The [Pregel](https://langchain-ai.github.io/langgraph/reference/pregel/) runtime is named after [Google's Pregel algorithm](https://research.google/pubs/pub37252/), which describes an efficient method for large-scale parallel computation using graphs.\n\n\n\n\n\n## Overview\n\nIn LangGraph, Pregel combines [**actors**](https://en.wikipedia.org/wiki/Actor_model) and **channels** into a single application. **Actors** read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the **Pregel Algorithm**/**Bulk Synchronous Parallel** model.\n\nEach step consists of three phases:\n\n- **Plan**: Determine which **actors** to execute in this step. For example, in the first step, select the **actors** that subscribe to the special **input** channels; in subsequent steps, select the **actors** that subscribe to channels updated in the previous step.\n- **Execution**: Execute all selected **actors** in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.\n- **Update**: Update the channels with the values written by the **actors** in this step.\n\nRepeat until no **actors** are selected for execution, or a maximum number of steps is reached.\n\n## Actors\n\nAn **actor** is a `PregelNode`. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an **actor** in the Pregel algorithm. `PregelNodes` implement LangChain's Runnable interface.\n\n## Channels\n\nChannels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:\n\n- [LastValue](https://langchain-ai.github.io/langgraph/reference/channels/#langgraph.channels.LastValue): The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.\n- [Topic](https://langchain-ai.github.io/langgraph/reference/channels/#langgraph.channels.Topic): A configurable PubSub Topic, useful for sending multiple values between **actors**, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.\n- [BinaryOperatorAggregate](https://langchain-ai.github.io/langgraph/reference/pregel/#langgraph.pregel.Pregel--advanced-channels-context-and-binaryoperatoraggregate): stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,`total = BinaryOperatorAggregate(int, operator.add)`\n\n\n\n\n## Examples\n\nWhile most users will interact with Pregel through the [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph) API or the [entrypoint](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint) decorator, it is possible to interact with Pregel directly.\n\n\n\n\nBelow are a few different examples to give you a sense of the Pregel API.\n\n=== \"Single node\"\n\n    ```python\n    from langgraph.channels import EphemeralValue\n    from langgraph.pregel import Pregel, NodeBuilder\n\n    node1 = (\n        NodeBuilder().subscribe_only(\"a\")\n        .do(lambda x: x + x)\n        .write_to(\"b\")\n    )\n\n    app = Pregel(\n        nodes={\"node1\": node1},\n        channels={\n            \"a\": EphemeralValue(str),\n            \"b\": EphemeralValue(str),\n        },\n        input_channels=[\"a\"],\n        output_channels=[\"b\"],\n    )\n\n    app.invoke({\"a\": \"foo\"})\n    ```\n\n    ```con\n    {'b': 'foofoo'}\n    ```\n\n\n\n\n=== \"Multiple nodes\"\n\n    ```python\n    from langgraph.channels import LastValue, EphemeralValue\n    from langgraph.pregel import Pregel, NodeBuilder\n\n    node1 = (\n        NodeBuilder().subscribe_only(\"a\")\n        .do(lambda x: x + x)\n        .write_to(\"b\")\n    )\n\n    node2 = (\n        NodeBuilder().subscribe_only(\"b\")\n        .do(lambda x: x + x)\n        .write_to(\"c\")\n    )\n\n\n    app = Pregel(\n        nodes={\"node1\": node1, \"node2\": node2},\n        channels={\n            \"a\": EphemeralValue(str),\n            \"b\": LastValue(str),\n            \"c\": EphemeralValue(str),\n        },\n        input_channels=[\"a\"],\n        output_channels=[\"b\", \"c\"],\n    )\n\n    app.invoke({\"a\": \"foo\"})\n    ```\n\n    ```con\n    {'b': 'foofoo', 'c': 'foofoofoofoo'}\n    ```\n\n\n\n\n=== \"Topic\"\n\n    ```python\n    from langgraph.channels import EphemeralValue, Topic\n    from langgraph.pregel import Pregel, NodeBuilder\n\n    node1 = (\n        NodeBuilder().subscribe_only(\"a\")\n        .do(lambda x: x + x)\n        .write_to(\"b\", \"c\")\n    )\n\n    node2 = (\n        NodeBuilder().subscribe_to(\"b\")\n        .do(lambda x: x[\"b\"] + x[\"b\"])\n        .write_to(\"c\")\n    )\n\n    app = Pregel(\n        nodes={\"node1\": node1, \"node2\": node2},\n        channels={\n            \"a\": EphemeralValue(str),\n            \"b\": EphemeralValue(str),\n            \"c\": Topic(str, accumulate=True),\n        },\n        input_channels=[\"a\"],\n        output_channels=[\"c\"],\n    )\n\n    app.invoke({\"a\": \"foo\"})\n    ```\n\n    ```pycon\n    {'c': ['foofoo', 'foofoofoofoo']}\n    ```\n\n\n\n\n=== \"BinaryOperatorAggregate\"\n\n    This examples demonstrates how to use the BinaryOperatorAggregate channel to implement a reducer.\n\n    ```python\n    from langgraph.channels import EphemeralValue, BinaryOperatorAggregate\n    from langgraph.pregel import Pregel, NodeBuilder\n\n\n    node1 = (\n        NodeBuilder().subscribe_only(\"a\")\n        .do(lambda x: x + x)\n        .write_to(\"b\", \"c\")\n    )\n\n    node2 = (\n        NodeBuilder().subscribe_only(\"b\")\n        .do(lambda x: x + x)\n        .write_to(\"c\")\n    )\n\n    def reducer(current, update):\n        if current:\n            return current + \" | \" + update\n        else:\n            return update\n\n    app = Pregel(\n        nodes={\"node1\": node1, \"node2\": node2},\n        channels={\n            \"a\": EphemeralValue(str),\n            \"b\": EphemeralValue(str),\n            \"c\": BinaryOperatorAggregate(str, operator=reducer),\n        },\n        input_channels=[\"a\"],\n        output_channels=[\"c\"],\n    )\n\n    app.invoke({\"a\": \"foo\"})\n    ```\n\n\n\n\n=== \"Cycle\"\n\n    This example demonstrates how to introduce a cycle in the graph, by having\n    a chain write to a channel it subscribes to. Execution will continue\n    until a `None` value is written to the channel.\n\n    ```python\n    from langgraph.channels import EphemeralValue\n    from langgraph.pregel import Pregel, NodeBuilder, ChannelWriteEntry\n\n    example_node = (\n        NodeBuilder().subscribe_only(\"value\")\n        .do(lambda x: x + x if len(x) < 10 else None)\n        .write_to(ChannelWriteEntry(\"value\", skip_none=True))\n    )\n\n    app = Pregel(\n        nodes={\"example_node\": example_node},\n        channels={\n            \"value\": EphemeralValue(str),\n        },\n        input_channels=[\"value\"],\n        output_channels=[\"value\"],\n    )\n\n    app.invoke({\"value\": \"a\"})\n    ```\n\n    ```pycon\n    {'value': 'aaaaaaaaaaaaaaaa'}\n    ```\n\n\n\n\n## High-level API\n\nLangGraph provides two high-level APIs for creating a Pregel application: the [StateGraph (Graph API)](./low_level.md) and the [Functional API](functional_api.md).\n\n=== \"StateGraph (Graph API)\"\n\n    The [StateGraph (Graph API)](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.\n\n    ```python\n    from typing import TypedDict, Optional\n\n    from langgraph.constants import START\n    from langgraph.graph import StateGraph\n\n    class Essay(TypedDict):\n        topic: str\n        content: Optional[str]\n        score: Optional[float]\n\n    def write_essay(essay: Essay):\n        return {\n            \"content\": f\"Essay about {essay['topic']}\",\n        }\n\n    def score_essay(essay: Essay):\n        return {\n            \"score\": 10\n        }\n\n    builder = StateGraph(Essay)\n    builder.add_node(write_essay)\n    builder.add_node(score_essay)\n    builder.add_edge(START, \"write_essay\")\n\n    # Compile the graph.\n    # This will return a Pregel instance.\n    graph = builder.compile()\n    ```\n\n\n\n\n    The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.\n\n    ```python\n    print(graph.nodes)\n    ```\n\n    You will see something like this:\n\n    ```pycon\n    {'__start__': <langgraph.pregel.read.PregelNode at 0x7d05e3ba1810>,\n     'write_essay': <langgraph.pregel.read.PregelNode at 0x7d05e3ba14d0>,\n     'score_essay': <langgraph.pregel.read.PregelNode at 0x7d05e3ba1710>}\n    ```\n\n    ```python\n    print(graph.channels)\n    ```\n\n    You should see something like this\n\n    ```pycon\n    {'topic': <langgraph.channels.last_value.LastValue at 0x7d05e3294d80>,\n     'content': <langgraph.channels.last_value.LastValue at 0x7d05e3295040>,\n     'score': <langgraph.channels.last_value.LastValue at 0x7d05e3295980>,\n     '__start__': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3297e00>,\n     'write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32960c0>,\n     'score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ab80>,\n     'branch:__start__:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32941c0>,\n     'branch:__start__:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d88800>,\n     'branch:write_essay:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3295ec0>,\n     'branch:write_essay:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ac00>,\n     'branch:score_essay:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d89700>,\n     'branch:score_essay:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b400>,\n     'start:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b280>}\n    ```\n\n\n\n\n=== \"Functional API\"\n\n    In the [Functional API](functional_api.md), you can use an [`entrypoint`](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint) to create a Pregel application. The `entrypoint` decorator allows you to define a function that takes input and returns output.\n\n    ```python\n    from typing import TypedDict, Optional\n\n    from langgraph.checkpoint.memory import InMemorySaver\n    from langgraph.func import entrypoint\n\n    class Essay(TypedDict):\n        topic: str\n        content: Optional[str]\n        score: Optional[float]\n\n\n    checkpointer = InMemorySaver()\n\n    @entrypoint(checkpointer=checkpointer)\n    def write_essay(essay: Essay):\n        return {\n            \"content\": f\"Essay about {essay['topic']}\",\n        }\n\n    print(\"Nodes: \")\n    print(write_essay.nodes)\n    print(\"Channels: \")\n    print(write_essay.channels)\n    ```\n\n    ```pycon\n    Nodes:\n    {'write_essay': <langgraph.pregel.read.PregelNode object at 0x7d05e2f9aad0>}\n    Channels:\n    {'__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d05e2c906c0>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7d05e2c90c40>, '__previous__': <langgraph.channels.last_value.LastValue object at 0x7d05e1007280>}\n    ```",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 12388,
    "word_count": 1145
  },
  {
    "title": "LangGraph Platform Plans",
    "source": "concepts/plans.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph Platform Plans\n\n\n## Overview\nLangGraph Platform is a solution for deploying agentic applications in production.\nThere are three different plans for using it.\n\n- **Developer**: All [LangSmith](https://smith.langchain.com/) users have access to this plan. You can sign up for this plan simply by creating a LangSmith account. This gives you access to the [local deployment](./deployment_options.md#free-deployment) option.\n- **Plus**: All [LangSmith](https://smith.langchain.com/) users with a [Plus account](https://docs.smith.langchain.com/administration/pricing) have access to this plan. You can sign up for this plan simply by upgrading your LangSmith account to the Plus plan type. This gives you access to the [Cloud](./deployment_options.md#cloud-saas) deployment option.\n- **Enterprise**: This is separate from LangSmith plans. You can sign up for this plan by [contacting our sales team](https://www.langchain.com/contact-sales). This gives you access to all [deployment options](./deployment_options.md).\n\n\n## Plan Details\n\n|                                                                  | Developer                                   | Plus                                                  | Enterprise                                          |\n|------------------------------------------------------------------|---------------------------------------------|-------------------------------------------------------|-----------------------------------------------------|\n| Deployment Options                                               | Local                          | Cloud SaaS                                         | <ul><li>Cloud SaaS</li><li>Self-Hosted Data Plane</li><li>Self-Hosted Control Plane</li><li>Standalone Container</li></ul> |\n| Usage                                                            | Free | See [Pricing](https://www.langchain.com/langgraph-platform-pricing) | Custom                                              |\n| APIs for retrieving and updating state and conversational history | ✅                                           | ✅                                                     | ✅                                                   |\n| APIs for retrieving and updating long-term memory                | ✅                                           | ✅                                                     | ✅                                                   |\n| Horizontally scalable task queues and servers                    | ✅                                           | ✅                                                     | ✅                                                   |\n| Real-time streaming of outputs and intermediate steps            | ✅                                           | ✅                                                     | ✅                                                   |\n| Assistants API (configurable templates for LangGraph apps)       | ✅                                           | ✅                                                     | ✅                                                   |\n| Cron scheduling                                                  | --                                          | ✅                                                     | ✅                                                   |\n| LangGraph Studio for prototyping                                 | \t✅                                         | ✅                                                    | ✅                                                  |\n| Authentication & authorization to call the LangGraph APIs        | --                                          | Coming Soon!                                          | Coming Soon!                                        |\n| Smart caching to reduce traffic to LLM API                       | --                                          | Coming Soon!                                          | Coming Soon!                                        |\n| Publish/subscribe API for state                                  | --                                          | Coming Soon!                                          | Coming Soon!                                        |\n| Scheduling prioritization                                        | --                                          | Coming Soon!                                          | Coming Soon!                                        |\n\nFor pricing information, see [LangGraph Platform Pricing](https://www.langchain.com/langgraph-platform-pricing).\n\n## Related\n\nFor more information, please see:\n\n* [Deployment Options conceptual guide](./deployment_options.md)\n* [LangGraph Platform Pricing](https://www.langchain.com/langgraph-platform-pricing)\n* [LangSmith Plans](https://docs.smith.langchain.com/administration/pricing)",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 4864,
    "word_count": 356
  },
  {
    "title": "Template Applications",
    "source": "concepts/template_applications.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Template Applications\n\nTemplates are open source reference applications designed to help you get started quickly when building with LangGraph. They provide working examples of common agentic workflows that can be customized to your needs.\n\nYou can create an application from a template using the LangGraph CLI.\n\n!!! info \"Requirements\"\n\n    - Python >= 3.11\n    - [LangGraph CLI](https://langchain-ai.github.io/langgraph/cloud/reference/cli/): Requires langchain-cli[inmem] >= 0.1.58\n\n## Install the LangGraph CLI\n\n```bash\npip install \"langgraph-cli[inmem]\" --upgrade\n```\n\nOr via [`uv`](https://docs.astral.sh/uv/getting-started/installation/) (recommended):\n\n```bash\nuvx --from \"langgraph-cli[inmem]\" langgraph dev --help\n```\n\n\n\n\n\n## Available Templates\n\n| Template | Description | Link |\n| -------- | ----------- | ------ |\n| **New LangGraph Project** | A simple, minimal chatbot with memory. | [Repo](https://github.com/langchain-ai/new-langgraph-project) |\n| **ReAct Agent** | A simple agent that can be flexibly extended to many tools. | [Repo](https://github.com/langchain-ai/react-agent) |\n| **Memory Agent** | A ReAct-style agent with an additional tool to store memories for use across threads. | [Repo](https://github.com/langchain-ai/memory-agent) |\n| **Retrieval Agent** | An agent that includes a retrieval-based question-answering system. | [Repo](https://github.com/langchain-ai/retrieval-agent-template) |\n| **Data-Enrichment Agent** | An agent that performs web searches and organizes its findings into a structured format. | [Repo](https://github.com/langchain-ai/data-enrichment) |\n\n\n\n\n\n## 🌱 Create a LangGraph App\n\nTo create a new app from a template, use the `langgraph new` command.\n\n```bash\nlanggraph new\n```\n\nOr via [`uv`](https://docs.astral.sh/uv/getting-started/installation/) (recommended):\n\n```bash\nuvx --from \"langgraph-cli[inmem]\" langgraph new\n```\n\n\n\n\n\n## Next Steps\n\nReview the `README.md` file in the root of your new LangGraph app for more information about the template and how to customize it.\n\nAfter configuring the app properly and adding your API keys, you can start the app using the LangGraph CLI:\n\n```bash\nlanggraph dev\n```\n\nOr via [`uv`](https://docs.astral.sh/uv/getting-started/installation/) (recommended):\n\n```bash\nuvx --from \"langgraph-cli[inmem]\" --with-editable . langgraph dev\n```\n\n!!! info \"Missing Local Package?\"\n\n    If you are not using `uv` and run into a \"`ModuleNotFoundError`\" or \"`ImportError`\", even after installing the local package (`pip install -e .`), it is likely the case that you need to install the CLI into your local virtual environment to make the CLI \"aware\" of the local package. You can do this by running `python -m pip install \"langgraph-cli[inmem]\"` and re-activating your virtual environment before running `langgraph dev`.\n\n\n\n\n\nSee the following guides for more information on how to deploy your app:\n\n- **[Launch Local LangGraph Server](../tutorials/langgraph-platform/local-server.md)**: This quick start guide shows how to start a LangGraph Server locally for the **ReAct Agent** template. The steps are similar for other templates.\n- **[Deploy to LangGraph Platform](../cloud/quick_start.md)**: Deploy your LangGraph app using LangGraph Platform.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3263,
    "word_count": 419
  },
  {
    "title": "Tools",
    "source": "concepts/tools.md",
    "content": "# Tools\n\nMany AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems—such as APIs, databases, or file systems—using structured input. In these scenarios, [tool calling](../how-tos/tool-calling.md) enables models to generate requests that conform to a specified input schema.\n\n**Tools** encapsulate a callable function and its input schema. These can be passed to compatible [chat models](https://python.langchain.com/docs/concepts/chat_models), allowing the model to decide whether to invoke a tool and with what arguments.\n\n\n\n\n## Tool calling\n\n![Diagram of a tool call by a model](./img/tool_call.png)\n\nTool calling is typically **conditional**. Based on the user input and available tools, the model may choose to issue a tool call request. This request is returned in an `AIMessage` object, which includes a `tool_calls` field that specifies the tool name and input arguments:\n\n```python\nllm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n# -> AIMessage(tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, ...}])\n```\n\n```\nAIMessage(\n  tool_calls=[\n    ToolCall(name=\"multiply\", args={\"a\": 2, \"b\": 3}),\n    ...\n  ]\n)\n```\n\n\n\n\n\nIf the input is unrelated to any tool, the model returns only a natural language message:\n\n```python\nllm_with_tools.invoke(\"Hello world!\")  # -> AIMessage(content=\"Hello!\")\n```\n\n\n\n\n\nImportantly, the model does not execute the tool—it only generates a request. A separate executor (such as a runtime or agent) is responsible for handling the tool call and returning the result.\n\nSee the [tool calling guide](../how-tos/tool-calling.md) for more details.\n\n## Prebuilt tools\n\nLangChain provides prebuilt tool integrations for common external systems including APIs, databases, file systems, and web data.\n\nBrowse the [integrations directory](https://python.langchain.com/docs/integrations/tools/) for available tools.\n\n\n\n\nCommon categories:\n\n- **Search**: Bing, SerpAPI, Tavily\n- **Code execution**: Python REPL, Node.js REPL\n- **Databases**: SQL, MongoDB, Redis\n- **Web data**: Scraping and browsing\n- **APIs**: OpenWeatherMap, NewsAPI, etc.\n\n## Custom tools\n\nYou can define custom tools using the `@tool` decorator or plain Python functions. For example:\n\n```python\nfrom langchain_core.tools import tool\n\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n```\n\n\n\n\n\nSee the [tool calling guide](../how-tos/tool-calling.md) for more details.\n\n## Tool execution\n\nWhile the model determines when to call a tool, execution of the tool call must be handled by a runtime component.\n\nLangGraph provides prebuilt components for this:\n\n- [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode): A prebuilt node that executes tools.\n- [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent): Constructs a full agent that manages tool calling automatically.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3049,
    "word_count": 380
  },
  {
    "title": "Subgraphs",
    "source": "concepts/subgraphs.md",
    "content": "# Subgraphs\n\nA subgraph is a [graph](./low_level.md#graphs) that is used as a [node](./low_level.md#nodes) in another graph — this is the concept of encapsulation applied to LangGraph. Subgraphs allow you to build complex systems with multiple components that are themselves graphs.\n\n![Subgraph](./img/subgraph.png)\n\nSome reasons for using subgraphs are:\n\n- building [multi-agent systems](./multi_agent.md)\n- when you want to reuse a set of nodes in multiple graphs\n- when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph\n\nThe main question when adding subgraphs is how the parent graph and subgraph communicate, i.e. how they pass the [state](./low_level.md#state) between each other during the graph execution. There are two scenarios:\n\n- parent and subgraph have **shared state keys** in their state [schemas](./low_level.md#state). In this case, you can [include the subgraph as a node in the parent graph](../how-tos/subgraph.ipynb#shared-state-schemas)\n\n  ```python hl_lines=\"12 17\"\n  from langgraph.graph import StateGraph, MessagesState, START\n\n  # Subgraph\n\n  def call_model(state: MessagesState):\n      response = model.invoke(state[\"messages\"])\n      return {\"messages\": response}\n\n  subgraph_builder = StateGraph(State)\n  subgraph_builder.add_node(call_model)\n  ...\n  subgraph = subgraph_builder.compile()\n\n  # Parent graph\n\n  builder = StateGraph(State)\n  builder.add_node(\"subgraph_node\", subgraph)\n  builder.add_edge(START, \"subgraph_node\")\n  graph = builder.compile()\n  ...\n  graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n  ```\n\n\n\n\n\n- parent graph and subgraph have **different schemas** (no shared state keys in their state [schemas](./low_level.md#state)). In this case, you have to [call the subgraph from inside a node in the parent graph](../how-tos/subgraph.ipynb#different-state-schemas): this is useful when the parent graph and the subgraph have different state schemas and you need to transform state before or after calling the subgraph\n\n  ```python hl_lines=\"7 11 19 28\"\n  from typing_extensions import TypedDict, Annotated\n  from langchain_core.messages import AnyMessage\n  from langgraph.graph import StateGraph, MessagesState, START\n  from langgraph.graph.message import add_messages\n\n  class SubgraphMessagesState(TypedDict):\n      subgraph_messages: Annotated[list[AnyMessage], add_messages]\n\n  # Subgraph\n\n  def call_model(state: SubgraphMessagesState):\n      response = model.invoke(state[\"subgraph_messages\"])\n      return {\"subgraph_messages\": response}\n\n  subgraph_builder = StateGraph(SubgraphMessagesState)\n  subgraph_builder.add_node(\"call_model_from_subgraph\", call_model)\n  subgraph_builder.add_edge(START, \"call_model_from_subgraph\")\n  ...\n  subgraph = subgraph_builder.compile()\n\n  # Parent graph\n\n  def call_subgraph(state: MessagesState):\n      response = subgraph.invoke({\"subgraph_messages\": state[\"messages\"]})\n      return {\"messages\": response[\"subgraph_messages\"]}\n\n  builder = StateGraph(State)\n  builder.add_node(\"subgraph_node\", call_subgraph)\n  builder.add_edge(START, \"subgraph_node\")\n  graph = builder.compile()\n  ...\n  graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n  ```",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3396,
    "word_count": 360
  },
  {
    "title": "Cloud SaaS",
    "source": "concepts/langgraph_cloud.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Cloud SaaS\n\nTo deploy a [LangGraph Server](../concepts/langgraph_server.md), follow the how-to guide for [how to deploy to Cloud SaaS](../cloud/deployment/cloud.md).\n\n## Overview\n\nThe Cloud SaaS deployment option is a fully managed model for deployment where we manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in our cloud.\n\n|                                    | [Control plane](../concepts/langgraph_control_plane.md)                                                                                     | [Data plane](../concepts/langgraph_data_plane.md)                                                                                                   |\n| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **What is it?**                    | <ul><li>Control plane UI for creating deployments and revisions</li><li>Control plane APIs for creating deployments and revisions</li></ul> | <ul><li>Data plane \"listener\" for reconciling deployments with control plane state</li><li>LangGraph Servers</li><li>Postgres, Redis, etc</li></ul> |\n| **Where is it hosted?**            | LangChain's cloud                                                                                                                           | LangChain's cloud                                                                                                                                   |\n| **Who provisions and manages it?** | LangChain                                                                                                                                   | LangChain                                                                                                                                           |\n\n## Architecture\n\n![Cloud SaaS](./img/self_hosted_control_plane_architecture.png)",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 2144,
    "word_count": 127
  },
  {
    "title": "concepts/langgraph_components.md",
    "source": "concepts/langgraph_components.md",
    "content": "## Components\n\nThe LangGraph Platform consists of components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications:\n\n- [LangGraph Server](./langgraph_server.md): The server defines an opinionated API and architecture that incorporates best practices for deploying agentic applications, allowing you to focus on building your agent logic rather than developing server infrastructure.\n- [LangGraph CLI](./langgraph_cli.md): LangGraph CLI is a command-line interface that helps to interact with a local LangGraph\n- [LangGraph Studio](./langgraph_studio.md): LangGraph Studio is a specialized IDE that can connect to a LangGraph Server to enable visualization, interaction, and debugging of the application locally.\n- [Python/JS SDK](./sdk.md): The Python/JS SDK provides a programmatic way to interact with deployed LangGraph Applications.\n- [Remote Graph](../how-tos/use-remote-graph.md): A RemoteGraph allows you to interact with any deployed LangGraph application as though it were running locally.\n- [LangGraph control plane](./langgraph_control_plane.md): The LangGraph Control Plane refers to the Control Plane UI where users create and update LangGraph Servers and the Control Plane APIs that support the UI experience.\n- [LangGraph data plane](./langgraph_data_plane.md): The LangGraph Data Plane refers to LangGraph Servers, the corresponding infrastructure for each server, and the \"listener\" application that continuously polls for updates from the LangGraph Control Plane.\n\n![LangGraph components](img/lg_platform.png)",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 1584,
    "word_count": 199
  },
  {
    "title": "Standalone Container",
    "source": "concepts/langgraph_standalone_container.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Standalone Container\n\nTo deploy a [LangGraph Server](../concepts/langgraph_server.md), follow the how-to guide for [how to deploy a Standalone Container](../cloud/deployment/standalone_container.md).\n\n## Overview\n\nThe Standalone Container deployment option is the least restrictive model for deployment. There is no [control plane](./langgraph_control_plane.md). [Data plane](./langgraph_data_plane.md) infrastructure is managed by you.\n\n|                   | [Control plane](../concepts/langgraph_control_plane.md) | [Data plane](../concepts/langgraph_data_plane.md) |\n|-------------------|-------------------|------------|\n| **What is it?** | n/a | <ul><li>LangGraph Servers</li><li>Postgres, Redis, etc</li></ul> |\n| **Where is it hosted?** | n/a | Your cloud |\n| **Who provisions and manages it?** | n/a | You |\n\n!!! warning\n\n      LangGraph Platform should not be deployed in serverless environments. Scale to zero may cause task loss and scaling up will not work reliably.\n\n## Architecture\n\n![Standalone Container](./img/langgraph_platform_deployment_architecture.png)\n\n## Compute Platforms\n\n### Kubernetes\n\nThe Standalone Container deployment option supports deploying data plane infrastructure to a Kubernetes cluster.\n\n### Docker\n\nThe Standalone Container deployment option supports deploying data plane infrastructure to any Docker-supported compute platform.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 1399,
    "word_count": 158
  },
  {
    "title": "Human-in-the-loop",
    "source": "concepts/human_in_the_loop.md",
    "content": "---\nsearch:\n  boost: 2\ntags:\n  - human-in-the-loop\n  - hil\n  - overview\nhide:\n  - tags\n---\n\n# Human-in-the-loop\n\nTo review, edit, and approve tool calls in an agent or workflow, [use LangGraph's human-in-the-loop features](../how-tos/human_in_the_loop/add-human-in-the-loop.md) to enable human intervention at any point in a workflow. This is especially useful in large language model (LLM)-driven applications where model output may require validation, correction, or additional context.\n\n<figure markdown=\"1\">\n![image](../concepts/img/human_in_the_loop/tool-call-review.png){: style=\"max-height:400px\"}\n</figure>\n\n!!! tip\n\n    For information on how to use human-in-the-loop, see [Enable human intervention](../how-tos/human_in_the_loop/add-human-in-the-loop.md) and [Human-in-the-loop using Server API](../cloud/how-tos/add-human-in-the-loop.md).\n\n## Key capabilities\n\n* **Persistent execution state**: Interrupts use LangGraph's [persistence](./persistence.md) layer, which saves the graph state, to indefinitely pause graph execution until you resume. This is possible because LangGraph checkpoints the graph state after each step, which allows the system to persist execution context and later resume the workflow, continuing from where it left off. This supports asynchronous human review or input without time constraints.\n\n    There are two ways to pause a graph:\n\n    - [Dynamic interrupts](../how-tos/human_in_the_loop/add-human-in-the-loop.md#pause-using-interrupt): Use `interrupt` to pause a graph from inside a specific node, based on the current state of the graph.\n    - [Static interrupts](../how-tos/human_in_the_loop/add-human-in-the-loop.md#debug-with-interrupts): Use `interrupt_before` and `interrupt_after` to pause the graph at pre-defined points, either before or after a node executes.\n\n    <figure markdown=\"1\">\n    ![image](./img/breakpoints.png){: style=\"max-height:400px\"}\n    <figcaption>An example graph consisting of 3 sequential steps with a breakpoint before step_3. </figcaption> </figure>\n\n* **Flexible integration points**: Human-in-the-loop logic can be introduced at any point in the workflow. This allows targeted human involvement, such as approving API calls, correcting outputs, or guiding conversations.\n\n## Patterns\n\nThere are four typical design patterns that you can implement using `interrupt` and `Command`:\n\n- [Approve or reject](../how-tos/human_in_the_loop/add-human-in-the-loop.md#approve-or-reject): Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involves routing the graph based on the human's input.\n- [Edit graph state](../how-tos/human_in_the_loop/add-human-in-the-loop.md#review-and-edit-state): Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves updating the state with the human's input.\n- [Review tool calls](../how-tos/human_in_the_loop/add-human-in-the-loop.md#review-tool-calls): Pause the graph to review and edit tool calls requested by the LLM before tool execution.\n- [Validate human input](../how-tos/human_in_the_loop/add-human-in-the-loop.md#validate-human-input): Pause the graph to validate human input before proceeding with the next step.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3424,
    "word_count": 397
  },
  {
    "title": "MCP endpoint in LangGraph Server",
    "source": "concepts/server-mcp.md",
    "content": "---\ntags:\n  - mcp\n  - platform\nhide:\n  - tags\n---\n\n# MCP endpoint in LangGraph Server\n\nThe [Model Context Protocol (MCP)](./mcp.md) is an open protocol for describing tools and data sources in a model-agnostic format, enabling LLMs to discover and use them via a structured API.\n\n[LangGraph Server](./langgraph_server.md) implements MCP using the [Streamable HTTP transport](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/transports/#streamable-http). This allows LangGraph **agents** to be exposed as **MCP tools**, making them usable with any MCP-compliant client supporting Streamable HTTP.\n\nThe MCP endpoint is available at `/mcp` on [LangGraph Server](./langgraph_server.md).\n\n## Requirements\n\nTo use MCP, ensure you have the following dependencies installed:\n\n- `langgraph-api >= 0.2.3`\n- `langgraph-sdk >= 0.1.61`\n\nInstall them with:\n\n```bash\npip install \"langgraph-api>=0.2.3\" \"langgraph-sdk>=0.1.61\"\n```\n\n\n\n\n\n## Exposing an agent as MCP tool\n\nWhen deployed, your agent will appear as a tool in the MCP endpoint\nwith this configuration:\n\n- **Tool name**: The agent's name.\n- **Tool description**: The agent's description.\n- **Tool input schema**: The agent's input schema.\n\n### Setting name and description\n\nYou can set the name and description of your agent in `langgraph.json`:\n\n```json\n{\n  \"graphs\": {\n    \"my_agent\": {\n      \"path\": \"./my_agent/agent.py:graph\",\n      \"description\": \"A description of what the agent does\"\n    }\n  },\n  \"env\": \".env\"\n}\n```\n\n\n\n\nAfter deployment, you can update the name and description using the LangGraph SDK.\n\n### Schema\n\nDefine clear, minimal input and output schemas to avoid exposing unnecessary internal complexity to the LLM.\n\nThe default [MessagesState](./low_level.md#messagesstate) uses `AnyMessage`, which supports many message types but is too general for direct LLM exposure.\n\n\nInstead, define **custom agents or workflows** that use explicitly typed input and output structures.\n\nFor example, a workflow answering documentation questions might look like this:\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# Define input schema\nclass InputState(TypedDict):\n    question: str\n\n# Define output schema\nclass OutputState(TypedDict):\n    answer: str\n\n# Combine input and output\nclass OverallState(InputState, OutputState):\n    pass\n\n# Define the processing node\ndef answer_node(state: InputState):\n    # Replace with actual logic and do something useful\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n\n# Build the graph with explicit schemas\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(answer_node)\nbuilder.add_edge(START, \"answer_node\")\nbuilder.add_edge(\"answer_node\", END)\ngraph = builder.compile()\n\n# Run the graph\nprint(graph.invoke({\"question\": \"hi\"}))\n```\n\nFor more details, see the [low-level concepts guide](https://langchain-ai.github.io/langgraph/concepts/low_level/#state).\n\n## Usage overview\n\nTo enable MCP:\n\n- Upgrade to use langgraph-api>=0.2.3. If you are deploying LangGraph Platform, this will be done for you automatically if you create a new revision.\n- MCP tools (agents) will be automatically exposed.\n- Connect with any MCP-compliant client that supports Streamable HTTP.\n\n### Client\n\nUse an MCP-compliant client to connect to the LangGraph server. The following example shows how to connect using [langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters).\n\nInstall the adapter with:\n\n```bash\npip install langchain-mcp-adapters\n```\n\nHere is an example of how to connect to a remote MCP endpoint and use an agent as a tool:\n\n```python\n# Create server parameters for stdio connection\nfrom mcp import ClientSession\nfrom mcp.client.streamable_http import streamablehttp_client\nimport asyncio\n\nfrom langchain_mcp_adapters.tools import load_mcp_tools\nfrom langgraph.prebuilt import create_react_agent\n\nserver_params = {\n    \"url\": \"https://mcp-finance-agent.xxx.us.langgraph.app/mcp\",\n    \"headers\": {\n        \"X-Api-Key\":\"lsv2_pt_your_api_key\"\n    }\n}\n\nasync def main():\n    async with streamablehttp_client(**server_params) as (read, write, _):\n        async with ClientSession(read, write) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # Load the remote graph as if it was a tool\n            tools = await load_mcp_tools(session)\n\n            # Create and run a react agent with the tools\n            agent = create_react_agent(\"openai:gpt-4.1\", tools)\n\n            # Invoke the agent with a message\n            agent_response = await agent.ainvoke({\"messages\": \"What can the finance agent do for me?\"})\n            print(agent_response)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n\n\n\n\n## Session behavior\n\nThe current LangGraph MCP implementation does not support sessions. Each `/mcp` request is stateless and independent.\n\n## Authentication\n\nThe `/mcp` endpoint uses the same authentication as the rest of the LangGraph API. Refer to the [authentication guide](./auth.md) for setup details.\n\n## Disable MCP\n\nTo disable the MCP endpoint, set `disable_mcp` to `true` in your `langgraph.json` configuration file:\n\n```json\n{\n  \"http\": {\n    \"disable_mcp\": true\n  }\n}\n```\n\nThis will prevent the server from exposing the `/mcp` endpoint.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 5350,
    "word_count": 653
  },
  {
    "title": "Self-Hosted Data Plane",
    "source": "concepts/langgraph_self_hosted_data_plane.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Self-Hosted Data Plane\n\nThere are two versions of the self-hosted deployment: [Self-Hosted Data Plane](./deployment_options.md#self-hosted-data-plane) and [Self-Hosted Control Plane](./deployment_options.md#self-hosted-control-plane).\n\n!!! info \"Important\"\n\n    The Self-Hosted Data Plane deployment option requires an [Enterprise](plans.md) plan.\n\n## Requirements\n\n- You use `langgraph-cli` and/or [LangGraph Studio](./langgraph_studio.md) app to test graph locally.\n- You use `langgraph build` command to build image.\n\n## Self-Hosted Data Plane\n\nThe [Self-Hosted Data Plane](../cloud/deployment/self_hosted_data_plane.md) deployment option is a \"hybrid\" model for deployment where we manage the [control plane](./langgraph_control_plane.md) in our cloud and you manage the [data plane](./langgraph_data_plane.md) in your cloud. This option provides a way to securely manage your data plane infrastructure, while offloading control plane management to us. When using the Self-Hosted Data Plane version, you authenticate with a [LangSmith](https://smith.langchain.com/) API key.\n\n|                                    | [Control plane](../concepts/langgraph_control_plane.md)                                                                                     | [Data plane](../concepts/langgraph_data_plane.md)                                                                                                   |\n| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **What is it?**                    | <ul><li>Control plane UI for creating deployments and revisions</li><li>Control plane APIs for creating deployments and revisions</li></ul> | <ul><li>Data plane \"listener\" for reconciling deployments with control plane state</li><li>LangGraph Servers</li><li>Postgres, Redis, etc</li></ul> |\n| **Where is it hosted?**            | LangChain's cloud                                                                                                                           | Your cloud                                                                                                                                          |\n| **Who provisions and manages it?** | LangChain                                                                                                                                   | You                                                                                                                                                 |\n\nFor information on how to deploy a [LangGraph Server](../concepts/langgraph_server.md) to Self-Hosted Data Plane, see [Deploy to Self-Hosted Data Plane](../cloud/deployment/self_hosted_data_plane.md)\n\n### Architecture\n\n![Self-Hosted Data Plane Architecture](./img/self_hosted_data_plane_architecture.png)\n\n### Compute Platforms\n\n- **Kubernetes**: The Self-Hosted Data Plane deployment option supports deploying data plane infrastructure to any Kubernetes cluster.\n- **Amazon ECS**: Coming soon!\n\n!!! tip\nIf you would like to deploy to Kubernetes, you can follow the [Self-Hosted Data Plane deployment guide](../cloud/deployment/self_hosted_data_plane.md).",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3421,
    "word_count": 269
  },
  {
    "title": "Streaming",
    "source": "concepts/streaming.md",
    "content": "---\nsearch:\nboost: 2\n---\n\n# Streaming\n\nLangGraph implements a streaming system to surface real-time updates, allowing for responsive and transparent user experiences.\n\nLangGraph’s streaming system lets you surface live feedback from graph runs to your app.  \nThere are three main categories of data you can stream:\n\n1. **Workflow progress** — get state updates after each graph node is executed.\n2. **LLM tokens** — stream language model tokens as they’re generated.\n3. **Custom updates** — emit user-defined signals (e.g., “Fetched 10/100 records”).\n\n## What’s possible with LangGraph streaming\n\n- [**Stream LLM tokens**](../how-tos/streaming.md#messages) — capture token streams from anywhere: inside nodes, subgraphs, or tools.\n- [**Emit progress notifications from tools**](../how-tos/streaming.md#stream-custom-data) — send custom updates or progress signals directly from tool functions.\n- [**Stream from subgraphs**](../how-tos/streaming.md#stream-subgraph-outputs) — include outputs from both the parent graph and any nested subgraphs.\n- [**Use any LLM**](../how-tos/streaming.md#use-with-any-llm) — stream tokens from any LLM, even if it's not a LangChain model using the `custom` streaming mode.\n- [**Use multiple streaming modes**](../how-tos/streaming.md#stream-multiple-modes) — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 1450,
    "word_count": 185
  },
  {
    "title": "Functional API concepts",
    "source": "concepts/functional_api.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Functional API concepts\n\n## Overview\n\nThe **Functional API** allows you to add LangGraph's key features — [persistence](./persistence.md), [memory](../how-tos/memory/add-memory.md), [human-in-the-loop](./human_in_the_loop.md), and [streaming](./streaming.md) — to your applications with minimal changes to your existing code.\n\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as `if` statements, `for` loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\n\nThe Functional API uses two key building blocks:\n\n- **`@entrypoint`** – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\n- **`@task`** – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.  \n\n\n\n\nThis provides a minimal abstraction for building workflows with state management and streaming.\n\n!!! tip\n\n    For information on how to use the functional API, see [Use Functional API](../how-tos/use-functional-api.md).\n\n## Functional API vs. Graph API\n\nFor users who prefer a more declarative approach, LangGraph's [Graph API](./low_level.md) allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\n\nHere are some key differences:\n\n- **Control flow**: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\n- **Short-term memory**: The **GraphAPI** requires declaring a [**State**](./low_level.md#state) and may require defining [**reducers**](./low_level.md#reducers) to manage updates to the graph state. `@entrypoint` and `@tasks` do not require explicit state management as their state is scoped to the function and is not shared across functions.\n- **Checkpointing**: Both APIs generate and use checkpoints. In the **Graph API** a new checkpoint is generated after every [superstep](./low_level.md). In the **Functional API**, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.\n- **Visualization**: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.\n\n## Example\n\nBelow we demonstrate a simple application that writes an essay and [interrupts](human_in_the_loop.md) to request human review.\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import interrupt\n\n@task\ndef write_essay(topic: str) -> str:\n    \"\"\"Write an essay about the given topic.\"\"\"\n    time.sleep(1) # A placeholder for a long-running task.\n    return f\"An essay about topic: {topic}\"\n\n@entrypoint(checkpointer=InMemorySaver())\ndef workflow(topic: str) -> dict:\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\n    essay = write_essay(\"cat\").result()\n    is_approved = interrupt({\n        # Any json-serializable payload provided to interrupt as argument.\n        # It will be surfaced on the client side as an Interrupt when streaming data\n        # from the workflow.\n        \"essay\": essay, # The essay we want reviewed.\n        # We can add any additional information that we need.\n        # For example, introduce a key called \"action\" with some instructions.\n        \"action\": \"Please approve/reject the essay\",\n    })\n\n    return {\n        \"essay\": essay, # The essay that was generated\n        \"is_approved\": is_approved, # Response from HIL\n    }\n```\n\n\n\n\n\n??? example \"Detailed Explanation\"\n\n    This workflow will write an essay about the topic \"cat\" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.\n\n    When the workflow is resumed, it executes from the very start, but because the result of the `writeEssay` task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.\n\n    ```python\n    import time\n    import uuid\n    from langgraph.func import entrypoint, task\n    from langgraph.types import interrupt\n    from langgraph.checkpoint.memory import InMemorySaver\n\n\n    @task\n    def write_essay(topic: str) -> str:\n        \"\"\"Write an essay about the given topic.\"\"\"\n        time.sleep(1)  # This is a placeholder for a long-running task.\n        return f\"An essay about topic: {topic}\"\n\n    @entrypoint(checkpointer=InMemorySaver())\n    def workflow(topic: str) -> dict:\n        \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\n        essay = write_essay(\"cat\").result()\n        is_approved = interrupt(\n            {\n                # Any json-serializable payload provided to interrupt as argument.\n                # It will be surfaced on the client side as an Interrupt when streaming data\n                # from the workflow.\n                \"essay\": essay,  # The essay we want reviewed.\n                # We can add any additional information that we need.\n                # For example, introduce a key called \"action\" with some instructions.\n                \"action\": \"Please approve/reject the essay\",\n            }\n        )\n        return {\n            \"essay\": essay,  # The essay that was generated\n            \"is_approved\": is_approved,  # Response from HIL\n        }\n\n\n    thread_id = str(uuid.uuid4())\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n    for item in workflow.stream(\"cat\", config):\n        print(item)\n    # > {'write_essay': 'An essay about topic: cat'}\n    # > {\n    # >     '__interrupt__': (\n    # >        Interrupt(\n    # >            value={\n    # >                'essay': 'An essay about topic: cat',\n    # >                'action': 'Please approve/reject the essay'\n    # >            },\n    # >            id='b9b2b9d788f482663ced6dc755c9e981'\n    # >        ),\n    # >    )\n    # > }\n    ```\n\n    An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:\n\n    ```python\n    from langgraph.types import Command\n\n    # Get review from a user (e.g., via a UI)\n    # In this case, we're using a bool, but this can be any json-serializable value.\n    human_review = True\n\n    for item in workflow.stream(Command(resume=human_review), config):\n        print(item)\n    ```\n\n    ```pycon\n    {'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}\n    ```\n\n    The workflow has been completed and the review has been added to the essay.\n\n\n\n\n## Entrypoint\n\nThe [`@entrypoint`](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint) decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling _long-running tasks_ and [interrupts](./human_in_the_loop.md).\n\n\n\n\n### Definition\n\nAn **entrypoint** is defined by decorating a function with the `@entrypoint` decorator.\n\nThe function **must accept a single positional argument**, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\n\nDecorating a function with an `entrypoint` produces a [`Pregel`](https://langchain-ai.github.io/langgraph/reference/pregel/#langgraph.pregel.Pregel.stream) instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\n\nYou will usually want to pass a **checkpointer** to the `@entrypoint` decorator to enable persistence and use features like **human-in-the-loop**.\n\n=== \"Sync\"\n\n    ```python\n    from langgraph.func import entrypoint\n\n    @entrypoint(checkpointer=checkpointer)\n    def my_workflow(some_input: dict) -> int:\n        # some logic that may involve long-running tasks like API calls,\n        # and may be interrupted for human-in-the-loop.\n        ...\n        return result\n    ```\n\n=== \"Async\"\n\n    ```python\n    from langgraph.func import entrypoint\n\n    @entrypoint(checkpointer=checkpointer)\n    async def my_workflow(some_input: dict) -> int:\n        # some logic that may involve long-running tasks like API calls,\n        # and may be interrupted for human-in-the-loop\n        ...\n        return result\n    ```\n\n\n\n\n\n!!! important \"Serialization\"\n\n    The **inputs** and **outputs** of entrypoints must be JSON-serializable to support checkpointing. Please see the [serialization](#serialization) section for more details.\n\n### Injectable parameters\n\nWhen declaring an `entrypoint`, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\n\n| Parameter    | Description                                                                                                                                                        |\n| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **previous** | Access the state associated with the previous `checkpoint` for the given thread. See [short-term-memory](#short-term-memory).                                      |\n| **store**    | An instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for [long-term memory](../how-tos/use-functional-api.md#long-term-memory).                      |\n| **writer**   | Use to access the StreamWriter when working with Async Python < 3.11. See [streaming with functional API for details](../how-tos/use-functional-api.md#streaming). |\n| **config**   | For accessing run time configuration. See [RunnableConfig](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) for information.                  |\n\n!!! important\n\n    Declare the parameters with the appropriate name and type annotation.\n\n??? example \"Requesting Injectable Parameters\"\n\n    ```python\n    from langchain_core.runnables import RunnableConfig\n    from langgraph.func import entrypoint\n    from langgraph.store.base import BaseStore\n    from langgraph.store.memory import InMemoryStore\n\n    in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\n\n    @entrypoint(\n        checkpointer=checkpointer,  # Specify the checkpointer\n        store=in_memory_store  # Specify the store\n    )\n    def my_workflow(\n        some_input: dict,  # The input (e.g., passed via `invoke`)\n        *,\n        previous: Any = None, # For short-term memory\n        store: BaseStore,  # For long-term memory\n        writer: StreamWriter,  # For streaming custom data\n        config: RunnableConfig  # For accessing the configuration passed to the entrypoint\n    ) -> ...:\n    ```\n\n\n\n### Executing\n\nUsing the [`@entrypoint`](#entrypoint) yields a [`Pregel`](https://langchain-ai.github.io/langgraph/reference/pregel/#langgraph.pregel.Pregel.stream) object that can be executed using the `invoke`, `ainvoke`, `stream`, and `astream` methods.\n\n=== \"Invoke\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n    my_workflow.invoke(some_input, config)  # Wait for the result synchronously\n    ```\n\n=== \"Async Invoke\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n    await my_workflow.ainvoke(some_input, config)  # Await result asynchronously\n    ```\n\n=== \"Stream\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    for chunk in my_workflow.stream(some_input, config):\n        print(chunk)\n    ```\n\n=== \"Async Stream\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    async for chunk in my_workflow.astream(some_input, config):\n        print(chunk)\n    ```\n\n\n\n\n\n### Resuming\n\nResuming an execution after an [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Interrupt) can be done by passing a **resume** value to the [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) primitive.\n\n=== \"Invoke\"\n\n    ```python\n    from langgraph.types import Command\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    my_workflow.invoke(Command(resume=some_resume_value), config)\n    ```\n\n=== \"Async Invoke\"\n\n    ```python\n    from langgraph.types import Command\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    await my_workflow.ainvoke(Command(resume=some_resume_value), config)\n    ```\n\n=== \"Stream\"\n\n    ```python\n    from langgraph.types import Command\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    for chunk in my_workflow.stream(Command(resume=some_resume_value), config):\n        print(chunk)\n    ```\n\n=== \"Async Stream\"\n\n    ```python\n    from langgraph.types import Command\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    async for chunk in my_workflow.astream(Command(resume=some_resume_value), config):\n        print(chunk)\n    ```\n\n\n\n\n\n**Resuming after an error**\n\nTo resume after an error, run the `entrypoint` with a `None` and the same **thread id** (config).\n\nThis assumes that the underlying **error** has been resolved and execution can proceed successfully.\n\n=== \"Invoke\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    my_workflow.invoke(None, config)\n    ```\n\n=== \"Async Invoke\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    await my_workflow.ainvoke(None, config)\n    ```\n\n=== \"Stream\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    for chunk in my_workflow.stream(None, config):\n        print(chunk)\n    ```\n\n=== \"Async Stream\"\n\n    ```python\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread_id\"\n        }\n    }\n\n    async for chunk in my_workflow.astream(None, config):\n        print(chunk)\n    ```\n\n\n\n\n\n### Short-term memory\n\nWhen an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in [checkpoints](persistence.md#checkpoints).\n\nThis allows accessing the state from the previous invocation using the `previous` parameter.\n\nBy default, the `previous` parameter is the return value of the previous invocation.\n\n```python\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(number: int, *, previous: Any = None) -> int:\n    previous = previous or 0\n    return number + previous\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(1, config)  # 1 (previous was None)\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)\n```\n\n\n\n\n\n#### `entrypoint.final`\n\n[`entrypoint.final`](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint.final) is a special primitive that can be returned from an entrypoint and allows **decoupling** the value that is **saved in the checkpoint** from the **return value of the entrypoint**.\n\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is `entrypoint.final[return_type, save_type]`.\n\n```python\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\n    previous = previous or 0\n    # This will return the previous value to the caller, saving\n    # 2 * number to the checkpoint, which will be used in the next invocation\n    # for the `previous` parameter.\n    return entrypoint.final(value=previous, save=2 * number)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\n\nmy_workflow.invoke(3, config)  # 0 (previous was None)\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\n```\n\n\n\n\n\n## Task\n\nA **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\n\n- **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\n- **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](persistence.md) for more details).\n\n### Definition\n\nTasks are defined using the `@task` decorator, which wraps a regular Python function.\n\n```python\nfrom langgraph.func import task\n\n@task()\ndef slow_computation(input_value):\n    # Simulate a long-running operation\n    ...\n    return result\n```\n\n\n\n\n\n!!! important \"Serialization\"\n\n    The **outputs** of tasks must be JSON-serializable to support checkpointing.\n\n### Execution\n\n**Tasks** can only be called from within an **entrypoint**, another **task**, or a [state graph node](./low_level.md#nodes).\n\nTasks _cannot_ be called directly from the main application code.\n\nWhen you call a **task**, it returns _immediately_ with a future object. A future is a placeholder for a result that will be available later.\n\nTo obtain the result of a **task**, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`).\n\n=== \"Synchronous Invocation\"\n\n    ```python\n    @entrypoint(checkpointer=checkpointer)\n    def my_workflow(some_input: int) -> int:\n        future = slow_computation(some_input)\n        return future.result()  # Wait for the result synchronously\n    ```\n\n=== \"Asynchronous Invocation\"\n\n    ```python\n    @entrypoint(checkpointer=checkpointer)\n    async def my_workflow(some_input: int) -> int:\n        return await slow_computation(some_input)  # Await result asynchronously\n    ```\n\n\n\n\n\n## When to use a task\n\n**Tasks** are useful in the following scenarios:\n\n- **Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.\n- **Human-in-the-loop**: If you're building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.\n- **Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\n- **Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.smith.langchain.com/).\n- **Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.\n\n## Serialization\n\nThere are two key aspects to serialization in LangGraph:\n\n1. `entrypoint` inputs and outputs must be JSON-serializable.\n2. `task` outputs must be JSON-serializable.\n\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\n\n\n\n\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\n\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\n\n## Determinism\n\nTo utilize features like **human-in-the-loop**, any randomness should be encapsulated inside of **tasks**. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same _sequence of steps_, even if **task** results are non-deterministic.\n\nLangGraph achieves this behavior by persisting **task** and [**subgraph**](./subgraphs.md) results as they execute. A well-designed workflow ensures that resuming execution follows the _same sequence of steps_, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running **tasks** or **tasks** with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\n\nWhile different runs of a workflow can produce different results, resuming a **specific** run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up **task** and **subgraph** results that were executed prior to the graph being interrupted and avoid recomputing them.\n\n## Idempotency\n\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside **tasks** functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a **task** starts, but does not complete successfully. Then, if the workflow is resumed, the **task** will run again. Use idempotency keys or verify existing results to avoid duplication.\n\n## Common Pitfalls\n\n### Handling side effects\n\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\n\n=== \"Incorrect\"\n\n    In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.\n\n    ```python hl_lines=\"5 6\"\n    @entrypoint(checkpointer=checkpointer)\n    def my_workflow(inputs: dict) -> int:\n        # This code will be executed a second time when resuming the workflow.\n        # Which is likely not what you want.\n        with open(\"output.txt\", \"w\") as f:\n            f.write(\"Side effect executed\")\n        value = interrupt(\"question\")\n        return value\n    ```\n\n\n\n\n=== \"Correct\"\n\n    In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.\n\n    ```python hl_lines=\"3 4\"\n    from langgraph.func import task\n\n    @task\n    def write_to_file():\n        with open(\"output.txt\", \"w\") as f:\n            f.write(\"Side effect executed\")\n\n    @entrypoint(checkpointer=checkpointer)\n    def my_workflow(inputs: dict) -> int:\n        # The side effect is now encapsulated in a task.\n        write_to_file().result()\n        value = interrupt(\"question\")\n        return value\n    ```\n\n\n\n\n### Non-deterministic control flow\n\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\n\n- In a task: Get random number (5) → interrupt → resume → (returns 5 again) → ...\n- Not in a task: Get random number (5) → interrupt → resume → get new random number (7) → ...\n\nThis is especially important when using **human-in-the-loop** workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value. This matching is strictly **index-based**, so the order of the resume values should match the order of the interrupts.\n\n\n\n\nIf order of execution is not maintained when resuming, one `interrupt` call may be matched with the wrong `resume` value, leading to incorrect results.\n\nPlease read the section on [determinism](#determinism) for more details.\n\n=== \"Incorrect\"\n\n    In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.\n\n    ```python hl_lines=\"6\"\n    from langgraph.func import entrypoint\n\n    @entrypoint(checkpointer=checkpointer)\n    def my_workflow(inputs: dict) -> int:\n        t0 = inputs[\"t0\"]\n        t1 = time.time()\n\n        delta_t = t1 - t0\n\n        if delta_t > 1:\n            result = slow_task(1).result()\n            value = interrupt(\"question\")\n        else:\n            result = slow_task(2).result()\n            value = interrupt(\"question\")\n\n        return {\n            \"result\": result,\n            \"value\": value\n        }\n    ```\n\n\n\n\n=== \"Correct\"\n\n    In this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.\n\n    ```python hl_lines=\"5 6 12\"\n    import time\n\n    from langgraph.func import task\n\n    @task\n    def get_time() -> float:\n        return time.time()\n\n    @entrypoint(checkpointer=checkpointer)\n    def my_workflow(inputs: dict) -> int:\n        t0 = inputs[\"t0\"]\n        t1 = get_time().result()\n\n        delta_t = t1 - t0\n\n        if delta_t > 1:\n            result = slow_task(1).result()\n            value = interrupt(\"question\")\n        else:\n            result = slow_task(2).result()\n            value = interrupt(\"question\")\n\n        return {\n            \"result\": result,\n            \"value\": value\n        }\n    ```",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 26027,
    "word_count": 3160
  },
  {
    "title": "Deployment Options",
    "source": "concepts/deployment_options.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Deployment Options\n\n## Free deployment\n\n[Local](../tutorials/langgraph-platform/local-server.md): Deploy for local testing and development.\n\n## Production deployment\n\nThere are 4 main options for deploying with the [LangGraph Platform](langgraph_platform.md):\n\n1. [Cloud SaaS](#cloud-saas)\n\n1. [Self-Hosted Data Plane](#self-hosted-data-plane)\n\n1. [Self-Hosted Control Plane](#self-hosted-control-plane)\n\n1. [Standalone Container](#standalone-container)\n\n\nA quick comparison:\n\n|                      | **Cloud SaaS** | **Self-Hosted Data Plane** | **Self-Hosted Control Plane** | **Standalone Container** |\n|----------------------|----------------|----------------------------|-------------------------------|--------------------------|\n| **[Control plane UI/API](../concepts/langgraph_control_plane.md)** | Yes | Yes | Yes | No |\n| **CI/CD** | Managed internally by platform | Managed externally by you | Managed externally by you | Managed externally by you |\n| **Data/compute residency** | LangChain's cloud | Your cloud | Your cloud | Your cloud |\n| **LangSmith compatibility** | Trace to LangSmith SaaS | Trace to LangSmith SaaS | Trace to Self-Hosted LangSmith | Optional tracing |\n| **[Pricing](https://www.langchain.com/pricing-langgraph-platform)** | Plus | Enterprise | Enterprise | Enterprise |\n\n## Cloud SaaS\n\nThe [Cloud SaaS](./langgraph_cloud.md) deployment option is a fully managed model for deployment where we manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in our cloud. This option provides a simple way to deploy and manage your LangGraph Servers.\n\nConnect your GitHub repositories to the platform and deploy your LangGraph Servers from the [control plane UI](./langgraph_control_plane.md#control-plane-ui). The build process (i.e. CI/CD) is managed internally by the platform.\n\nFor more information, please see:\n\n* [Cloud SaaS Conceptual Guide](./langgraph_cloud.md)\n* [How to deploy to Cloud SaaS](../cloud/deployment/cloud.md)\n\n## Self-Hosted Data Plane\n\n!!! info \"Important\"\n    The Self-Hosted Data Plane deployment option requires an [Enterprise](../concepts/plans.md) plan.\n\nThe [Self-Hosted Data Plane](./langgraph_self_hosted_data_plane.md) deployment option is a \"hybrid\" model for deployment where we manage the [control plane](./langgraph_control_plane.md) in our cloud and you manage the [data plane](./langgraph_data_plane.md) in your cloud. This option provides a way to securely manage your data plane infrastructure, while offloading control plane management to us.\n\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server from the [control plane UI](./langgraph_control_plane.md#control-plane-ui).\n\nSupported Compute Platforms: [Kubernetes](https://kubernetes.io/), [Amazon ECS](https://aws.amazon.com/ecs/) (coming soon!)\n\nFor more information, please see:\n\n* [Self-Hosted Data Plane Conceptual Guide](./langgraph_self_hosted_data_plane.md)\n* [How to deploy the Self-Hosted Data Plane](../cloud/deployment/self_hosted_data_plane.md)\n\n## Self-Hosted Control Plane\n\n!!! info \"Important\"\n    The Self-Hosted Control Plane deployment option requires an [Enterprise](../concepts/plans.md) plan.\n\nThe [Self-Hosted Control Plane](./langgraph_self_hosted_control_plane.md) deployment option is a fully self-hosted model for deployment where you manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in your cloud. This option gives you full control and responsibility of the control plane and data plane infrastructure.\n\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server from the [control plane UI](./langgraph_control_plane.md#control-plane-ui).\n\nSupported Compute Platforms: [Kubernetes](https://kubernetes.io/)\n\nFor more information, please see:\n\n* [Self-Hosted Control Plane Conceptual Guide](./langgraph_self_hosted_control_plane.md)\n* [How to deploy the Self-Hosted Control Plane](../cloud/deployment/self_hosted_control_plane.md)\n\n## Standalone Container\n\nThe [Standalone Container](./langgraph_standalone_container.md) deployment option is the least restrictive model for deployment. Deploy standalone instances of a LangGraph Server in your cloud, using any of the [available](./plans.md) license options.\n\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server using the container deployment tooling of your choice. Images can be deployed to any compute platform.\n\nFor more information, please see:\n\n* [Standalone Container Conceptual Guide](./langgraph_standalone_container.md)\n* [How to deploy a Standalone Container](../cloud/deployment/standalone_container.md)\n\n## Related\n\nFor more information, please see:\n\n* [LangGraph Platform plans](./plans.md)\n* [LangGraph Platform pricing](https://www.langchain.com/langgraph-platform-pricing)",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 4949,
    "word_count": 539
  },
  {
    "title": "LangGraph SDK",
    "source": "concepts/sdk.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph SDK\n\nLangGraph Platform provides a python SDK for interacting with [LangGraph Server](./langgraph_server.md).\n\n!!! tip \"Python SDK reference\"\n\n    For detailed information about the Python SDK, see [Python SDK reference docs](../cloud/reference/sdk/python_sdk_ref.md).\n\n## Installation\n\nYou can install the LangGraph SDK using the following command:\n\n```bash\npip install langgraph-sdk\n```\n\n## Python sync vs. async\n\nThe Python SDK provides both synchronous (`get_sync_client`) and asynchronous (`get_client`) clients for interacting with LangGraph Server:\n\n=== \"Sync\"\n\n    ```python\n    from langgraph_sdk import get_sync_client\n\n    client = get_sync_client(url=..., api_key=...)\n    client.assistants.search()\n    ```\n\n=== \"Async\"\n\n    ```python\n    from langgraph_sdk import get_client\n\n    client = get_client(url=..., api_key=...)\n    await client.assistants.search()\n    ```\n\n## Learn more\n\n- [Python SDK Reference](../cloud/reference/sdk/python_sdk_ref.md)\n- [LangGraph CLI API Reference](../cloud/reference/cli.md)",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 1062,
    "word_count": 113
  },
  {
    "title": "LangGraph Server",
    "source": "concepts/langgraph_server.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph Server\n\n**LangGraph Server** offers an API for creating and managing agent-based applications. It is built on the concept of [assistants](assistants.md), which are agents configured for specific tasks, and includes built-in [persistence](persistence.md#memory-store) and a **task queue**. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions.\n\nUse LangGraph Server to create and manage [assistants](assistants.md), [threads](./persistence.md#threads), [runs](./assistants.md#execution), [cron jobs](../cloud/concepts/cron_jobs.md), [webhooks](../cloud/concepts/webhooks.md), and more.\n\n!!! tip \"API reference\"\n  \n    For detailed information on the API endpoints and data models, see [LangGraph Platform API reference docs](../cloud/reference/api/api_ref.html).\n\n## Application structure\n\nTo deploy a LangGraph Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.\n\nRead the [application structure](./application_structure.md) guide to learn how to structure your LangGraph application for deployment.\n\n## Parts of a deployment\n\nWhen you deploy LangGraph Server, you are deploying one or more [graphs](#graphs), a database for [persistence](persistence.md), and a task queue.\n\n### Graphs\n\nWhen you deploy a graph with LangGraph Server, you are deploying a \"blueprint\" for an [Assistant](assistants.md). \n\nAn [Assistant](assistants.md) is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases\nthat can be served by the same graph.\n\nUpon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph's default configuration settings.\n\n!!! note\n\n    We often think of a graph as implementing an [agent](agentic_concepts.md), but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple\n    chatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use [multiple agents](./multi_agent.md) working in tandem.\n\n### Persistence and task queue\n\nLangGraph Server leverages a database for [persistence](persistence.md) and a task queue.\n\nCurrently, only [Postgres](https://www.postgresql.org/) is supported as a database for LangGraph Server and [Redis](https://redis.io/) as the task queue.\n\nIf you're deploying using [LangGraph Platform](./langgraph_cloud.md), these components are managed for you. If you're deploying LangGraph Server on your own infrastructure, you'll need to set up and manage these components yourself.\n\nPlease review the [deployment options](./deployment_options.md) guide for more information on how these components are set up and managed.\n\n## Learn more\n\n* LangGraph [Application Structure](./application_structure.md) guide explains how to structure your LangGraph application for deployment.\n* The [LangGraph Platform API Reference](../cloud/reference/api/api_ref.html) provides detailed information on the API endpoints and data models.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3347,
    "word_count": 423
  },
  {
    "title": "LangGraph Platform",
    "source": "concepts/langgraph_platform.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph Platform\n\nDevelop, deploy, scale, and manage agents with **LangGraph Platform** — the purpose-built platform for long-running, agentic workflows.\n\n!!! tip \"Get started with LangGraph Platform\"\n\n    Check out the [LangGraph Platform quickstart](../tutorials/langgraph-platform/local-server.md) for instructions on how to use LangGraph Platform to run a LangGraph application locally.\n\n## Why use LangGraph Platform?\n\n<div align=\"center\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pfAQxBS5z88?si=XGS6Chydn6lhSO1S\" title=\"What is LangGraph Platform?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\nLangGraph Platform makes it easy to get your agent running in production —  whether it’s built with LangGraph or another framework — so you can focus on your app logic, not infrastructure. Deploy with one click to get a live endpoint, and use our robust APIs and built-in task queues to handle production scale. \n\n- **[Streaming Support](../cloud/how-tos/streaming.md)**: As agents grow more sophisticated, they often benefit from streaming both token outputs and intermediate states back to the user. Without this, users are left waiting for potentially long operations with no feedback. LangGraph Server provides multiple streaming modes optimized for various application needs.\n\n- **[Background Runs](../cloud/how-tos/background_run.md)**: For agents that take longer to process (e.g., hours), maintaining an open connection can be impractical. The LangGraph Server supports launching agent runs in the background and provides both polling endpoints and webhooks to monitor run status effectively.\n \n- **Support for long runs**: Regular server setups often encounter timeouts or disruptions when handling requests that take a long time to complete. LangGraph Server’s API provides robust support for these tasks by sending regular heartbeat signals, preventing unexpected connection closures during prolonged processes.\n\n- **Handling Burstiness**: Certain applications, especially those with real-time user interaction, may experience \"bursty\" request loads where numerous requests hit the server simultaneously. LangGraph Server includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads.\n\n- **[Double-texting](../cloud/how-tos/interrupt_concurrent.md)**: In user-driven applications, it’s common for users to send multiple messages rapidly. This “double texting” can disrupt agent flows if not handled properly. LangGraph Server offers built-in strategies to address and manage such interactions.\n\n- **[Checkpointers and memory management](persistence.md#checkpoints)**: For agents needing persistence (e.g., conversation memory), deploying a robust storage solution can be complex. LangGraph Platform includes optimized [checkpointers](persistence.md#checkpoints) and a [memory store](persistence.md#memory-store), managing state across sessions without the need for custom solutions.\n\n- **[Human-in-the-loop support](../cloud/how-tos/human_in_the_loop_breakpoint.md)**: In many applications, users require a way to intervene in agent processes. LangGraph Server provides specialized endpoints for human-in-the-loop scenarios, simplifying the integration of manual oversight into agent workflows.\n\n- **[LangGraph Studio](./langgraph_studio.md)**: Enables visualization, interaction, and debugging of agentic systems that implement the LangGraph Server API protocol. Studio also integrates with LangSmith to enable tracing, evaluation, and prompt engineering.\n\n- **[Deployment](./deployment_options.md)**: There are four ways to deploy on LangGraph Platform: [Cloud SaaS](../concepts/langgraph_cloud.md), [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md), [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md), and [Standalone Container](../concepts/langgraph_standalone_container.md).",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 4103,
    "word_count": 461
  },
  {
    "title": "Tracing",
    "source": "concepts/tracing.md",
    "content": "# Tracing\n\nTraces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](../how-tos/enable-tracing.md). This enables you to do the following:\n\n- [Debug a locally running application](../cloud/how-tos/clone_traces_studio.md).\n- [Evaluate the application performance](../agents/evals.md).\n- [Monitor the application](https://docs.smith.langchain.com/observability/how_to_guides/dashboards).\n\nTo get started, sign up for a free account at [LangSmith](https://smith.langchain.com/).\n\n## Learn more\n\n- [Graph runs in LangSmith](../how-tos/run-id-langsmith.md)\n- [LangSmith Observability quickstart](https://docs.smith.langchain.com/observability)\n- [Trace with LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)\n- [Tracing conceptual guide](https://docs.smith.langchain.com/observability/concepts#traces)",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 1060,
    "word_count": 98
  },
  {
    "title": "Memory",
    "source": "concepts/memory.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Memory\n\n[Memory](../how-tos/memory/add-memory.md) is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nThis conceptual guide covers two types of memory, based on their recall scope:\n\n- [Short-term memory](#short-term-memory), or [thread](persistence.md#threads)-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's [state](low_level.md#state). State is persisted to a database using a [checkpointer](persistence.md#checkpoints) so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.\n\n- [Long-term memory](#long-term-memory) stores user-specific or application-level data across sessions and is shared _across_ conversational threads. It can be recalled _at any time_ and _in any thread_. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides [stores](persistence.md#memory-store) ([reference doc](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)) to let you save and recall long-term memories.\n\n![](img/memory/short-vs-long.png)\n\n\n## Short-term memory\n\n[Short-term memory](../how-tos/memory/add-memory.md#add-short-term-memory) lets your application remember previous interactions within a single [thread](persistence.md#threads) or conversation. A [thread](persistence.md#threads) organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n\nLangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.\n\n### Manage short-term memory\n\nConversation history is the most common form of short-term memory, and long conversations pose a challenge to today's LLMs. A full history may not fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs.\n\nChat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.\n\n![](img/memory/filter.png)\n\nFor more information on common techniques for managing messages, see the [Add and manage memory](../how-tos/memory/add-memory.md#manage-short-term-memory) guide.\n\n## Long-term memory\n\n[Long-term memory](../how-tos/memory/add-memory.md#add-long-term-memory) in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is **thread-scoped**, long-term memory is saved within custom \"namespaces.\"\n\nLong-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:\n\n- [What is the type of memory?](#memory-types) Humans use memories to remember facts ([semantic memory](#semantic-memory)), experiences ([episodic memory](#episodic-memory)), and rules ([procedural memory](#procedural-memory)). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.\n\n- [When do you want to update memories?](#writing-memories) Memory can be updated as part of an agent's application logic (e.g., \"on the hot path\"). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the [section below](#writing-memories).\n\n### Memory types\n\nDifferent applications require various types of memory. Although the analogy isn't perfect, examining [human memory types](https://www.psychologytoday.com/us/basics/memory/types-of-memory?ref=blog.langchain.dev) can be insightful. Some research (e.g., the [CoALA paper](https://arxiv.org/pdf/2309.02427)) have even mapped these human memory types to those used in AI agents.\n\n| Memory Type | What is Stored | Human Example | Agent Example |\n|-------------|----------------|---------------|---------------|\n| [Semantic](#semantic-memory) | Facts | Things I learned in school | Facts about a user |\n| [Episodic](#episodic-memory) | Experiences | Things I did | Past agent actions |\n| [Procedural](#procedural-memory) | Instructions | Instincts or motor skills | Agent system prompt |\n\n#### Semantic memory\n\n[Semantic memory](https://en.wikipedia.org/wiki/Semantic_memory), both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions. \n\n!!! note\n\n    Semantic memory is different from \"semantic search,\" which is a technique for finding similar content using \"meaning\" (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.\n\n\n##### Profile\n\nSemantic memories can be managed in different ways. For example, memories can be a single, continuously updated \"profile\" of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you've selected to represent your domain. \n\nWhen remembering a profile, you will want to make sure that you are **updating** the profile each time. As a result, you will want to pass in the previous profile and [ask the model to generate a new profile](https://github.com/langchain-ai/memory-template) (or some [JSON patch](https://github.com/hinthornw/trustcall) to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or **strict** decoding when generating documents to ensure the memory schemas remains valid.\n\n![](img/memory/update-profile.png)\n\n##### Collection\n\nAlternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you're less likely to **lose** information over time. It's easier for an LLM to generate _new_ objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to [higher recall downstream](https://en.wikipedia.org/wiki/Precision_and_recall).\n\nHowever, this shifts some complexity memory updating. The model must now _delete_ or _update_ existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the [Trustcall](https://github.com/hinthornw/trustcall) package for one way to manage this and consider evaluation (e.g., with a tool like [LangSmith](https://docs.smith.langchain.com/tutorials/Developers/evaluation)) to help you tune the behavior.\n\nWorking with document collections also shifts complexity to memory **search** over the list. The `Store` currently supports both [semantic search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.query) and [filtering by content](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.filter).\n\nFinally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.\n\n![](img/memory/update-list.png)\n\nRegardless of memory management approach, the central point is that the agent will use the semantic memories to [ground its responses](https://python.langchain.com/docs/concepts/rag/), which often leads to more personalized and relevant interactions.\n\n#### Episodic memory\n\n[Episodic memory](https://en.wikipedia.org/wiki/Episodic_memory), in both humans and AI agents, involves recalling past events or actions. The [CoALA paper](https://arxiv.org/pdf/2309.02427) frames this well: facts can be written to semantic memory, whereas *experiences* can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task. \n\nIn practice, episodic memories are often implemented through [few-shot example prompting](https://python.langchain.com/docs/concepts/few_shot_prompting/), where agents learn from past sequences to perform tasks correctly. Sometimes it's easier to \"show\" than \"tell\" and LLMs learn well from examples. Few-shot learning lets you [\"program\"](https://x.com/karpathy/status/1627366413840322562) your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various [best-practices](https://python.langchain.com/docs/concepts/#1-generating-examples) can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.\n\n\n\n\nNote that the memory [store](persistence.md#memory-store) is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a [LangSmith Dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection) to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity ([using a BM25-like algorithm](https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection) for keyword based similarity). \n\nSee this how-to [video](https://www.youtube.com/watch?v=37VaU7e7t5o) for example usage of dynamic few-shot example selection in LangSmith. Also, see this [blog post](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/) showcasing few-shot prompting to improve tool calling performance and this [blog post](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/) using few-shot example to align an LLMs to human preferences.\n\n\n\n\n#### Procedural memory\n\n[Procedural memory](https://en.wikipedia.org/wiki/Procedural_memory), in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent's prompt that collectively determine the agent's functionality. \n\nIn practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts. \n\nOne effective approach to refining an agent's instructions is through [\"Reflection\"](https://blog.langchain.dev/reflection-agents/) or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.\n\nFor example, we built a [Tweet generator](https://www.youtube.com/watch?v=Vn8A3BxfplE) using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify *a priori*, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process. \n\nThe below pseudo-code shows how you might implement this with the LangGraph memory [store](persistence.md#memory-store), using the store to save a prompt, the `update_instructions` node to get the current prompt (as well as feedback from the conversation with the user captured in `state[\"messages\"]`), update the prompt, and save the new prompt back to the store. Then, the `call_model` get the updated prompt from the store and uses it to generate a response.\n\n```python\n# Node that *uses* the instructions\ndef call_model(state: State, store: BaseStore):\n    namespace = (\"agent_instructions\", )\n    instructions = store.get(namespace, key=\"agent_a\")[0]\n    # Application logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"])\n    ...\n\n# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=current_instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n```\n\n\n\n\n![](img/memory/update-instructions.png)\n\n### Writing memories\n\nThere are two primary methods for agents to write memories: [\"in the hot path\"](#in-the-hot-path) and [\"in the background\"](#in-the-background).\n\n![](img/memory/hot_path_vs_background.png)\n\n#### In the hot path\n\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\n\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\n\nAs an example, ChatGPT uses a [save_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent) template as an reference implementation.\n\n#### In the background\n\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\n\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\n\nSee our [memory-service](https://github.com/langchain-ai/memory-template) template as an reference implementation.\n\n### Memory storage\n\nLangGraph stores long-term memories as JSON documents in a [store](persistence.md#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\n\n```python\nfrom langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context)\nstore.put(\n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\")\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search(\n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n```\n\n\n\n\nFor more information about the memory store, see the [Persistence](persistence.md#memory-store) guide.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 18708,
    "word_count": 2352
  },
  {
    "title": "Agent architectures",
    "source": "concepts/agentic_concepts.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Agent architectures\n\nMany LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, [RAG](https://github.com/langchain-ai/rag-from-scratch) performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context. \n\nInstead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an [agent](https://blog.langchain.dev/what-is-an-agent/): *an agent is a system that uses an LLM to decide the control flow of an application.* There are many ways that an LLM can control application:\n\n- An LLM can route between two potential paths\n- An LLM can decide which of many tools to call\n- An LLM can decide whether the generated answer is sufficient or more work is needed\n\nAs a result, there are many different types of [agent architectures](https://blog.langchain.dev/what-is-a-cognitive-architecture/), which give an LLM varying levels of control. \n\n![Agent Types](img/agent_types.png)\n\n## Router\n\nA router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from a limited set of pre-defined options. Routers typically employ a few different concepts to achieve this.\n\n### Structured Output\n\nStructured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:\n\n1. Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.\n2. Output parsers: Using post-processing to extract structured data from LLM responses.\n3. Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.\n\nStructured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about [structured outputs in this how-to guide](https://python.langchain.com/docs/how_to/structured_output/).\n\n## Tool-calling agent\n\nWhile a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:\n\n1. Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.\n2. Tool access: The LLM can choose from and use a variety of tools to accomplish tasks.\n\n[ReAct](https://arxiv.org/abs/2210.03629) is a popular general purpose agent architecture that combines these expansions, integrating three core concepts. \n\n1. [Tool calling](#tool-calling): Allowing the LLM to select and use various tools as needed.\n2. [Memory](#memory): Enabling the agent to retain and use information from previous steps.\n3. [Planning](#planning): Empowering the LLM to create and follow multi-step plans to achieve goals.\n\nThis architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. Unlike the original [paper](https://arxiv.org/abs/2210.03629), today's agents rely on LLMs' [tool calling](#tool-calling) capabilities and operate on a list of [messages](./low_level.md#why-use-messages).\n\nIn LangGraph, you can use the prebuilt [agent](../agents/agents.md#2-create-an-agent) to get started with tool-calling agents.\n\n### Tool calling\n\nTools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema. \n\n[Many LLM providers support tool calling](https://python.langchain.com/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple: you can simply pass any Python `function` into `ChatModel.bind_tools(function)`.\n\n![Tools](img/tool_call.png)\n\n### Memory\n\n[Memory](../how-tos/memory/add-memory.md) is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales:\n\n1. [Short-term memory](../how-tos/memory/add-memory.md#add-short-term-memory): Allows the agent to access information acquired during earlier steps in a sequence.\n2. [Long-term memory](../how-tos/memory/add-memory.md#add-long-term-memory): Enables the agent to recall information from previous interactions, such as past messages in a conversation.\n\nLangGraph provides full control over memory implementation:\n\n- [`State`](./low_level.md#state): User-defined schema specifying the exact structure of memory to retain.\n- [`Checkpointer`](./persistence.md#checkpoints): Mechanism to store state at every step across different interactions within a session.\n- [`Store`](./persistence.md#memory-store): Mechanism to store user-specific or application-level data across sessions.\n\nThis flexible approach allows you to tailor the memory system to your specific agent architecture needs. Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time. For a practical guide on adding and managing memory, see [Memory](../how-tos/memory/add-memory.md).\n\n### Planning\n\nIn a tool-calling [agent](../agents/overview.md#what-is-an-agent), an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools.\n\n## Custom agent architectures\n\nWhile routers and tool-calling agents (like ReAct) are common, [customizing agent architectures](https://blog.langchain.dev/why-you-should-outsource-your-agentic-infrastructure-but-own-your-cognitive-architecture/) often leads to better performance for specific tasks. LangGraph offers several powerful features for building tailored agent systems:\n\n### Human-in-the-loop\n\nHuman involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve:\n\n- Approving specific actions\n- Providing feedback to update the agent's state\n- Offering guidance in complex decision-making processes\n\nHuman-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our [human-in-the-loop guide](./human_in_the_loop.md).\n\n### Parallelization \n\nParallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its [Send](./low_level.md#send) API, enabling:\n\n- Concurrent processing of multiple states\n- Implementation of map-reduce-like operations\n- Efficient handling of independent subtasks\n\nFor practical implementation, see our [map-reduce tutorial](../how-tos/graph-api.md#map-reduce-and-the-send-api)\n\n### Subgraphs\n\n[Subgraphs](./subgraphs.md) are essential for managing complex agent architectures, particularly in [multi-agent systems](./multi_agent.md). They allow:\n\n- Isolated state management for individual agents\n- Hierarchical organization of agent teams\n- Controlled communication between agents and the main system\n\nSubgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our [subgraph how-to guide](../how-tos/subgraph.md).\n\n### Reflection\n\nReflection mechanisms can significantly improve agent reliability by:\n\n1. Evaluating task completion and correctness\n2. Providing feedback for iterative improvement\n3. Enabling self-correction and learning\n\nWhile often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in [this video using LangGraph for self-corrective code generation](https://www.youtube.com/watch?v=MvNdgmM7uyc).\n\nBy leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 8924,
    "word_count": 1156
  },
  {
    "title": "Authentication & Access Control",
    "source": "concepts/auth.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Authentication & Access Control\n\nLangGraph Platform provides a flexible authentication and authorization system that can integrate with most authentication schemes.\n\n## Core Concepts\n\n### Authentication vs Authorization\n\nWhile often used interchangeably, these terms represent distinct security concepts:\n\n- [**Authentication**](#authentication) (\"AuthN\") verifies _who_ you are. This runs as middleware for every request.\n- [**Authorization**](#authorization) (\"AuthZ\") determines _what you can do_. This validates the user's privileges and roles on a per-resource basis.\n\nIn LangGraph Platform, authentication is handled by your [`@auth.authenticate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.authenticate) handler, and authorization is handled by your [`@auth.on`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.on) handlers.\n\n\n\n\n## Default Security Models\n\nLangGraph Platform provides different security defaults:\n\n### LangGraph Platform\n\n- Uses LangSmith API keys by default\n- Requires valid API key in `x-api-key` header\n- Can be customized with your auth handler\n\n!!! note \"Custom auth\"\n\n   Custom auth **is supported** for all plans in LangGraph Platform.\n\n### Self-Hosted\n\n- No default authentication\n- Complete flexibility to implement your security model\n- You control all aspects of authentication and authorization\n\n## System Architecture\n\nA typical authentication setup involves three main components:\n\n1. **Authentication Provider** (Identity Provider/IdP)\n\n   - A dedicated service that manages user identities and credentials\n   - Handles user registration, login, password resets, etc.\n   - Issues tokens (JWT, session tokens, etc.) after successful authentication\n   - Examples: Auth0, Supabase Auth, Okta, or your own auth server\n\n2. **LangGraph Backend** (Resource Server)\n\n   - Your LangGraph application that contains business logic and protected resources\n   - Validates tokens with the auth provider\n   - Enforces access control based on user identity and permissions\n   - Doesn't store user credentials directly\n\n3. **Client Application** (Frontend)\n\n   - Web app, mobile app, or API client\n   - Collects time-sensitive user credentials and sends to auth provider\n   - Receives tokens from auth provider\n   - Includes these tokens in requests to LangGraph backend\n\nHere's how these components typically interact:\n\n```mermaid\nsequenceDiagram\n    participant Client as Client App\n    participant Auth as Auth Provider\n    participant LG as LangGraph Backend\n\n    Client->>Auth: 1. Login (username/password)\n    Auth-->>Client: 2. Return token\n    Client->>LG: 3. Request with token\n    Note over LG: 4. Validate token (@auth.authenticate)\n    LG-->>Auth:  5. Fetch user info\n    Auth-->>LG: 6. Confirm validity\n    Note over LG: 7. Apply access control (@auth.on.*)\n    LG-->>Client: 8. Return resources\n```\n\nYour [`@auth.authenticate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.authenticate) handler in LangGraph handles steps 4-6, while your [`@auth.on`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.on) handlers implement step 7.\n\n\n\n\n## Authentication\n\nAuthentication in LangGraph runs as middleware on every request. Your [`@auth.authenticate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.authenticate) handler receives request information and should:\n\n1. Validate the credentials\n2. Return [user info](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.MinimalUserDict) containing the user's identity and user information if valid\n3. Raise an [HTTPException](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.exceptions.HTTPException) or AssertionError if invalid\n\n```python\nfrom langgraph_sdk import Auth\n\nauth = Auth()\n\n@auth.authenticate\nasync def authenticate(headers: dict) -> Auth.types.MinimalUserDict:\n    # Validate credentials (e.g., API key, JWT token)\n    api_key = headers.get(\"x-api-key\")\n    if not api_key or not is_valid_key(api_key):\n        raise Auth.exceptions.HTTPException(\n            status_code=401,\n            detail=\"Invalid API key\"\n        )\n\n    # Return user info - only identity and is_authenticated are required\n    # Add any additional fields you need for authorization\n    return {\n        \"identity\": \"user-123\",        # Required: unique user identifier\n        \"is_authenticated\": True,      # Optional: assumed True by default\n        \"permissions\": [\"read\", \"write\"] # Optional: for permission-based auth\n        # You can add more custom fields if you want to implement other auth patterns\n        \"role\": \"admin\",\n        \"org_id\": \"org-456\"\n\n    }\n```\n\nThe returned user information is available:\n\n- To your authorization handlers via [`ctx.user`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.AuthContext)\n- In your application via `config[\"configuration\"][\"langgraph_auth_user\"]`\n\n\n\n\n??? tip \"Supported Parameters\"\n\n    The [`@auth.authenticate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.authenticate) handler can accept any of the following parameters by name:\n\n    * request (Request): The raw ASGI request object\n    * body (dict): The parsed request body\n    * path (str): The request path, e.g., \"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream\"\n    * method (str): The HTTP method, e.g., \"GET\"\n    * path_params (dict[str, str]): URL path parameters, e.g., {\"thread_id\": \"abcd-1234-abcd-1234\", \"run_id\": \"abcd-1234-abcd-1234\"}\n    * query_params (dict[str, str]): URL query parameters, e.g., {\"stream\": \"true\"}\n    * headers (dict[bytes, bytes]): Request headers\n    * authorization (str | None): The Authorization header value (e.g., \"Bearer <token>\")\n\n\n\n\n    In many of our tutorials, we will just show the \"authorization\" parameter to be concise, but you can opt to accept more information as needed\n    to implement your custom authentication scheme.\n\n### Agent authentication\n\nCustom authentication permits delegated access. The values you return in `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.\n\n```mermaid\nsequenceDiagram\n  %% Actors\n  participant ClientApp as Client\n  participant AuthProv  as Auth Provider\n  participant LangGraph as LangGraph Backend\n  participant SecretStore as Secret Store\n  participant ExternalService as External Service\n\n  %% Platform login / AuthN\n  ClientApp  ->> AuthProv: 1. Login (username / password)\n  AuthProv   -->> ClientApp: 2. Return token\n  ClientApp  ->> LangGraph: 3. Request with token\n\n  Note over LangGraph: 4. Validate token (@auth.authenticate)\n  LangGraph  -->> AuthProv: 5. Fetch user info\n  AuthProv   -->> LangGraph: 6. Confirm validity\n\n  %% Fetch user tokens from secret store\n  LangGraph  ->> SecretStore: 6a. Fetch user tokens\n  SecretStore -->> LangGraph: 6b. Return tokens\n\n  Note over LangGraph: 7. Apply access control (@auth.on.*)\n\n  %% External Service round-trip\n  LangGraph  ->> ExternalService: 8. Call external service (with header)\n  Note over ExternalService: 9. External service validates header and executes action\n  ExternalService  -->> LangGraph: 10. Service response\n\n  %% Return to caller\n  LangGraph  -->> ClientApp: 11. Return resources\n```\n\nAfter authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.\nThis object contains information about the current user, including any custom fields you return from your [`@auth.authenticate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.authenticate) handler.\n\nTo enable an agent to act on behalf of the user, use [custom authentication middleware](../how-tos/auth/custom_auth.md). This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.\n\nFor more information, see the [Use custom auth](../how-tos/auth/custom_auth.md#enable-agent-authentication) guide.\n\n### Agent authentication with MCP\n\nFor information on how to authenticate an agent to an MCP server, see the [MCP conceptual guide](../concepts/mcp.md).\n\n## Authorization\n\nAfter authentication, LangGraph calls your authorization handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:\n\n1. Add metadata to be saved during resource creation by mutating the metadata. See the [supported actions table](#supported-actions) for the list of types the value can take for each action.\n2. Filter resources by metadata during search/list or read operations by returning a [filter](#filter-operations).\n3. Raise an HTTP exception if access is denied.\n\nIf you want to just implement simple user-scoped access control, you can use a single authorization handler for all resources and actions. If you want to have different control depending on the resource and action, you can use [resource-specific handlers](#resource-specific-handlers). See the [Supported Resources](#supported-resources) section for a full list of the resources that support access control.\n\nYour [`@auth.on`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.Auth.on) handlers control access by mutating the `value[\"metadata\"]` dictionary directly and returning a [filter dictionary](#filter-operations).\n\n```python\n@auth.on\nasync def add_owner(\n    ctx: Auth.types.AuthContext,\n    value: dict  # The payload being sent to this access method\n) -> dict:  # Returns a filter dict that restricts access to resources\n    \"\"\"Authorize all access to threads, runs, crons, and assistants.\n\n    This handler does two things:\n        - Adds a value to resource metadata (to persist with the resource so it can be filtered later)\n        - Returns a filter (to restrict access to existing resources)\n\n    Args:\n        ctx: Authentication context containing user info, permissions, the path, and\n        value: The request payload sent to the endpoint. For creation\n              operations, this contains the resource parameters. For read\n              operations, this contains the resource being accessed.\n\n    Returns:\n        A filter dictionary that LangGraph uses to restrict access to resources.\n        See [Filter Operations](#filter-operations) for supported operators.\n    \"\"\"\n    # Create filter to restrict access to just this user's resources\n    filters = {\"owner\": ctx.user.identity}\n\n    # Get or create the metadata dictionary in the payload\n    # This is where we store persistent info about the resource\n    metadata = value.setdefault(\"metadata\", {})\n\n    # Add owner to metadata - if this is a create or update operation,\n    # this information will be saved with the resource\n    # So we can filter by it later in read operations\n    metadata.update(filters)\n\n    # Return filters to restrict access\n    # These filters are applied to ALL operations (create, read, update, search, etc.)\n    # to ensure users can only access their own resources\n    return filters\n```\n\n\n\n\n\n### Resource-Specific Handlers {#resource-specific-handlers}\n\nYou can register handlers for specific resources and actions by chaining the resource and action names together with the authorization decorator.\nWhen a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:\n\n1. Authenticated users are able to create threads, read threads, and create runs on threads\n2. Only users with the \"assistants:create\" permission are allowed to create new assistants\n3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.\n\n!!! tip \"Supported Handlers\"\n\n    For a full list of supported resources and actions, see the [Supported Resources](#supported-resources) section below.\n\n```python\n# Generic / global handler catches calls that aren't handled by more specific handlers\n@auth.on\nasync def reject_unhandled_requests(ctx: Auth.types.AuthContext, value: Any) -> False:\n    print(f\"Request to {ctx.path} by {ctx.user.identity}\")\n    raise Auth.exceptions.HTTPException(\n        status_code=403,\n        detail=\"Forbidden\"\n    )\n\n# Matches the \"thread\" resource and all actions - create, read, update, delete, search\n# Since this is **more specific** than the generic @auth.on handler, it will take precedence\n# over the generic handler for all actions on the \"threads\" resource\n@auth.on.threads\nasync def on_thread_create(\n    ctx: Auth.types.AuthContext,\n    value: Auth.types.threads.create.value\n):\n    if \"write\" not in ctx.permissions:\n        raise Auth.exceptions.HTTPException(\n            status_code=403,\n            detail=\"User lacks the required permissions.\"\n        )\n    # Setting metadata on the thread being created\n    # will ensure that the resource contains an \"owner\" field\n    # Then any time a user tries to access this thread or runs within the thread,\n    # we can filter by owner\n    metadata = value.setdefault(\"metadata\", {})\n    metadata[\"owner\"] = ctx.user.identity\n    return {\"owner\": ctx.user.identity}\n\n# Thread creation. This will match only on thread create actions\n# Since this is **more specific** than both the generic @auth.on handler and the @auth.on.threads handler,\n# it will take precedence for any \"create\" actions on the \"threads\" resources\n@auth.on.threads.create\nasync def on_thread_create(\n    ctx: Auth.types.AuthContext,\n    value: Auth.types.threads.create.value\n):\n    # Setting metadata on the thread being created\n    # will ensure that the resource contains an \"owner\" field\n    # Then any time a user tries to access this thread or runs within the thread,\n    # we can filter by owner\n    metadata = value.setdefault(\"metadata\", {})\n    metadata[\"owner\"] = ctx.user.identity\n    return {\"owner\": ctx.user.identity}\n\n# Reading a thread. Since this is also more specific than the generic @auth.on handler, and the @auth.on.threads handler,\n# it will take precedence for any \"read\" actions on the \"threads\" resource\n@auth.on.threads.read\nasync def on_thread_read(\n    ctx: Auth.types.AuthContext,\n    value: Auth.types.threads.read.value\n):\n    # Since we are reading (and not creating) a thread,\n    # we don't need to set metadata. We just need to\n    # return a filter to ensure users can only see their own threads\n    return {\"owner\": ctx.user.identity}\n\n# Run creation, streaming, updates, etc.\n# This takes precedenceover the generic @auth.on handler and the @auth.on.threads handler\n@auth.on.threads.create_run\nasync def on_run_create(\n    ctx: Auth.types.AuthContext,\n    value: Auth.types.threads.create_run.value\n):\n    metadata = value.setdefault(\"metadata\", {})\n    metadata[\"owner\"] = ctx.user.identity\n    # Inherit thread's access control\n    return {\"owner\": ctx.user.identity}\n\n# Assistant creation\n@auth.on.assistants.create\nasync def on_assistant_create(\n    ctx: Auth.types.AuthContext,\n    value: Auth.types.assistants.create.value\n):\n    if \"assistants:create\" not in ctx.permissions:\n        raise Auth.exceptions.HTTPException(\n            status_code=403,\n            detail=\"User lacks the required permissions.\"\n        )\n```\n\n\n\n\n\nNotice that we are mixing global and resource-specific handlers in the above example. Since each request is handled by the most specific handler, a request to create a `thread` would match the `on_thread_create` handler but NOT the `reject_unhandled_requests` handler. A request to `update` a thread, however would be handled by the global handler, since we don't have a more specific handler for that resource and action.\n\n### Filter Operations {#filter-operations}\n\nAuthorization handlers can return different types of values:\n\n- `None` and `True` mean \"authorize access to all underling resources\"\n- `False` means \"deny access to all underling resources (raises a 403 exception)\"\n- A metadata filter dictionary will restrict access to resources\n\nA filter dictionary is a dictionary with keys that match the resource metadata. It supports three operators:\n\n- The default value is a shorthand for exact match, or \"$eq\", below. For example, `{\"owner\": user_id}` will include only resources with metadata containing `{\"owner\": user_id}`\n- `$eq`: Exact match (e.g., `{\"owner\": {\"$eq\": user_id}}`) - this is equivalent to the shorthand above, `{\"owner\": user_id}`\n- `$contains`: List membership (e.g., `{\"allowed_users\": {\"$contains\": user_id}}`) The value here must be an element of the list. The metadata in the stored resource must be a list/container type.\n\nA dictionary with multiple keys is treated using a logical `AND` filter. For example, `{\"owner\": org_id, \"allowed_users\": {\"$contains\": user_id}}` will only match resources with metadata whose \"owner\" is `org_id` and whose \"allowed_users\" list contains `user_id`.\nSee the reference [here](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.FilterType) for more information.\n\n\n\n\n## Common Access Patterns\n\nHere are some typical authorization patterns:\n\n### Single-Owner Resources\n\nThis common pattern lets you scope all threads, assistants, crons, and runs to a single user. It's useful for common single-user use cases like regular chatbot-style apps.\n\n```python\n@auth.on\nasync def owner_only(ctx: Auth.types.AuthContext, value: dict):\n    metadata = value.setdefault(\"metadata\", {})\n    metadata[\"owner\"] = ctx.user.identity\n    return {\"owner\": ctx.user.identity}\n```\n\n\n\n\n\n### Permission-based Access\n\nThis pattern lets you control access based on **permissions**. It's useful if you want certain roles to have broader or more restricted access to resources.\n\n```python\n# In your auth handler:\n@auth.authenticate\nasync def authenticate(headers: dict) -> Auth.types.MinimalUserDict:\n    ...\n    return {\n        \"identity\": \"user-123\",\n        \"is_authenticated\": True,\n        \"permissions\": [\"threads:write\", \"threads:read\"]  # Define permissions in auth\n    }\n\ndef _default(ctx: Auth.types.AuthContext, value: dict):\n    metadata = value.setdefault(\"metadata\", {})\n    metadata[\"owner\"] = ctx.user.identity\n    return {\"owner\": ctx.user.identity}\n\n@auth.on.threads.create\nasync def create_thread(ctx: Auth.types.AuthContext, value: dict):\n    if \"threads:write\" not in ctx.permissions:\n        raise Auth.exceptions.HTTPException(\n            status_code=403,\n            detail=\"Unauthorized\"\n        )\n    return _default(ctx, value)\n\n\n@auth.on.threads.read\nasync def rbac_create(ctx: Auth.types.AuthContext, value: dict):\n    if \"threads:read\" not in ctx.permissions and \"threads:write\" not in ctx.permissions:\n        raise Auth.exceptions.HTTPException(\n            status_code=403,\n            detail=\"Unauthorized\"\n        )\n    return _default(ctx, value)\n```\n\n\n\n\n\n## Supported Resources\n\nLangGraph provides three levels of authorization handlers, from most general to most specific:\n\n1. **Global Handler** (`@auth.on`): Matches all resources and actions\n2. **Resource Handler** (e.g., `@auth.on.threads`, `@auth.on.assistants`, `@auth.on.crons`): Matches all actions for a specific resource\n3. **Action Handler** (e.g., `@auth.on.threads.create`, `@auth.on.threads.read`): Matches a specific action on a specific resource\n\nThe most specific matching handler will be used. For example, `@auth.on.threads.create` takes precedence over `@auth.on.threads` for thread creation.\nIf a more specific handler is registered, the more general handler will not be called for that resource and action.\n\n\n\n\n???+ tip \"Type Safety\"\nEach handler has type hints available for its `value` parameter. For example:\n\n    ```python\n    @auth.on.threads.create\n    async def on_thread_create(\n        ctx: Auth.types.AuthContext,\n        value: Auth.types.on.threads.create.value  # Specific type for thread creation\n    ):\n        ...\n\n    @auth.on.threads\n    async def on_threads(\n        ctx: Auth.types.AuthContext,\n        value: Auth.types.on.threads.value  # Union type of all thread actions\n    ):\n        ...\n\n    @auth.on\n    async def on_all(\n        ctx: Auth.types.AuthContext,\n        value: dict  # Union type of all possible actions\n    ):\n        ...\n    ```\n\n    More specific handlers provide better type hints since they handle fewer action types.\n\n\n\n#### Supported actions and types {#supported-actions}\n\nHere are all the supported action handlers:\n\n| Resource | Handler | Description | Value Type |\n|----------|---------|-------------|------------|\n| **Threads** | `@auth.on.threads.create` | Thread creation | [`ThreadsCreate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.ThreadsCreate) |\n| | `@auth.on.threads.read` | Thread retrieval | [`ThreadsRead`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.ThreadsRead) |\n| | `@auth.on.threads.update` | Thread updates | [`ThreadsUpdate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.ThreadsUpdate) |\n| | `@auth.on.threads.delete` | Thread deletion | [`ThreadsDelete`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.ThreadsDelete) |\n| | `@auth.on.threads.search` | Listing threads | [`ThreadsSearch`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.ThreadsSearch) |\n| | `@auth.on.threads.create_run` | Creating or updating a run | [`RunsCreate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.RunsCreate) |\n| **Assistants** | `@auth.on.assistants.create` | Assistant creation | [`AssistantsCreate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.AssistantsCreate) |\n| | `@auth.on.assistants.read` | Assistant retrieval | [`AssistantsRead`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.AssistantsRead) |\n| | `@auth.on.assistants.update` | Assistant updates | [`AssistantsUpdate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.AssistantsUpdate) |\n| | `@auth.on.assistants.delete` | Assistant deletion | [`AssistantsDelete`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.AssistantsDelete) |\n| | `@auth.on.assistants.search` | Listing assistants | [`AssistantsSearch`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.AssistantsSearch) |\n| **Crons** | `@auth.on.crons.create` | Cron job creation | [`CronsCreate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.CronsCreate) |\n| | `@auth.on.crons.read` | Cron job retrieval | [`CronsRead`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.CronsRead) |\n| | `@auth.on.crons.update` | Cron job updates | [`CronsUpdate`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.CronsUpdate) |\n| | `@auth.on.crons.delete` | Cron job deletion | [`CronsDelete`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.CronsDelete) |\n| | `@auth.on.crons.search` | Listing cron jobs | [`CronsSearch`](../cloud/reference/sdk/python_sdk_ref.md#langgraph_sdk.auth.types.CronsSearch) |\n\n\n\n\n???+ note \"About Runs\"\n\n    Runs are scoped to their parent thread for access control. This means permissions are typically inherited from the thread, reflecting the conversational nature of the data model. All run operations (reading, listing) except creation are controlled by the thread's handlers.\n\n    There is a specific `create_run` handler for creating new runs because it had more arguments that you can view in the handler.\n\n\n\n\n## Next Steps\n\nFor implementation details:\n\n- Check out the introductory tutorial on [setting up authentication](../tutorials/auth/getting_started.md)\n- See the how-to guide on implementing a [custom auth handlers](../how-tos/auth/custom_auth.md)",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 23663,
    "word_count": 2712
  },
  {
    "title": "FAQ",
    "source": "concepts/faq.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# FAQ\n\nCommon questions and their answers!\n\n## Do I need to use LangChain to use LangGraph? What’s the difference?\n\nNo. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\n\n## How is LangGraph different from other agent frameworks?\n\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks. LangGraph provides a more expressive framework to handle your unique tasks without restricting you to a single black-box cognitive architecture.\n\n## Does LangGraph impact the performance of my app?\n\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\n\n## Is LangGraph open source? Is it free?\n\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\n\n## How are LangGraph and LangGraph Platform different?\n\nLangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.\n\n| Features            | LangGraph (open source)                                   | LangGraph Platform                                                                                     |\n| ------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n| Description         | Stateful orchestration framework for agentic applications | Scalable infrastructure for deploying LangGraph applications                                           |\n| SDKs                | Python and JavaScript                                     | Python and JavaScript                                                                                  |\n| HTTP APIs           | None                                                      | Yes - useful for retrieving & updating state or long-term memory, or creating a configurable assistant |\n| Streaming           | Basic                                                     | Dedicated mode for token-by-token messages                                                             |\n| Checkpointer        | Community contributed                                     | Supported out-of-the-box                                                                               |\n| Persistence Layer   | Self-managed                                              | Managed Postgres with efficient storage                                                                |\n| Deployment          | Self-managed                                              | • Cloud SaaS <br> • Free self-hosted <br> • Enterprise (paid self-hosted)                              |\n| Scalability         | Self-managed                                              | Auto-scaling of task queues and servers                                                                |\n| Fault-tolerance     | Self-managed                                              | Automated retries                                                                                      |\n| Concurrency Control | Simple threading                                          | Supports double-texting                                                                                |\n| Scheduling          | None                                                      | Cron scheduling                                                                                        |\n| Monitoring          | None                                                      | Integrated with LangSmith for observability                                                            |\n| IDE integration     | LangGraph Studio                                          | LangGraph Studio                                                                                       |\n\n## Is LangGraph Platform open source?\n\nNo. LangGraph Platform is proprietary software.\n\nThere is a free, self-hosted version of LangGraph Platform with access to basic features. The Cloud SaaS deployment option and the Self-Hosted deployment options are paid services. [Contact our sales team](https://www.langchain.com/contact-sales) to learn more.\n\nFor more information, see our [LangGraph Platform pricing page](https://www.langchain.com/pricing-langgraph-platform).\n\n## Does LangGraph work with LLMs that don't support tool calling?\n\nYes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.\n\n## Does LangGraph work with OSS LLMs?\n\nYes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don't. But tool calling is not necessary (see [this section](#does-langgraph-work-with-llms-that-dont-support-tool-calling)) so you can totally use LangGraph with OSS LLMs.\n\n## Can I use LangGraph Studio without logging in to LangSmith\n\nYes! You can use the [development version of LangGraph Server](../tutorials/langgraph-platform/local-server.md) to run the backend locally.\nThis will connect to the studio frontend hosted as part of LangSmith.\nIf you set an environment variable of `LANGSMITH_TRACING=false`, then no traces will be sent to LangSmith.\n\n## What does \"nodes executed\" mean for LangGraph Platform usage?\n\n**Nodes Executed** is the aggregate number of nodes in a LangGraph application that are called and completed successfully during an invocation of the application. If a node in the graph is not called during execution or ends in an error state, these nodes will not be counted. If a node is called and completes successfully multiple times, each occurrence will be counted.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 6301,
    "word_count": 705
  },
  {
    "title": "MCP",
    "source": "concepts/mcp.md",
    "content": "# MCP\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to language models. LangGraph agents can use tools defined on MCP servers through the `langchain-mcp-adapters` library.\n\n![MCP](../agents/assets/mcp.png)\n\nInstall the `langchain-mcp-adapters` library to use MCP tools in LangGraph:\n\n```bash\npip install langchain-mcp-adapters\n```",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 443,
    "word_count": 50
  },
  {
    "title": "Multi-agent systems",
    "source": "concepts/multi_agent.md",
    "content": "# Multi-agent systems\n\nAn [agent](./agentic_concepts.md#agent-architectures) is _a system that uses an LLM to decide the control flow of an application_. As you develop these systems, they might grow more complex over time, making them harder to manage and scale. For example, you might run into the following problems:\n\n- agent has too many tools at its disposal and makes poor decisions about which tool to call next\n- context grows too complex for a single agent to keep track of\n- there is a need for multiple specialization areas in the system (e.g. planner, researcher, math expert, etc.)\n\nTo tackle these, you might consider breaking your application into multiple smaller, independent agents and composing them into a **multi-agent system**. These independent agents can be as simple as a prompt and an LLM call, or as complex as a [ReAct](./agentic_concepts.md#tool-calling-agent) agent (and more!).\n\nThe primary benefits of using multi-agent systems are:\n\n- **Modularity**: Separate agents make it easier to develop, test, and maintain agentic systems.\n- **Specialization**: You can create expert agents focused on specific domains, which helps with the overall system performance.\n- **Control**: You can explicitly control how agents communicate (as opposed to relying on function calling).\n\n## Multi-agent architectures\n\n![](./img/multi_agent/architectures.png)\n\nThere are several ways to connect agents in a multi-agent system:\n\n- **Network**: each agent can communicate with [every other agent](../tutorials/multi_agent/multi-agent-collaboration.ipynb/). Any agent can decide which other agent to call next.\n- **Supervisor**: each agent communicates with a single [supervisor](../tutorials/multi_agent/agent_supervisor.md/) agent. Supervisor agent makes decisions on which agent should be called next.\n- **Supervisor (tool-calling)**: this is a special case of supervisor architecture. Individual agents can be represented as tools. In this case, a supervisor agent uses a tool-calling LLM to decide which of the agent tools to call, as well as the arguments to pass to those agents.\n- **Hierarchical**: you can define a multi-agent system with [a supervisor of supervisors](../tutorials/multi_agent/hierarchical_agent_teams.ipynb/). This is a generalization of the supervisor architecture and allows for more complex control flows.\n- **Custom multi-agent workflow**: each agent communicates with only a subset of agents. Parts of the flow are deterministic, and only some agents can decide which other agents to call next.\n\n### Handoffs\n\nIn multi-agent architectures, agents can be represented as graph nodes. Each agent node executes its step(s) and decides whether to finish execution or route to another agent, including potentially routing to itself (e.g., running in a loop). A common pattern in multi-agent interactions is **handoffs**, where one agent _hands off_ control to another. Handoffs allow you to specify:\n\n- **destination**: target agent to navigate to (e.g., name of the node to go to)\n- **payload**: [information to pass to that agent](#communication-and-state-management) (e.g., state update)\n\nTo implement handoffs in LangGraph, agent nodes can return [`Command`](./low_level.md#command) object that allows you to combine both control flow and state updates:\n\n```python\ndef agent(state) -> Command[Literal[\"agent\", \"another_agent\"]]:\n    # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    goto = get_next_agent(...)  # 'agent' / 'another_agent'\n    return Command(\n        # Specify which agent to call next\n        goto=goto,\n        # Update the graph state\n        update={\"my_state_key\": \"my_state_value\"}\n    )\n```\n\n\n\n\n\nIn a more complex scenario where each agent node is itself a graph (i.e., a [subgraph](./subgraphs.md)), a node in one of the agent subgraphs might want to navigate to a different agent. For example, if you have two agents, `alice` and `bob` (subgraph nodes in a parent graph), and `alice` needs to navigate to `bob`, you can set `graph=Command.PARENT` in the `Command` object:\n\n```python\ndef some_node_inside_alice(state):\n    return Command(\n        goto=\"bob\",\n        update={\"my_state_key\": \"my_state_value\"},\n        # specify which graph to navigate to (defaults to the current graph)\n        graph=Command.PARENT,\n    )\n```\n\n\n\n\n\n!!! note\n\n    If you need to support visualization for subgraphs communicating using `Command(graph=Command.PARENT)` you would need to wrap them in a node function with `Command` annotation:\n    Instead of this:\n\n    ```python\n    builder.add_node(alice)\n    ```\n\n    you would need to do this:\n\n    ```python\n    def call_alice(state) -> Command[Literal[\"bob\"]]:\n        return alice.invoke(state)\n\n    builder.add_node(\"alice\", call_alice)\n    ```\n\n\n\n\n\n#### Handoffs as tools\n\nOne of the most common agent types is a [tool-calling agent](../agents/overview.md). For those types of agents, a common pattern is wrapping a handoff in a tool call:\n\n```python\nfrom langchain_core.tools import tool\n\n@tool\ndef transfer_to_bob():\n    \"\"\"Transfer to bob.\"\"\"\n    return Command(\n        # name of the agent (node) to go to\n        goto=\"bob\",\n        # data to send to the agent\n        update={\"my_state_key\": \"my_state_value\"},\n        # indicate to LangGraph that we need to navigate to\n        # agent node in a parent graph\n        graph=Command.PARENT,\n    )\n```\n\n\n\n\n\nThis is a special case of updating the graph state from tools where, in addition to the state update, the control flow is included as well.\n\n!!! important\n\n      If you want to use tools that return `Command`, you can use the prebuilt [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) / [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) components, or else implement your own logic:\n\n      ```python\n      def call_tools(state):\n          ...\n          commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n          return commands\n      ```\n\n\n\n\nLet's now take a closer look at the different multi-agent architectures.\n\n### Network\n\nIn this architecture, agents are defined as graph nodes. Each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. This architecture is good for problems that do not have a clear hierarchy of agents or a specific sequence in which agents should be called.\n\n```python\nfrom typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nmodel = ChatOpenAI()\n\ndef agent_1(state: MessagesState) -> Command[Literal[\"agent_2\", \"agent_3\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the LLM's decision\n    # if the LLM returns \"__end__\", the graph will finish execution\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\ndef agent_2(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_3\", END]]:\n    response = model.invoke(...)\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\ndef agent_3(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", END]]:\n    ...\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\nbuilder.add_node(agent_3)\n\nbuilder.add_edge(START, \"agent_1\")\nnetwork = builder.compile()\n```\n\n\n\n\n\n### Supervisor\n\nIn this architecture, we define agents as nodes and add a supervisor node (LLM) that decides which agent nodes should be called next. We use [`Command`](./low_level.md#command) to route execution to the appropriate agent node based on supervisor's decision. This architecture also lends itself well to running multiple agents in parallel or using [map-reduce](../how-tos/graph-api.md#map-reduce-and-the-send-api) pattern.\n\n```python\nfrom typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nmodel = ChatOpenAI()\n\ndef supervisor(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the supervisor's decision\n    # if the supervisor returns \"__end__\", the graph will finish execution\n    return Command(goto=response[\"next_agent\"])\n\ndef agent_1(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    return Command(\n        goto=\"supervisor\",\n        update={\"messages\": [response]},\n    )\n\ndef agent_2(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n    response = model.invoke(...)\n    return Command(\n        goto=\"supervisor\",\n        update={\"messages\": [response]},\n    )\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(supervisor)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n\nbuilder.add_edge(START, \"supervisor\")\n\nsupervisor = builder.compile()\n```\n\n\n\n\n\n\n\nCheck out this [tutorial](../tutorials/multi_agent/agent_supervisor.md) for an example of supervisor multi-agent architecture.\n\n### Supervisor (tool-calling)\n\nIn this variant of the [supervisor](#supervisor) architecture, we define a supervisor [agent](./agentic_concepts.md#agent-architectures) which is responsible for calling sub-agents. The sub-agents are exposed to the supervisor as tools, and the supervisor agent decides which tool to call next. The supervisor agent follows a [standard implementation](./agentic_concepts.md#tool-calling-agent) as an LLM running in a while loop calling tools until it decides to stop.\n\n```python\nfrom typing import Annotated\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import InjectedState, create_react_agent\n\nmodel = ChatOpenAI()\n\n# this is the agent function that will be called as tool\n# notice that you can pass the state to the tool via InjectedState annotation\ndef agent_1(state: Annotated[dict, InjectedState]):\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    # return the LLM response as a string (expected tool response format)\n    # this will be automatically turned to ToolMessage\n    # by the prebuilt create_react_agent (supervisor)\n    return response.content\n\ndef agent_2(state: Annotated[dict, InjectedState]):\n    response = model.invoke(...)\n    return response.content\n\ntools = [agent_1, agent_2]\n# the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph\n# that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node\nsupervisor = create_react_agent(model, tools)\n```\n\n\n\n\n\n### Hierarchical\n\nAs you add more agents to your system, it might become too hard for the supervisor to manage all of them. The supervisor might start making poor decisions about which agent to call next, or the context might become too complex for a single supervisor to keep track of. In other words, you end up with the same problems that motivated the multi-agent architecture in the first place.\n\nTo address this, you can design your system _hierarchically_. For example, you can create separate, specialized teams of agents managed by individual supervisors, and a top-level supervisor to manage the teams.\n\n```python\nfrom typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.types import Command\nmodel = ChatOpenAI()\n\n# define team 1 (same as the single supervisor example above)\n\ndef team_1_supervisor(state: MessagesState) -> Command[Literal[\"team_1_agent_1\", \"team_1_agent_2\", END]]:\n    response = model.invoke(...)\n    return Command(goto=response[\"next_agent\"])\n\ndef team_1_agent_1(state: MessagesState) -> Command[Literal[\"team_1_supervisor\"]]:\n    response = model.invoke(...)\n    return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\n\ndef team_1_agent_2(state: MessagesState) -> Command[Literal[\"team_1_supervisor\"]]:\n    response = model.invoke(...)\n    return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\n\nteam_1_builder = StateGraph(Team1State)\nteam_1_builder.add_node(team_1_supervisor)\nteam_1_builder.add_node(team_1_agent_1)\nteam_1_builder.add_node(team_1_agent_2)\nteam_1_builder.add_edge(START, \"team_1_supervisor\")\nteam_1_graph = team_1_builder.compile()\n\n# define team 2 (same as the single supervisor example above)\nclass Team2State(MessagesState):\n    next: Literal[\"team_2_agent_1\", \"team_2_agent_2\", \"__end__\"]\n\ndef team_2_supervisor(state: Team2State):\n    ...\n\ndef team_2_agent_1(state: Team2State):\n    ...\n\ndef team_2_agent_2(state: Team2State):\n    ...\n\nteam_2_builder = StateGraph(Team2State)\n...\nteam_2_graph = team_2_builder.compile()\n\n\n# define top-level supervisor\n\nbuilder = StateGraph(MessagesState)\ndef top_level_supervisor(state: MessagesState) -> Command[Literal[\"team_1_graph\", \"team_2_graph\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which team to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_team\" field)\n    response = model.invoke(...)\n    # route to one of the teams or exit based on the supervisor's decision\n    # if the supervisor returns \"__end__\", the graph will finish execution\n    return Command(goto=response[\"next_team\"])\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(top_level_supervisor)\nbuilder.add_node(\"team_1_graph\", team_1_graph)\nbuilder.add_node(\"team_2_graph\", team_2_graph)\nbuilder.add_edge(START, \"top_level_supervisor\")\nbuilder.add_edge(\"team_1_graph\", \"top_level_supervisor\")\nbuilder.add_edge(\"team_2_graph\", \"top_level_supervisor\")\ngraph = builder.compile()\n```\n\n\n\n\n\n### Custom multi-agent workflow\n\nIn this architecture we add individual agents as graph nodes and define the order in which agents are called ahead of time, in a custom workflow. In LangGraph the workflow can be defined in two ways:\n\n- **Explicit control flow (normal edges)**: LangGraph allows you to explicitly define the control flow of your application (i.e. the sequence of how agents communicate) explicitly, via [normal graph edges](./low_level.md#normal-edges). This is the most deterministic variant of this architecture above — we always know which agent will be called next ahead of time.\n\n- **Dynamic control flow (Command)**: in LangGraph you can allow LLMs to decide parts of your application control flow. This can be achieved by using [`Command`](./low_level.md#command). A special case of this is a [supervisor tool-calling](#supervisor-tool-calling) architecture. In that case, the tool-calling LLM powering the supervisor agent will make decisions about the order in which the tools (agents) are being called.\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START\n\nmodel = ChatOpenAI()\n\ndef agent_1(state: MessagesState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\ndef agent_2(state: MessagesState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n# define the flow explicitly\nbuilder.add_edge(START, \"agent_1\")\nbuilder.add_edge(\"agent_1\", \"agent_2\")\n```\n\n\n\n\n\n## Communication and state management\n\nThe most important thing when building multi-agent systems is figuring out how the agents communicate.\n\nA common, generic way for agents to communicate is via a list of messages. This opens up the following questions:\n\n- Do agents communicate [**via handoffs or via tool calls**](#handoffs-vs-tool-calls)?\n- What messages are [**passed from one agent to the next**](#message-passing-between-agents)?\n- How are [**handoffs represented in the list of messages**](#representing-handoffs-in-message-history)?\n- How do you [**manage state for subagents**](#state-management-for-subagents)?\n\nAdditionally, if you are dealing with more complex agents or wish to keep individual agent state separate from the multi-agent system state, you may need to use [**different state schemas**](#using-different-state-schemas).\n\n### Handoffs vs tool calls\n\nWhat is the \"payload\" that is being passed around between agents? In most of the architectures discussed above, the agents communicate via [handoffs](#handoffs) and pass the [graph state](./low_level.md#state) as part of the handoff payload. Specifically, agents pass around lists of messages as part of the graph state. In the case of the [supervisor with tool-calling](#supervisor-tool-calling), the payloads are tool call arguments.\n\n![](./img/multi_agent/request.png)\n\n### Message passing between agents\n\nThe most common way for agents to communicate is via a shared state channel, typically a list of messages. This assumes that there is always at least a single channel (key) in the state that is shared by the agents (e.g., `messages`). When communicating via a shared message list, there is an additional consideration: should the agents [share the full history](#sharing-full-thought-process) of their thought process or only [the final result](#sharing-only-final-results)?\n\n![](./img/multi_agent/response.png)\n\n#### Sharing full thought process\n\nAgents can **share the full history** of their thought process (i.e., \"scratchpad\") with all other agents. This \"scratchpad\" would typically look like a [list of messages](./low_level.md#why-use-messages). The benefit of sharing the full thought process is that it might help other agents make better decisions and improve reasoning ability for the system as a whole. The downside is that as the number of agents and their complexity grows, the \"scratchpad\" will grow quickly and might require additional strategies for [memory management](../how-tos/memory/add-memory.md).\n\n#### Sharing only final results\n\nAgents can have their own private \"scratchpad\" and only **share the final result** with the rest of the agents. This approach might work better for systems with many agents or agents that are more complex. In this case, you would need to define agents with [different state schemas](#using-different-state-schemas).\n\nFor agents called as tools, the supervisor determines the inputs based on the tool schema. Additionally, LangGraph allows [passing state](../how-tos/tool-calling.md#short-term-memory) to individual tools at runtime, so subordinate agents can access parent state, if needed.\n\n#### Indicating agent name in messages\n\nIt can be helpful to indicate which agent a particular AI message is from, especially for long message histories. Some LLM providers (like OpenAI) support adding a `name` parameter to messages — you can use that to attach the agent name to the message. If that is not supported, you can consider manually injecting the agent name into the message content, e.g., `<agent>alice</agent><message>message from alice</message>`.\n\n### Representing handoffs in message history\n\nHandoffs are typically done via the LLM calling a dedicated [handoff tool](#handoffs-as-tools). This is represented as an [AI message](https://python.langchain.com/docs/concepts/messages/#aimessage) with tool calls that is passed to the next agent (LLM). Most LLM providers don't support receiving AI messages with tool calls **without** corresponding tool messages.\n\n\n\n\nYou therefore have two options:\n\n1. Add an extra [tool message](https://python.langchain.com/docs/concepts/messages/#toolmessage) to the message list, e.g., \"Successfully transferred to agent X\"\n2. Remove the AI message with the tool calls\n\n\n\n\nIn practice, we see that most developers opt for option (1).\n\n### State management for subagents\n\nA common practice is to have multiple agents communicating on a shared message list, but only [adding their final messages to the list](#sharing-only-final-results). This means that any intermediate messages (e.g., tool calls) are not saved in this list.\n\nWhat if you **do** want to save these messages so that if this particular subagent is invoked in the future you can pass those back in?\n\nThere are two high-level approaches to achieve that:\n\n1. Store these messages in the shared message list, but filter the list before passing it to the subagent LLM. For example, you can choose to filter out all tool calls from **other** agents.\n2. Store a separate message list for each agent (e.g., `alice_messages`) in the subagent's graph state. This would be their \"view\" of what the message history looks like.\n\n\n\n\n### Using different state schemas\n\nAn agent might need to have a different state schema from the rest of the agents. For example, a search agent might only need to keep track of queries and retrieved documents. There are two ways to achieve this in LangGraph:\n\n- Define [subgraph](./subgraphs.md) agents with a separate state schema. If there are no shared state keys (channels) between the subgraph and the parent graph, it's important to [add input / output transformations](../how-tos/subgraph.md#different-state-schemas) so that the parent graph knows how to communicate with the subgraphs.\n- Define agent node functions with a [private input state schema](../how-tos/graph-api.md#pass-private-state-between-nodes) that is distinct from the overall graph state schema. This allows passing information that is only needed for executing that particular agent.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 22526,
    "word_count": 2847
  },
  {
    "title": "Scalability & Resilience",
    "source": "concepts/scalability_and_resilience.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Scalability & Resilience\n\nLangGraph Platform is designed to scale horizontally with your workload. Each instance of the service is stateless, and keeps no resources in memory. The service is designed to gracefully handle new instances being added or removed, including hard shutdown cases.\n\n## Server scalability\n\nAs you add more instances to a service, they will share the HTTP load as long as an appropriate load balancer mechanism is placed in front of them. In most deployment modalities we configure a load balancer for the service automatically. In the “self-hosted without control plane” modality it’s your responsibility to add a load balancer. Since the instances are stateless any load balancing strategy will work, no session stickiness is needed, or recommended. Any instance of the server can communicate with any queue instance (through Redis PubSub), meaning that requests to cancel or stream an in-progress run can be handled by any arbitrary instance.\n\n## Queue scalability\n\nAs you add more instances to a service, they will increase run throughput linearly, as each instance is configured to handle a set number of concurrent runs (by default 10). Each attempt for each run will be handled by a single instance, with exactly-once semantics enforced through Postgres’s MVCC model (refer to section below for crash resilience details). Attempts that fail due to transient database errors are retried up to 3 times. We do not make use of long-lived transactions or locks, this enables us to make more efficient use of Postgres resources.\n\n## Resilience\n\nWhile a run is being handled by a queue instance, a periodic heartbeat timestamp will be recorded in Redis by that queue worker.\n\nWhen a graceful shutdown request is received (SIGINT) an instance enters shutdown mode, which\n\n- stops accepting new HTTP requests\n- gives any in-progress runs a limited number of seconds to finish (if not finished it will be put back in the queue)\n- stops the instance from picking up more runs from the queue\n\nIf a hard shutdown occurs due to a server crash or an infrastructure failure, any runs that were in progress will be picked up by an internal sweeper task that looks for in-progress runs that have breached their heartbeat window. The sweeper runs every 2 minutes and will put the runs back in the queue for another instance to pick them up.\n\n## Postgres resilience\n\nFor deployment modalities where we manage the Postgres database, we have periodic backups and continuously replicated standby replicas for automatic failover. This Postgres configuration is available in the [Cloud SaaS deployment option](../concepts/langgraph_cloud.md) for [`Production` deployment types](../concepts/langgraph_control_plane.md#deployment-types) only.\n\nAll communication with Postgres implements retries for retry-able errors. If Postgres is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Postgres will render the LangGraph Server unavailable.\n\n## Redis resilience\n\nAll data that requires durable storage is stored in Postgres, not Redis. Redis is used only for ephemeral metadata, and communication between instances. Therefore we place no durability requirements on Redis.\n\nAll communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the LangGraph Server unavailable.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3539,
    "word_count": 541
  },
  {
    "title": "Graph API concepts",
    "source": "concepts/low_level.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Graph API concepts\n\n## Graphs\n\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\n\n1. [`State`](#state): A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\n\n2. [`Nodes`](#nodes): Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\n\n3. [`Edges`](#edges): Functions that determine which `Node` to execute next based on the current state. They can be conditional branches or fixed transitions.\n\nBy composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: `Nodes` and `Edges` are nothing more than functions - they can contain an LLM or just good ol' code.\n\nIn short: _nodes do the work, edges tell what to do next_.\n\nLangGraph's underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete \"super-steps.\"\n\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or \"channels\"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit.\n\n### StateGraph\n\nThe `StateGraph` class is the main graph class to use. This is parameterized by a user defined `State` object.\n\n### Compiling your graph\n\nTo build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed?\n\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](./persistence.md) and breakpoints. You compile your graph by just calling the `.compile` method:\n\n```python\ngraph = graph_builder.compile(...)\n```\n\n\n\n\n\nYou **MUST** compile your graph before you can use it.\n\n## State\n\nThe first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.\n\n\n\n\n### Schema\n\nThe main documented way to specify the schema of a graph is by using a [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict). If you want to provide default values in your state, use a [`dataclass`](https://docs.python.org/3/library/dataclasses.html). We also support using a Pydantic [BaseModel](../how-tos/graph-api.md#use-pydantic-models-for-graph-state) as your graph state if you want recursive data validation (though note that pydantic is less performant than a `TypedDict` or `dataclass`).\n\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the [guide here](../how-tos/graph-api.md#define-input-and-output-schemas) for how to use.\n\n\n\n\n#### Multiple schemas\n\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\n\n- Internal nodes can pass information that is not required in the graph's input / output.\n- We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\n\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`.\n\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an \"internal\" schema that contains _all_ keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the \"internal\" schema to constrain the input and output of the graph. See [this guide](../how-tos/graph-api.md#define-input-and-output-schemas) for more detail.\n\nLet's look at an example:\n\n```python\nclass InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -> OverallState:\n    # Write to OverallState\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -> PrivateState:\n    # Read from OverallState, write to PrivateState\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -> OutputState:\n    # Read from PrivateState, write to OutputState\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n# {'graph_output': 'My name is Lance'}\n```\n\n\n\n\n\nThere are two subtle and important points to note here:\n\n1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node _can write to any state channel in the graph state._ The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.\n\n2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because _nodes can also declare additional state channels_ as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.\n\n\n\n\n### Reducers\n\nReducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\n\n#### Default Reducer\n\nThese two examples show how to use the default reducer:\n\n**Example A:**\n\n```python\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n```\n\n\n\n\n\nIn this example, no reducer functions are specified for any key. Let's assume the input to the graph is:\n\n`{\"foo\": 1, \"bar\": [\"hi\"]}`. Let's then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"bye\"]}`\n\n\n\n\n**Example B:**\n\n```python\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n```\n\nIn this example, we've used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{\"foo\": 1, \"bar\": [\"hi\"]}`. Let's then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}`. Notice here that the `bar` key is updated by adding the two lists together.\n\n\n\n\n### Working with Messages in Graph State\n\n#### Why use messages?\n\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as `HumanMessage` (user input) or `AIMessage` (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide.\n\n\n\n\n#### Using Messages in your Graph\n\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.\n\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `add_messages` function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\n\n\n\n\n#### Serialization\n\nIn addition to keeping track of message IDs, the `add_messages` function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format:\n\n```python\n# this is supported\n{\"messages\": [HumanMessage(content=\"message\")]}\n\n# and this is also supported\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\n```\n\nSince the state updates are always deserialized into LangChain `Messages` when using `add_messages`, you should use dot notation to access message attributes, like `state[\"messages\"][-1].content`. Below is an example of a graph that uses `add_messages` as its reducer function.\n\n```python\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n```\n\n\n\n\n\n#### MessagesState\n\nSince having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the `add_messages` reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\n\n```python\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    documents: list[str]\n```\n\n\n\n## Nodes\n\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\n\n1. `state`: The [state](#state) of the graph\n2. `config`: A `RunnableConfig` object that contains configuration information like `thread_id` and tracing information like `tags`\n3. `runtime`: A `Runtime` object that contains [runtime `context`](#runtime-context) and other information like `store` and `stream_writer`\n\nSimilar to `NetworkX`, you add these nodes to a graph using the [add_node](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_node) method:\n\n```python\nfrom dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom langgraph.runtime import Runtime\n\nclass State(TypedDict):\n    input: str\n    results: str\n\n@dataclass\nclass Context:\n    user_id: str\n\nbuilder = StateGraph(State)\n\ndef plain_node(state: State):\n    return state\n\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\n    print(\"In node: \", runtime.context.user_id)\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\ndef node_with_config(state: State, config: RunnableConfig):\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\nbuilder.add_node(\"plain_node\", plain_node)\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\nbuilder.add_node(\"node_with_config\", node_with_config)\n...\n```\n\n\n\n\n\nBehind the scenes, functions are converted to [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)s, which add batch and async support to your function, along with native tracing and debugging.\n\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\n\n```python\nbuilder.add_node(my_node)\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\n```\n\n\n\n\n\n### `START` Node\n\nThe `START` Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\n\n```python\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n\n\n\n\n### `END` Node\n\nThe `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\n\n```python\nfrom langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n```\n\n\n\n\n\n### Node Caching\n\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\n\n- Specify a cache when compiling a graph (or specifying an entrypoint)\n- Specify a cache policy for nodes. Each cache policy supports:\n  - `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.\n  - `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.\n\nFor example:\n\n```python\nimport time\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.types import CachePolicy\n\n\nclass State(TypedDict):\n    x: int\n    result: int\n\n\nbuilder = StateGraph(State)\n\n\ndef expensive_node(state: State) -> dict[str, int]:\n    # expensive computation\n    time.sleep(2)\n    return {\"result\": state[\"x\"] * 2}\n\n\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\nbuilder.set_entry_point(\"expensive_node\")\nbuilder.set_finish_point(\"expensive_node\")\n\ngraph = builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (1)!\n[{'expensive_node': {'result': 10}}]\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (2)!\n[{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]\n```\n\n1. First run takes two seconds to run (due to mocked expensive computation).\n2. Second run utilizes cache and returns quickly.\n\n\n\n\n## Edges\n\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\n\n- Normal Edges: Go directly from one node to the next.\n- Conditional Edges: Call a function to determine which node(s) to go to next.\n- Entry Point: Which node to call first when user input arrives.\n- Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\n\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.\n\n### Normal Edges\n\nIf you **always** want to go from node A to node B, you can use the [add_edge](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly.\n\n```python\ngraph.add_edge(\"node_a\", \"node_b\")\n```\n\n\n\n\n\n### Conditional Edges\n\nIf you want to **optionally** route to 1 or more edges (or optionally terminate), you can use the [add_conditional_edges](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a \"routing function\" to call after that node is executed:\n\n```python\ngraph.add_conditional_edges(\"node_a\", routing_function)\n```\n\nSimilar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.\n\nBy default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\n\nYou can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node.\n\n```python\ngraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\n```\n\n\n\n\n\n!!! tip\n\n    Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.\n\n### Entry Point\n\nThe entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph.\n\n```python\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n\n\n\n\n### Conditional Entry Point\n\nA conditional entry point lets you start at different nodes depending on custom logic. You can use [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to accomplish this.\n\n```python\nfrom langgraph.graph import START\n\ngraph.add_conditional_edges(START, routing_function)\n```\n\nYou can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node.\n\n```python\ngraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\n```\n\n\n\n\n\n## `Send`\n\nBy default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object).\n\nTo support this design pattern, LangGraph supports returning [`Send`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node.\n\n```python\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\n```\n\n\n\n\n\n## `Command`\n\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [`Command`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions:\n\n```python\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n```\n\nWith `Command` you can also achieve dynamic control flow behavior (identical to [conditional edges](#conditional-edges)):\n\n```python\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n```\n\n\n\n\n\n!!! important\n\n    When returning `Command` in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal[\"my_other_node\"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`.\n\nCheck out this [how-to guide](../how-tos/graph-api.md#combine-control-flow-and-state-updates-with-command) for an end-to-end example of how to use `Command`.\n\n### When should I use Command instead of conditional edges?\n\n- Use `Command` when you need to **both** update the graph state **and** route to a different node. For example, when implementing [multi-agent handoffs](./multi_agent.md#handoffs) where it's important to route to a different agent and pass some information to that agent.\n- Use [conditional edges](#conditional-edges) to route between nodes conditionally without updating the state.\n\n### Navigating to a node in a parent graph\n\nIf you are using [subgraphs](./subgraphs.md), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:\n\n```python\ndef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n```\n\n!!! note\n\n    Setting `graph` to `Command.PARENT` will navigate to the closest parent graph.\n\n!!! important \"State updates with `Command.PARENT`\"\n\n    When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you're updating in the parent graph state. See this [example](../how-tos/graph-api.md#navigate-to-a-node-in-a-parent-graph).\n\n\n\n\n\n\n\nThis is particularly useful when implementing [multi-agent handoffs](./multi_agent.md#handoffs).\n\nCheck out [this guide](../how-tos/graph-api.md#navigate-to-a-node-in-a-parent-graph) for detail.\n\n### Using inside tools\n\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\n\nRefer to [this guide](../how-tos/graph-api.md#use-inside-tools) for detail.\n\n### Human-in-the-loop\n\n`Command` is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, `Command` is then used to supply the input and resume execution via `Command(resume=\"User input\")`. Check out [this conceptual guide](./human_in_the_loop.md) for more information.\n\n\n\n\n## Graph Migrations\n\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\n\n- For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\n- For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.\n- For modifying state, we have full backwards and forwards compatibility for adding and removing keys\n- State keys that are renamed lose their saved state in existing threads\n- State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.\n\n## Runtime Context\n\nWhen creating a graph, you can specify a `context_schema` for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\n\n```python\n@dataclass\nclass ContextSchema:\n    llm_provider: str = \"openai\"\n\ngraph = StateGraph(State, context_schema=ContextSchema)\n```\n\n\n\n\n\nYou can then pass this context into the graph using the `context` parameter of the `invoke` method.\n\n```python\ngraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\n```\n\n\n\n\n\nYou can then access and use this context inside a node or conditional edge:\n\n```python\nfrom langgraph.runtime import Runtime\n\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\n    llm = get_llm(runtime.context.llm_provider)\n    ...\n```\n\nSee [this guide](../how-tos/graph-api.md#add-runtime-configuration) for a full breakdown on configuration.\n:::\n\n\n\n### Recursion Limit\n\nThe recursion limit sets the maximum number of [super-steps](#graphs) the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `.invoke`/`.stream` via the config dictionary. Importantly, `recursion_limit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below:\n\n```python\ngraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\n```\n\nRead [this how-to](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/) to learn more about how the recursion limit works.\n\n\n\n\n## Visualization\n\nIt's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](../how-tos/graph-api.md#visualize-your-graph) for more info.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 28399,
    "word_count": 3859
  },
  {
    "title": "LangGraph Control Plane",
    "source": "concepts/langgraph_control_plane.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph Control Plane\n\nThe term \"control plane\" is used broadly to refer to the control plane UI where users create and update [LangGraph Servers](./langgraph_server.md) (deployments) and the control plane APIs that support the UI experience.\n\nWhen a user makes an update through the control plane UI, the update is stored in the control plane state. The [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application polls for these updates by calling the control plane APIs.\n\n## Control Plane UI\n\nFrom the control plane UI, you can:\n\n- View a list of outstanding deployments.\n- View details of an individual deployment.\n- Create a new deployment.\n- Update a deployment.\n- Update environment variables for a deployment.\n- View build and server logs of a deployment.\n- View deployment metrics such as CPU and memory usage.\n- Delete a deployment.\n\nThe Control Plane UI is embedded in [LangSmith](https://docs.smith.langchain.com/langgraph_cloud).\n\n## Control Plane API\n\nThis section describes the data model of the control plane API. The API is used to create, update, and delete deployments. See the [control plane API reference](../cloud/reference/api/api_ref_control_plane.md) for more details.\n\n### Deployment\n\nA deployment is an instance of a LangGraph Server. A single deployment can have many revisions.\n\n### Revision\n\nA revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update secrets for a deployment, a new revision must be created.\n\n## Control Plane Features\n\nThis section describes various features of the control plane.\n\n### Deployment Types\n\nFor simplicity, the control plane offers two deployment types with different resource allocations: `Development` and `Production`.\n\n| **Deployment Type** | **CPU/Memory**  | **Scaling**       | **Database**                                                                     |\n| ------------------- | --------------- | ----------------- | -------------------------------------------------------------------------------- |\n| Development         | 1 CPU, 1 GB RAM | Up to 1 replica   | 10 GB disk, no backups                                                           |\n| Production          | 2 CPU, 2 GB RAM | Up to 10 replicas | Autoscaling disk, automatic backups, highly available (multi-zone configuration) |\n\nCPU and memory resources are per replica.\n\n!!! warning \"Immutable Deployment Type\"\n\n    Once a deployment is created, the deployment type cannot be changed.\n\n!!! info \"Self-Hosted Deployment\"\nResources for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments can be fully customized. Deployment types are only applicable for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\n\n#### Production\n\n`Production` type deployments are suitable for \"production\" workloads. For example, select `Production` for customer-facing applications in the critical path.\n\nResources for `Production` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact support@langchain.dev to request an increase in resources.\n\n#### Development\n\n`Development` type deployments are suitable development and testing. For example, select `Development` for internal testing environments. `Development` type deployments are not suitable for \"production\" workloads.\n\n!!! danger \"Preemptible Compute Infrastructure\"\n`Development` type deployments (API server, queue server, and database) are provisioned on preemptible compute infrastructure. This means the compute infrastructure **may be terminated at any time without notice**. This may result in intermittent...\n\n    - Redis connection timeouts/errors\n    - Postgres connection timeouts/errors\n    - Failed or retrying background runs\n\n    This behavior is expected. Preemptible compute infrastructure **significantly reduces the cost to provision a `Development` type deployment**. By design, LangGraph Server is fault-tolerant. The implementation will automatically attempt to recover from Redis/Postgres connection errors and retry failed background runs.\n\n    `Production` type deployments are provisioned on durable compute infrastructure, not preemptible compute infrastructure.\n\nDatabase disk size for `Development` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. For most use cases, [TTLs](../how-tos/ttl/configure_ttl.md) should be configured to manage disk usage. Contact support@langchain.dev to request an increase in resources.\n\n### Database Provisioning\n\nThe control plane and [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application coordinate to automatically create a Postgres database for each deployment. The database serves as the [persistence layer](../concepts/persistence.md) for the deployment.\n\nWhen implementing a LangGraph application, a [checkpointer](../concepts/persistence.md#checkpointer-libraries) does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured.\n\nThere is no direct access to the database. All access to the database occurs through the [LangGraph Server](../concepts/langgraph_server.md).\n\nThe database is never deleted until the deployment itself is deleted.\n\n!!! info\nA custom Postgres instance can be configured for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.\n\n### Asynchronous Deployment\n\nInfrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.\n\n- When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.\n- When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.\n- The deployment process for each revision contains a build step, which can take up to a few minutes.\n\nThe control plane and [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application coordinate to achieve asynchronous deployments.\n\n### Monitoring\n\nAfter a deployment is ready, the control plane monitors the deployment and records various metrics, such as:\n\n- CPU and memory usage of the deployment.\n- Number of container restarts.\n- Number of replicas (this will increase with [autoscaling](../concepts/langgraph_data_plane.md#autoscaling)).\n- [Postgres](../concepts/langgraph_data_plane.md#postgres) CPU, memory usage, and disk usage.\n- [LangGraph Server queue](../concepts/langgraph_server.md#persistence-and-task-queue) pending/active run count.\n- [LangGraph Server API](../concepts/langgraph_server.md) success response count, error response count, and latency.\n\nThese metrics are displayed as charts in the Control Plane UI.\n\n### LangSmith Integration\n\nA [LangSmith](https://docs.smith.langchain.com/) tracing project and LangSmith API key are automatically created for each deployment. The deployment uses the API key to automatically send traces to LangSmith.\n\n- The tracing project has the same name as the deployment.\n- The API key has the description `LangGraph Platform: <deployment_name>`.\n- The API key is never revealed and cannot be deleted manually.\n- When creating a deployment, the `LANGCHAIN_TRACING` and `LANGSMITH_API_KEY`/`LANGCHAIN_API_KEY` environment variables do not need to be specified; they are set automatically by the control plane.\n\nWhen a deployment is deleted, the traces and the tracing project are not deleted. However, the API will be deleted when the deployment is deleted.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 8180,
    "word_count": 1048
  },
  {
    "title": "Durable Execution",
    "source": "concepts/durable_execution.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Durable Execution\n\n**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](./human_in_the_loop.md), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).\n\nLangGraph's built-in [persistence](./persistence.md) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](./human_in_the_loop.md) interactions -- it can be resumed from its last recorded state.\n\n!!! tip\n\n    If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\n    To make the most of durable execution, ensure that your workflow is designed to be [deterministic](#determinism-and-consistent-replay) and [idempotent](#determinism-and-consistent-replay) and wrap any side effects or non-deterministic operations inside [tasks](./functional_api.md#task). You can use [tasks](./functional_api.md#task) from both the [StateGraph (Graph API)](./low_level.md) and the [Functional API](./functional_api.md).\n\n## Requirements\n\nTo leverage durable execution in LangGraph, you need to:\n\n1. Enable [persistence](./persistence.md) in your workflow by specifying a [checkpointer](./persistence.md#checkpointer-libraries) that will save workflow progress.\n2. Specify a [thread identifier](./persistence.md#threads) when executing a workflow. This will track the execution history for a particular instance of the workflow.\n\n3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside [tasks](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.task) to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see [Determinism and Consistent Replay](#determinism-and-consistent-replay).\n\n\n\n\n\n## Determinism and Consistent Replay\n\nWhen you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate [starting point](#starting-points-for-resuming-workflows) from which to pick up where it left off. This means that the workflow will replay all steps from the [starting point](#starting-points-for-resuming-workflows) until it reaches the point where it was stopped.\n\nAs a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside [tasks](./functional_api.md#task) or [nodes](./low_level.md#nodes).\n\nTo ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:\n\n- **Avoid Repeating Work**: If a [node](./low_level.md#nodes) contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.\n- **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.\n- **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow's resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.\n\nFor some examples of pitfalls to avoid, see the [Common Pitfalls](./functional_api.md#common-pitfalls) section in the functional API, which shows\nhow to structure your code using **tasks** to avoid these issues. The same principles apply to the [StateGraph (Graph API)](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph).\n\n\n\n\n## Durability modes\n\nLangGraph supports three durability modes that allow you to balance performance and data consistency based on your application's requirements. The durability modes, from least to most durable, are as follows:\n\n- [`\"exit\"`](#exit)\n- [`\"async\"`](#async)\n- [`\"sync\"`](#sync)\n\nA higher durability mode add more overhead to the workflow execution.\n\n!!! version-added \"Added in version 0.6.0\"\n\n    Use the `durability` parameter instead of `checkpoint_during` (deprecated in v0.6.0) for persistence policy management:\n    \n    * `durability=\"async\"` replaces `checkpoint_during=True`\n    * `durability=\"exit\"` replaces `checkpoint_during=False`\n    \n    for persistence policy management, with the following mapping:\n\n    * `checkpoint_during=True` -> `durability=\"async\"`\n    * `checkpoint_during=False` -> `durability=\"exit\"`\n\n### `\"exit\"`\n\nChanges are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.\n\n### `\"async\"`\n\nChanges are persisted asynchronously while the next step executes. This provides good performance and durability, but there's a small risk that checkpoints might not be written if the process crashes during execution.\n\n### `\"sync\"`\n\nChanges are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\n\nYou can specify the durability mode when calling any graph execution method:\n\n```python\ngraph.stream(\n    {\"input\": \"test\"}, \n    durability=\"sync\"\n)\n```\n\n\n\n## Using tasks in nodes\n\nIf a [node](./low_level.md#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.\n\n=== \"Original\"\n\n    ```python hl_lines=\"16\"\n    from typing import NotRequired\n    from typing_extensions import TypedDict\n    import uuid\n\n    from langgraph.checkpoint.memory import InMemorySaver\n    from langgraph.graph import StateGraph, START, END\n    import requests\n\n    # Define a TypedDict to represent the state\n    class State(TypedDict):\n        url: str\n        result: NotRequired[str]\n\n    def call_api(state: State):\n        \"\"\"Example node that makes an API request.\"\"\"\n        result = requests.get(state['url']).text[:100]  # Side-effect\n        return {\n            \"result\": result\n        }\n\n    # Create a StateGraph builder and add a node for the call_api function\n    builder = StateGraph(State)\n    builder.add_node(\"call_api\", call_api)\n\n    # Connect the start and end nodes to the call_api node\n    builder.add_edge(START, \"call_api\")\n    builder.add_edge(\"call_api\", END)\n\n    # Specify a checkpointer\n    checkpointer = InMemorySaver()\n\n    # Compile the graph with the checkpointer\n    graph = builder.compile(checkpointer=checkpointer)\n\n    # Define a config with a thread ID.\n    thread_id = uuid.uuid4()\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n\n    # Invoke the graph\n    graph.invoke({\"url\": \"https://www.example.com\"}, config)\n    ```\n\n=== \"With task\"\n\n    ```python hl_lines=\"19 23\"\n    from typing import NotRequired\n    from typing_extensions import TypedDict\n    import uuid\n\n    from langgraph.checkpoint.memory import InMemorySaver\n    from langgraph.func import task\n    from langgraph.graph import StateGraph, START, END\n    import requests\n\n    # Define a TypedDict to represent the state\n    class State(TypedDict):\n        urls: list[str]\n        result: NotRequired[list[str]]\n\n\n    @task\n    def _make_request(url: str):\n        \"\"\"Make a request.\"\"\"\n        return requests.get(url).text[:100]\n\n    def call_api(state: State):\n        \"\"\"Example node that makes an API request.\"\"\"\n        requests = [_make_request(url) for url in state['urls']]\n        results = [request.result() for request in requests]\n        return {\n            \"results\": results\n        }\n\n    # Create a StateGraph builder and add a node for the call_api function\n    builder = StateGraph(State)\n    builder.add_node(\"call_api\", call_api)\n\n    # Connect the start and end nodes to the call_api node\n    builder.add_edge(START, \"call_api\")\n    builder.add_edge(\"call_api\", END)\n\n    # Specify a checkpointer\n    checkpointer = InMemorySaver()\n\n    # Compile the graph with the checkpointer\n    graph = builder.compile(checkpointer=checkpointer)\n\n    # Define a config with a thread ID.\n    thread_id = uuid.uuid4()\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n\n    # Invoke the graph\n    graph.invoke({\"urls\": [\"https://www.example.com\"]}, config)\n    ```\n\n\n\n\n\n## Resuming Workflows\n\nOnce you have enabled durable execution in your workflow, you can resume execution for the following scenarios:\n\n- **Pausing and Resuming Workflows:** Use the [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Interrupt) function to pause a workflow at specific points and the [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) primitive to resume it with updated state. See [**Human-in-the-Loop**](./human_in_the_loop.md) for more details.\n- **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `None` as the input value (see this [example](../how-tos/use-functional-api.md#resuming-after-an-error) with the functional API).\n\n\n\n\n\n## Starting Points for Resuming Workflows\n\n- If you're using a [StateGraph (Graph API)](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph), the starting point is the beginning of the [**node**](./low_level.md#nodes) where execution stopped.\n- If you're making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.\n  Inside the subgraph, the starting point will be the specific [**node**](./low_level.md#nodes) where execution stopped.\n- If you're using the Functional API, the starting point is the beginning of the [**entrypoint**](./functional_api.md#entrypoint) where execution stopped.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 11487,
    "word_count": 1392
  },
  {
    "title": "LangGraph Data Plane",
    "source": "concepts/langgraph_data_plane.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph Data Plane\n\nThe term \"data plane\" is used broadly to refer to [LangGraph Servers](./langgraph_server.md) (deployments), the corresponding infrastructure for each server, and the \"listener\" application that continuously polls for updates from the [LangGraph Control Plane](./langgraph_control_plane.md).\n\n## Server Infrastructure\n\nIn addition to the [LangGraph Server](./langgraph_server.md) itself, the following infrastructure components for each server are also included in the broad definition of \"data plane\":\n\n- Postgres\n- Redis\n- Secrets store\n- Autoscalers\n\n## \"Listener\" Application\n\nThe data plane \"listener\" application periodically calls [control plane APIs](../concepts/langgraph_control_plane.md#control-plane-api) to:\n\n- Determine if new deployments should be created.\n- Determine if existing deployments should be updated (i.e. new revisions).\n- Determine if existing deployments should be deleted.\n\nIn other words, the data plane \"listener\" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.\n\n## Postgres\n\nPostgres is the persistence layer for all user, run, and long-term memory data in a LangGraph Server. This stores both checkpoints (see more info [here](./persistence.md)), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info [here](./persistence.md#memory-store)).\n\n## Redis\n\nRedis is used in each LangGraph Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.\n\n### Communication\n\nAll runs in a LangGraph Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.\n\n1. A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from Postgres by the worker.\n2. A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.\n3. A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open `/stream` request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.\n\n### Ephemeral metadata\n\nRuns in a LangGraph Server may be retried for specific failures (currently only for transient Postgres errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.\n\n## Data Plane Features\n\nThis section describes various features of the data plane.\n\n### Data Region\n\n!!! info \"Only for Cloud SaaS\"\n    Data regions are only applicable for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\n\nDeployments can be created in 2 data regions: US and EU\n\nThe data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.\n\n### Autoscaling\n\n[`Production` type](../concepts/langgraph_control_plane.md#deployment-types) deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:\n\n1. CPU utilization\n1. Memory utilization\n1. Number of pending (in progress) [runs](./assistants.md#execution)\n\nFor CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.\n\nFor number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs in 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).\n\nEach metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the largest number of containers.\n\nScale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This \"cool down\" period ensures that deployments do not scale up and down too frequently.\n\n### Static IP Addresses\n\n!!! info \"Only for Cloud SaaS\"\nStatic IP addresses are only available for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\n\nAll traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:\n\n| US             | EU             |\n| -------------- | -------------- |\n| 35.197.29.146  | 34.13.192.67   |\n| 34.145.102.123 | 34.147.105.64  |\n| 34.169.45.153  | 34.90.22.166   |\n| 34.82.222.17   | 34.147.36.213  |\n| 35.227.171.135 | 34.32.137.113  |\n| 34.169.88.30   | 34.91.238.184  |\n| 34.19.93.202   | 35.204.101.241 |\n| 34.19.34.50    | 35.204.48.32   |\n\n### Custom Postgres\n\n!!! info\nCustom Postgres instances are only available for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.\n\nA custom Postgres instance can be used instead of the [one automatically created by the control plane](./langgraph_control_plane.md#database-provisioning). Specify the [`POSTGRES_URI_CUSTOM`](../cloud/reference/env_var.md#postgres_uri_custom) environment variable to use a custom Postgres instance.\n\nMultiple deployments can share the same Postgres instance. For example, for `Deployment A`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`. `<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.\n\n### Custom Redis\n\n!!! info\nCustom Redis instances are only available for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_control_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.\n\nA custom Redis instance can be used instead of the one automatically created by the control plane. Specify the [REDIS_URI_CUSTOM](../cloud/reference/env_var.md#redis_uri_custom) environment variable to use a custom Redis instance.\n\nMultiple deployments can share the same Redis instance. For example, for `Deployment A`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/1` and for `Deployment B`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/2`. `1` and `2` are different database numbers within the same instance, but `<hostname_1>` is shared. **The same database number cannot be used for separate deployments**.\n\n### LangSmith Tracing\n\nLangGraph Server is automatically configured to send traces to LangSmith. See the table below for details with respect to each deployment option.\n\n| Cloud SaaS                               | Self-Hosted Data Plane                                      | Self-Hosted Control Plane                                          | Standalone Container                                                                         |\n| ---------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------ | -------------------------------------------------------------------------------------------- |\n| Required<br><br>Trace to LangSmith SaaS. | Optional<br><br>Disable tracing or trace to LangSmith SaaS. | Optional<br><br>Disable tracing or trace to Self-Hosted LangSmith. | Optional<br><br>Disable tracing, trace to LangSmith SaaS, or trace to Self-Hosted LangSmith. |\n\n### Telemetry\n\nLangGraph Server is automatically configured to report telemetry metadata for billing purposes. See the table below for details with respect to each deployment option.\n\n| Cloud SaaS                        | Self-Hosted Data Plane            | Self-Hosted Control Plane                                                                                                           | Standalone Container                                                                                                                |\n| --------------------------------- | --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |\n| Telemetry sent to LangSmith SaaS. | Telemetry sent to LangSmith SaaS. | Self-reported usage (audit) for air-gapped license key.<br><br>Telemetry sent to LangSmith SaaS for LangGraph Platform License Key. | Self-reported usage (audit) for air-gapped license key.<br><br>Telemetry sent to LangSmith SaaS for LangGraph Platform License Key. |\n\n### Licensing\n\nLangGraph Server is automatically configured to perform license key validation. See the table below for details with respect to each deployment option.\n\n| Cloud SaaS                                          | Self-Hosted Data Plane                              | Self-Hosted Control Plane                                                                  | Standalone Container                                                                       |\n| --------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |\n| LangSmith API Key validated against LangSmith SaaS. | LangSmith API Key validated against LangSmith SaaS. | Air-gapped license key or LangGraph Platform License Key validated against LangSmith SaaS. | Air-gapped license key or LangGraph Platform License Key validated against LangSmith SaaS. |",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 11121,
    "word_count": 1378
  },
  {
    "title": "Overview",
    "source": "concepts/why-langgraph.md",
    "content": "# Overview\n\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\n\n- **Reliability and controllability.** Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\n- **Low-level and extensible.** Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\n- **First-class streaming support.** With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\n\n## Learn LangGraph basics\n\nTo get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\n\n1. [Build a basic chatbot](../tutorials/get-started/1-build-basic-chatbot.md)\n2. [Add tools](../tutorials/get-started/2-add-tools.md)\n3. [Add memory](../tutorials/get-started/3-add-memory.md)\n4. [Add human-in-the-loop controls](../tutorials/get-started/4-human-in-the-loop.md)\n5. [Customize state](../tutorials/get-started/5-customize-state.md)\n6. [Time travel](../tutorials/get-started/6-time-travel.md)\n\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\n\n* ✅ **Answer common questions** by searching the web\n* ✅ **Maintain conversation state** across calls  \n* ✅ **Route complex queries** to a human for review  \n* ✅ **Use custom state** to control its behavior  \n* ✅ **Rewind and explore** alternative conversation paths",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 1709,
    "word_count": 206
  },
  {
    "title": "Self-Hosted Control Plane",
    "source": "concepts/langgraph_self_hosted_control_plane.md",
    "content": "# Self-Hosted Control Plane\n\nThere are two versions of the self-hosted deployment: [Self-Hosted Data Plane](./deployment_options.md#self-hosted-data-plane) and [Self-Hosted Control Plane](./deployment_options.md#self-hosted-control-plane).\n\n!!! info \"Important\"\n\n    The Self-Hosted Control Plane deployment option requires an [Enterprise](plans.md) plan.\n\n## Requirements\n\n- You use the [LangGraph CLI](./langgraph_cli.md) and/or [LangGraph Studio](./langgraph_studio.md) app to test graph locally.\n- You use `langgraph build` command to build image.\n- You have a Self-Hosted LangSmith instance deployed.\n- You are using Ingress for your LangSmith instance. All agents will be deployed as Kubernetes services behind this ingress.\n\n## Self-Hosted Control Plane\n\nThe [Self-Hosted Control Plane](./langgraph_self_hosted_control_plane.md) deployment option is a fully self-hosted model for deployment where you manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in your cloud. This option gives you full control and responsibility of the control plane and data plane infrastructure.\n\n|                                    | [Control plane](../concepts/langgraph_control_plane.md)                                                                                     | [Data plane](../concepts/langgraph_data_plane.md)                                                                                                   |\n| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **What is it?**                    | <ul><li>Control plane UI for creating deployments and revisions</li><li>Control plane APIs for creating deployments and revisions</li></ul> | <ul><li>Data plane \"listener\" for reconciling deployments with control plane state</li><li>LangGraph Servers</li><li>Postgres, Redis, etc</li></ul> |\n| **Where is it hosted?**            | Your cloud                                                                                                                                  | Your cloud                                                                                                                                          |\n| **Who provisions and manages it?** | You                                                                                                                                         | You                                                                                                                                                 |\n\n### Architecture\n\n![Self-Hosted Control Plane Architecture](./img/self_hosted_control_plane_architecture.png)\n\n### Compute Platforms\n\n- **Kubernetes**: The Self-Hosted Control Plane deployment option supports deploying control plane and data plane infrastructure to any Kubernetes cluster.\n\n!!! tip\nIf you would like to enable this on your LangSmith instance, please follow the [Self-Hosted Control Plane deployment guide](../cloud/deployment/self_hosted_control_plane.md).",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 3260,
    "word_count": 253
  },
  {
    "title": "LangGraph CLI",
    "source": "concepts/langgraph_cli.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# LangGraph CLI\n\n**LangGraph CLI** is a multi-platform command-line tool for building and running the [LangGraph API server](./langgraph_server.md) locally. The resulting server includes all API endpoints for your graph's runs, threads, assistants, etc. as well as the other services required to run your agent, including a managed database for checkpointing and storage.\n\n## Installation\n\nThe LangGraph CLI can be installed via pip or [Homebrew](https://brew.sh/):\n\n=== \"pip\"\n\n    ```bash\n    pip install langgraph-cli\n    ```\n\n=== \"Homebrew\"\n\n    ```bash\n    brew install langgraph-cli\n    ```\n\n\n\n\n## Commands\n\nLangGraph CLI provides the following core functionality:\n\n| Command                                                        | Description                                                                                                                                                                                                                                                                            |\n| -------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`langgraph build`](../cloud/reference/cli.md#build)           | Builds a Docker image for the [LangGraph API server](./langgraph_server.md) that can be directly deployed.                                                                                                                                                                             |\n| [`langgraph dev`](../cloud/reference/cli.md#dev)               | Starts a lightweight development server that requires no Docker installation. This server is ideal for rapid development and testing.                                                                                                                                                  |\n| [`langgraph dockerfile`](../cloud/reference/cli.md#dockerfile) | Generates a [Dockerfile](https://docs.docker.com/reference/dockerfile/) that can be used to build images for and deploy instances of the [LangGraph API server](./langgraph_server.md). This is useful if you want to further customize the dockerfile or deploy in a more custom way. |\n| [`langgraph up`](../cloud/reference/cli.md#up)                 | Starts an instance of the [LangGraph API server](./langgraph_server.md) locally in a docker container. This requires the docker server to be running locally. It also requires a LangSmith API key for local development or a license key for production use.                          |\n\nFor more information, see the [LangGraph CLI Reference](../cloud/reference/cli.md).",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 2871,
    "word_count": 239
  },
  {
    "title": "Persistence",
    "source": "concepts/persistence.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Persistence\n\nLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.\n\n![Checkpoints](img/persistence/checkpoints.jpg)\n\n!!! info \"LangGraph API handles checkpointing automatically\"\n\n    When using the LangGraph API, you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.\n\n## Threads\n\nA thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of [runs](./assistants.md#execution). When a run is executed, the [state](../concepts/low_level.md#state) of the underlying graph of the assistant will be persisted to the thread.\n\nWhen invoking a graph with a checkpointer, you **must** specify a `thread_id` as part of the `configurable` portion of the config:\n\n```python\n{\"configurable\": {\"thread_id\": \"1\"}}\n```\n\n\n\n\n\nA thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangGraph Platform API provides several endpoints for creating and managing threads and thread state. See the [API reference](../cloud/reference/api/api_ref.html#tag/threads) for more details.\n\n## Checkpoints\n\nThe state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:\n\n- `config`: Config associated with this checkpoint.\n- `metadata`: Metadata associated with this checkpoint.\n- `values`: Values of the state channels at this point in time.\n- `next` A tuple of the node names to execute next in the graph.\n- `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](../how-tos/human_in_the_loop/add-human-in-the-loop.md#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts.\n\nCheckpoints are persisted and can be used to restore the state of a thread at a later time.\n\nLet's see what checkpoints are saved when a simple graph is invoked as follows:\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\"}, config)\n```\n\n\n\n\n\n\n\nAfter we run the graph, we expect to see exactly 4 checkpoints:\n\n- empty checkpoint with `START` as the next node to be executed\n- checkpoint with the user input `{'foo': '', 'bar': []}` and `node_a` as the next node to be executed\n- checkpoint with the outputs of `node_a` `{'foo': 'a', 'bar': ['a']}` and `node_b` as the next node to be executed\n- checkpoint with the outputs of `node_b` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed\n\nNote that we `bar` channel values contain outputs from both nodes as we have a reducer for `bar` channel.\n\n\n\n\n\n### Get state\n\nWhen interacting with the saved graph state, you **must** specify a [thread identifier](#threads). You can view the _latest_ state of the graph by calling `graph.get_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.\n\n```python\n# get the latest state snapshot\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.get_state(config)\n\n# get a state snapshot for a specific checkpoint_id\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\ngraph.get_state(config)\n```\n\n\n\n\n\nIn our example, the output of `get_state` will look like this:\n\n```\nStateSnapshot(\n    values={'foo': 'b', 'bar': ['a', 'b']},\n    next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n    created_at='2024-08-29T19:19:38.821749+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n)\n```\n\n\n\n\n\n### Get state history\n\nYou can get the full history of the graph execution for a given thread by calling `graph.get_state_history(config)`. This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list.\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\nlist(graph.get_state_history(config))\n```\n\n\n\n\n\nIn our example, the output of `get_state_history` will look like this:\n\n```\n[\n    StateSnapshot(\n        values={'foo': 'b', 'bar': ['a', 'b']},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n        created_at='2024-08-29T19:19:38.821749+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        tasks=(),\n    ),\n    StateSnapshot(\n        values={'foo': 'a', 'bar': ['a']},\n        next=('node_b',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\n        created_at='2024-08-29T19:19:38.819946+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'foo': '', 'bar': []},\n        next=('node_a',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 0},\n        created_at='2024-08-29T19:19:38.817813+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'bar': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\n        created_at='2024-08-29T19:19:38.816205+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\n    )\n]\n```\n\n\n\n\n\n![State](img/persistence/get_state.jpg)\n\n### Replay\n\nIt's also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread_id` and a `checkpoint_id`, then we will _re-play_ the previously executed steps _before_ a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps _after_ the checkpoint.\n\n- `thread_id` is the ID of a thread.\n- `checkpoint_id` is an identifier that refers to a specific checkpoint within a thread.\n\nYou must pass these when invoking the graph as part of the `configurable` portion of the config:\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\ngraph.invoke(None, config=config)\n```\n\n\n\n\n\nImportantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply _re-plays_ that particular step in the graph and does not re-execute the step, but only for the steps _before_ the provided `checkpoint_id`. All of the steps _after_ `checkpoint_id` will be executed (i.e., a new fork), even if they have been executed previously. See this [how to guide on time-travel to learn more about replaying](../how-tos/human_in_the_loop/time-travel.md).\n\n![Replay](img/persistence/re_play.png)\n\n### Update state\n\nIn addition to re-playing the graph from specific `checkpoints`, we can also _edit_ the graph state. We do this using `graph.update_state()`. This method accepts three different arguments:\n\n\n\n\n\n#### `config`\n\nThe config should contain `thread_id` specifying which thread to update. When only the `thread_id` is passed, we update (or fork) the current state. Optionally, if we include `checkpoint_id` field, then we fork that selected checkpoint.\n\n#### `values`\n\nThese are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the [reducer](./low_level.md#reducers) functions, if they are defined for some of the channels in the graph state. This means that `update_state` does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let's walk through an example.\n\nLet's assume you have defined the state of your graph with the following schema (see full example above):\n\n```python\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n```\n\n\n\n\n\nLet's now assume the current state of the graph is\n\n```\n{\"foo\": 1, \"bar\": [\"a\"]}\n```\n\n\n\n\n\nIf you update the state as below:\n\n```python\ngraph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n```\n\n\n\n\n\nThen the new state of the graph will be:\n\n```\n{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\n```\n\nThe `foo` key (channel) is completely changed (because there is no reducer specified for that channel, so `update_state` overwrites it). However, there is a reducer specified for the `bar` key, and so it appends `\"b\"` to the state of `bar`.\n\n\n\n\n#### `as_node`\n\nThe final thing you can optionally specify when calling `update_state` is `as_node`. If you provided it, the update will be applied as if it came from node `as_node`. If `as_node` is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this [how to guide on time-travel to learn more about forking state](../how-tos/human_in_the_loop/time-travel.md).\n\n\n\n\n![Update](img/persistence/checkpoints_full_story.jpg)\n\n## Memory Store\n\n![Model of shared state](img/persistence/shared_state.png)\n\nA [state schema](low_level.md#schema) specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.\n\nBut, what if we want to retain some information _across threads_? Consider the case of a chatbot where we want to retain specific information about the user across _all_ chat conversations (e.g., threads) with that user!\n\nWith checkpointers alone, we cannot share information across threads. This motivates the need for the [`Store`](../reference/store.md#langgraph.store.base.BaseStore) interface. As an illustration, we can define an `InMemoryStore` to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new `in_memory_store` variable.\n\n!!! info \"LangGraph API handles stores automatically\"\n\n    When using the LangGraph API, you don't need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\n\n### Basic Usage\n\nFirst, let's showcase this in isolation without using LangGraph.\n\n```python\nfrom langgraph.store.memory import InMemoryStore\nin_memory_store = InMemoryStore()\n```\n\n\n\n\n\nMemories are namespaced by a `tuple`, which in this specific example will be `(<user_id>, \"memories\")`. The namespace can be any length and represent anything, does not have to be user specific.\n\n```python\nuser_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\n```\n\n\n\n\n\nWe use the `store.put` method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (`memory_id`) and the value (a dictionary) is the memory itself.\n\n```python\nmemory_id = str(uuid.uuid4())\nmemory = {\"food_preference\" : \"I like pizza\"}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\n```\n\n\n\n\n\nWe can read out memories in our namespace using the `store.search` method, which will return all memories for a given user as a list. The most recent memory is the last in the list.\n\n```python\nmemories = in_memory_store.search(namespace_for_memory)\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n```\n\nEach memory type is a Python class ([`Item`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.Item)) with certain attributes. We can access it as a dictionary by converting via `.dict` as above.\n\nThe attributes it has are:\n\n- `value`: The value (itself a dictionary) of this memory\n- `key`: A unique key for this memory in this namespace\n- `namespace`: A list of strings, the namespace of this memory type\n- `created_at`: Timestamp for when this memory was created\n- `updated_at`: Timestamp for when this memory was updated\n\n\n\n\n\n### Semantic Search\n\nBeyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:\n\n```python\nfrom langchain.embeddings import init_embeddings\n\nstore = InMemoryStore(\n    index={\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n        \"dims\": 1536,                              # Embedding dimensions\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n    }\n)\n```\n\n\n\n\n\nNow when searching, you can use natural language queries to find relevant memories:\n\n```python\n# Find memories about food preferences\n# (This can be done after putting memories into the store)\nmemories = store.search(\n    namespace_for_memory,\n    query=\"What does the user like to eat?\",\n    limit=3  # Return top 3 matches\n)\n```\n\n\n\n\n\nYou can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories:\n\n```python\n# Store with specific fields to embed\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\n        \"food_preference\": \"I love Italian cuisine\",\n        \"context\": \"Discussing dinner plans\"\n    },\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n)\n\n# Store without embedding (still retrievable, but not searchable)\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\"system_info\": \"Last updated: 2024-01-01\"},\n    index=False\n)\n```\n\n\n\n\n\n### Using in LangGraph\n\nWith this all in place, we use the `in_memory_store` in LangGraph. The `in_memory_store` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `in_memory_store` allows us to store arbitrary information for access _across_ threads. We compile the graph with both the checkpointer and the `in_memory_store` as follows.\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# We need this because we want to enable threads (conversations)\ncheckpointer = InMemorySaver()\n\n# ... Define the graph ...\n\n# Compile the graph with the checkpointer and store\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n```\n\n\n\n\n\nWe invoke the graph with a `thread_id`, as before, and also with a `user_id`, which we'll use to namespace our memories to this particular user as we showed above.\n\n```python\n# Invoke the graph\nuser_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n\n# First let's just say hi to the AI\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n```\n\n\n\n\n\nWe can access the `in_memory_store` and the `user_id` in _any node_ by passing `store: BaseStore` and `config: RunnableConfig` as node arguments. Here's how we might use semantic search in a node to find relevant memories:\n\n```python\ndef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # ... Analyze conversation and create a new memory\n\n    # Create a new memory ID\n    memory_id = str(uuid.uuid4())\n\n    # We create a new memory\n    store.put(namespace, memory_id, {\"memory\": memory})\n\n```\n\n\n\n\n\nAs we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.\n\n```python\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n```\n\n\n\n\n\nWe can access the memories and use them in our model call.\n\n```python\ndef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # Search based on the most recent message\n    memories = store.search(\n        namespace,\n        query=state[\"messages\"][-1].content,\n        limit=3\n    )\n    info = \"\\n\".join([d.value[\"memory\"] for d in memories])\n\n    # ... Use memories in the model call\n```\n\n\n\n\n\nIf we create a new thread, we can still access the same memories so long as the `user_id` is the same.\n\n```python\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n\n# Let's say hi again\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n```\n\n\n\n\n\nWhen we use the LangGraph Platform, either locally (e.g., in LangGraph Studio) or with LangGraph Platform, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you **do** need to configure the indexing settings in your `langgraph.json` file. For example:\n\n```json\n{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"openai:text-embeddings-3-small\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n```\n\nSee the [deployment guide](../cloud/deployment/semantic_search.md) for more details and configuration options.\n\n## Checkpointer libraries\n\nUnder the hood, checkpointing is powered by checkpointer objects that conform to [BaseCheckpointSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:\n\n- `langgraph-checkpoint`: The base interface for checkpointer savers ([BaseCheckpointSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)) and serialization/deserialization interface ([SerializerProtocol](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)). Includes in-memory checkpointer implementation ([InMemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)) for experimentation. LangGraph comes with `langgraph-checkpoint` included.\n- `langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ([SqliteSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [AsyncSqliteSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)). Ideal for experimentation and local workflows. Needs to be installed separately.\n- `langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ([PostgresSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [AsyncPostgresSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)), used in LangGraph Platform. Ideal for using in production. Needs to be installed separately.\n\n\n\n\n\n### Checkpointer interface\n\nEach checkpointer conforms to [BaseCheckpointSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface and implements the following methods:\n\n- `.put` - Store a checkpoint with its configuration and metadata.\n- `.put_writes` - Store intermediate writes linked to a checkpoint (i.e. [pending writes](#pending-writes)).\n- `.get_tuple` - Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.get_state()`.\n- `.list` - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.get_state_history()`\n\nIf the checkpointer is used with asynchronous graph execution (i.e. executing the graph via `.ainvoke`, `.astream`, `.abatch`), asynchronous versions of the above methods will be used (`.aput`, `.aput_writes`, `.aget_tuple`, `.alist`).\n\n!!! note\n\n    For running your graph asynchronously, you can use `InMemorySaver`, or async versions of Sqlite/Postgres checkpointers -- `AsyncSqliteSaver` / `AsyncPostgresSaver` checkpointers.\n\n\n\n\n\n### Serializer\n\nWhen checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.\n\n`langgraph_checkpoint` defines [protocol](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol) for implementing serializers provides a default implementation ([JsonPlusSerializer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.\n\n#### Serialization with `pickle`\n\nThe default serializer, [`JsonPlusSerializer`](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer), uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.\n\nIf you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),\nyou can use the `pickle_fallback` argument of the `JsonPlusSerializer`:\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n\n# ... Define the graph ...\ngraph.compile(\n    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\n)\n```\n\n#### Encryption\n\nCheckpointers can optionally encrypt all persisted state. To enable this, pass an instance of [`EncryptedSerializer`](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer) to the `serde` argument of any `BaseCheckpointSaver` implementation. The easiest way to create an encrypted serializer is via [`from_pycryptodome_aes`](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes), which reads the AES key from the `LANGGRAPH_AES_KEY` environment variable (or accepts a `key` argument):\n\n```python\nimport sqlite3\n\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\n```\n\n```python\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\ncheckpointer.setup()\n```\n\nWhen running on LangGraph Platform, encryption is automatically enabled whenever `LANGGRAPH_AES_KEY` is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing [`CipherProtocol`](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol) and supplying it to `EncryptedSerializer`.\n\n\n\n\n## Capabilities\n\n### Human-in-the-loop\n\nFirst, checkpointers facilitate [human-in-the-loop workflows](agentic_concepts.md#human-in-the-loop) workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See [the how-to guides](../how-tos/human_in_the_loop/add-human-in-the-loop.md) for examples.\n\n### Memory\n\nSecond, checkpointers allow for [\"memory\"](../concepts/memory.md) between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See [Add memory](../how-tos/memory/add-memory.md) for information on how to add and manage conversation memory using checkpointers.\n\n### Time Travel\n\nThird, checkpointers allow for [\"time travel\"](time-travel.md), allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.\n\n### Fault-tolerance\n\nLastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.\n\n#### Pending writes\n\nAdditionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 28587,
    "word_count": 3344
  },
  {
    "title": "Application Structure",
    "source": "concepts/application_structure.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Application Structure\n\n## Overview\n\nA LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.\n\nThis guide shows a typical structure of an application and shows how the required information to deploy an application using the LangGraph Platform is specified.\n\n## Key Concepts\n\nTo deploy using the LangGraph Platform, the following information should be provided:\n\n1. A [LangGraph configuration file](#configuration-file-concepts) (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.\n2. The [graphs](#graphs) that implement the logic of the application.\n3. A file that specifies [dependencies](#dependencies) required to run the application.\n4. [Environment variables](#environment-variables) that are required for the application to run.\n\n## File Structure\n\nBelow are examples of directory structures for applications:\n\n=== \"Python (requirements.txt)\"\n\n    ```plaintext\n    my-app/\n    ├── my_agent # all project code lies within here\n    │   ├── utils # utilities for your graph\n    │   │   ├── __init__.py\n    │   │   ├── tools.py # tools for your graph\n    │   │   ├── nodes.py # node functions for your graph\n    │   │   └── state.py # state definition of your graph\n    │   ├── __init__.py\n    │   └── agent.py # code for constructing your graph\n    ├── .env # environment variables\n    ├── requirements.txt # package dependencies\n    └── langgraph.json # configuration file for LangGraph\n    ```\n\n=== \"Python (pyproject.toml)\"\n\n    ```plaintext\n    my-app/\n    ├── my_agent # all project code lies within here\n    │   ├── utils # utilities for your graph\n    │   │   ├── __init__.py\n    │   │   ├── tools.py # tools for your graph\n    │   │   ├── nodes.py # node functions for your graph\n    │   │   └── state.py # state definition of your graph\n    │   ├── __init__.py\n    │   └── agent.py # code for constructing your graph\n    ├── .env # environment variables\n    ├── langgraph.json  # configuration file for LangGraph\n    └── pyproject.toml # dependencies for your project\n    ```\n\n\n\n\n\n!!! note\n\n    The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\n\n## Configuration File {#configuration-file-concepts}\n\nThe `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\n\nSee the [LangGraph configuration file reference](../cloud/reference/cli.md#configuration-file) for details on all supported keys in the JSON file.\n\n!!! tip\n\n    The [LangGraph CLI](./langgraph_cli.md) defaults to using the configuration file `langgraph.json` in the current directory.\n\n### Examples\n\n- The dependencies involve a custom local package and the `langchain_openai` package.\n- A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.\n- The environment variables are loaded from the `.env` file.\n\n```json\n{\n  \"dependencies\": [\"langchain_openai\", \"./your_package\"],\n  \"graphs\": {\n    \"my_agent\": \"./your_package/your_file.py:agent\"\n  },\n  \"env\": \"./.env\"\n}\n```\n\n\n\n\n\n## Dependencies\n\nA LangGraph application may depend on other Python packages.\n\n\n\n\nYou will generally need to specify the following information for dependencies to be set up correctly:\n\n1. A file in the directory that specifies the dependencies (e.g. `requirements.txt`, `pyproject.toml`, or `package.json`).\n\n\n\n\n2. A `dependencies` key in the [LangGraph configuration file](#configuration-file-concepts) that specifies the dependencies required to run the LangGraph application.\n3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the [LangGraph configuration file](#configuration-file-concepts).\n\n## Graphs\n\nUse the `graphs` key in the [LangGraph configuration file](#configuration-file-concepts) to specify which graphs will be available in the deployed LangGraph application.\n\nYou can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\n\n## Environment Variables\n\nIf you're working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the [LangGraph configuration file](#configuration-file-concepts).\n\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 4696,
    "word_count": 630
  },
  {
    "title": "Time Travel ⏱️",
    "source": "concepts/time-travel.md",
    "content": "---\nsearch:\n  boost: 2\n---\n\n# Time Travel ⏱️\n\nWhen working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:\n\n1. 🤔 **Understand reasoning**: Analyze the steps that led to a successful result.\n2. 🐞 **Debug mistakes**: Identify where and why errors occurred.\n3. 🔍 **Explore alternatives**: Test different paths to uncover better solutions.\n\nLangGraph provides [time travel functionality](../how-tos/human_in_the_loop/time-travel.md) to support these use cases. Specifically, you can resume execution from a prior checkpoint — either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.\n\n!!! tip\n\n    For information on how to use time travel, see [Use time travel](../how-tos/human_in_the_loop/time-travel.md) and [Time travel using Server API](../cloud/how-tos/human_in_the_loop_time_travel.md).",
    "source_file": "langgraph",
    "category": "concepts",
    "char_count": 995,
    "word_count": 132
  },
  {
    "title": "Chat models",
    "source": "https://docs.langchain.com/oss/python/integrations/chat/index",
    "content": "[Chat models](/oss/python/langchain/models) are language models that use a sequence of [messages](/oss/python/langchain/messages) as inputs and return messages as outputs <Tooltip tip=\"Older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models typically do not include the prefix 'Chat' in their name or include 'LLM' as a suffix.\">(as opposed to traditional, plaintext LLMs)</Tooltip>.\n\n## Featured models\n\n<Info>\n  **While these LangChain classes support the indicated advanced feature**, you may need to refer to provider-specific documentation to learn which hosted models or backends support the feature.\n</Info>\n\n| Model                                                                          | [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output/) | JSON mode | Local | [Multimodal](/oss/python/langchain/messages#multimodal) |\n| ------------------------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------- | --------- | ----- | ------------------------------------------------------- |\n| [`ChatAnthropic`](/oss/python/integrations/chat/anthropic)                     | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |\n| [`ChatOpenAI`](/oss/python/integrations/chat/openai)                           | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |\n| [`AzureChatOpenAI`](/oss/python/integrations/chat/azure_chat_openai)           | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |\n| [`ChatVertexAI`](/oss/python/integrations/chat/google_vertex_ai)               | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |\n| [`ChatGoogleGenerativeAI`](/oss/python/integrations/chat/google_generative_ai) | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |\n| [`ChatGroq`](/oss/python/integrations/chat/groq)                               | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       |\n| [`ChatBedrock`](/oss/python/integrations/chat/bedrock)                         | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |\n| [`ChatHuggingFace`](/oss/python/integrations/chat/huggingface)                 | ✅                                           | ✅                                                             | ❌         | ✅     | ❌                                                       |\n| [`ChatOllama`](/oss/python/integrations/chat/ollama)                           | ✅                                           | ✅                                                             | ✅         | ✅     | ❌                                                       |\n| [`ChatWatsonx`](/oss/python/integrations/chat/ibm_watsonx)                     | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |\n| [`ChatXAI`](/oss/python/integrations/chat/xai)                                 | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |\n| [`ChatNVIDIA`](/oss/python/integrations/chat/nvidia_ai_endpoints)              | ✅                                           | ✅                                                             | ✅         | ✅     | ✅                                                       |\n| [`ChatCohere`](/oss/python/integrations/chat/cohere)                           | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |\n| [`ChatMistralAI`](/oss/python/integrations/chat/mistralai)                     | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |\n| [`ChatTogether`](/oss/python/integrations/chat/together)                       | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       |\n| [`ChatFireworks`](/oss/python/integrations/chat/fireworks)                     | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       |\n| [`ChatLlamaCpp`](/oss/python/integrations/chat/llamacpp)                       | ✅                                           | ✅                                                             | ❌         | ✅     | ❌                                                       |\n| [`ChatDatabricks`](/oss/python/integrations/chat/databricks)                   | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |\n| [`ChatPerplexity`](/oss/python/integrations/chat/perplexity)                   | ❌                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |\n\n## Chat Completions API\n\nCertain model providers offer endpoints that are compatible with OpenAI's (legacy) [Chat Completions API](https://platform.openai.com/docs/guides/completions). In such case, you can use [`ChatOpenAI`](/oss/python/integrations/chat/openai) with a custom `base_url` to connect to these endpoints. Note that features built on top of the Chat Completions API may not be fully supported by `ChatOpenAI`; in such cases, consider using a provider-specific class if available (e.g. [`ChatLiteLLM`](https://github.com/Akshay-Dongare/langchain-litellm/) (community-maintained) for [LiteLLM](https://litellm.ai/)).\n\n<Accordion title=\"Example: OpenRouter\">\n  To use OpenRouter, you will need to sign up for an account and obtain an [API key](https://openrouter.ai/docs/api-reference/authentication).\n\n  ```python  theme={null}\n  from langchain_openai import ChatOpenAI\n\n  model = ChatOpenAI(\n      model=\"...\",  # Specify a model available on OpenRouter\n      api_key=\"OPENROUTER_API_KEY\",\n      base_url=\"https://openrouter.ai/api/v1\",\n  )\n  ```\n\n  Refer to the [OpenRouter documentation](https://openrouter.ai/docs/quickstart) for more details.\n\n  <Note>\n    To capture [reasoning tokens](https://openrouter.ai/docs/use-cases/reasoning-tokens),\n\n    1. Switch imports from `langchain_openai` to `langchain_deepseek`\n    2. Use `ChatDeepSeek` instead of `ChatOpenAI`. You will need to change param `base_url` to `api_base`.\n    3. Adjust reasoning parameters as needed under `extra_body`, e.g.:\n\n    ```python  theme={null}\n    model = ChatDeepSeek(\n        model=\"...\",\n        api_key=\"...\",\n        api_base=\"https://openrouter.ai/api/v1\",\n        extra_body={\"reasoning\": {\"enabled\": True}},\n    )\n    ```\n\n    This is a known limitation with `ChatOpenAI` and will be addressed in a future release.\n  </Note>\n</Accordion>\n\n## All chat models\n\n<Columns cols={3}>\n  <Card title=\"Abso\" icon=\"link\" href=\"/oss/python/integrations/chat/abso\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AI21 Labs\" icon=\"link\" href=\"/oss/python/integrations/chat/ai21\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AI/ML API\" icon=\"link\" href=\"/oss/python/integrations/chat/aimlapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Alibaba Cloud PAI EAS\" icon=\"link\" href=\"/oss/python/integrations/chat/alibaba_cloud_pai_eas\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Anthropic\" icon=\"link\" href=\"/oss/python/integrations/chat/anthropic\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AzureAIChatCompletionsModel\" icon=\"link\" href=\"/oss/python/integrations/chat/azure_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure OpenAI\" icon=\"link\" href=\"/oss/python/integrations/chat/azure_chat_openai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure ML Endpoint\" icon=\"link\" href=\"/oss/python/integrations/chat/azureml_chat_endpoint\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baichuan Chat\" icon=\"link\" href=\"/oss/python/integrations/chat/baichuan\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baidu Qianfan\" icon=\"link\" href=\"/oss/python/integrations/chat/baidu_qianfan_endpoint\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baseten\" icon=\"link\" href=\"/oss/python/integrations/chat/baseten\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AWS Bedrock\" icon=\"link\" href=\"/oss/python/integrations/chat/bedrock\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cerebras\" icon=\"link\" href=\"/oss/python/integrations/chat/cerebras\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"CloudflareWorkersAI\" icon=\"link\" href=\"/oss/python/integrations/chat/cloudflare_workersai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cohere\" icon=\"link\" href=\"/oss/python/integrations/chat/cohere\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ContextualAI\" icon=\"link\" href=\"/oss/python/integrations/chat/contextual\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Coze Chat\" icon=\"link\" href=\"/oss/python/integrations/chat/coze\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dappier AI\" icon=\"link\" href=\"/oss/python/integrations/chat/dappier\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Databricks\" icon=\"link\" href=\"/oss/python/integrations/chat/databricks\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DeepInfra\" icon=\"link\" href=\"/oss/python/integrations/chat/deepinfra\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DeepSeek\" icon=\"link\" href=\"/oss/python/integrations/chat/deepseek\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Eden AI\" icon=\"link\" href=\"/oss/python/integrations/chat/edenai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"EverlyAI\" icon=\"link\" href=\"/oss/python/integrations/chat/everlyai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Featherless AI\" icon=\"link\" href=\"/oss/python/integrations/chat/featherless_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Fireworks\" icon=\"link\" href=\"/oss/python/integrations/chat/fireworks\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatFriendli\" icon=\"link\" href=\"/oss/python/integrations/chat/friendli\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Gemini\" icon=\"link\" href=\"/oss/python/integrations/chat/google_generative_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud Vertex AI\" icon=\"link\" href=\"/oss/python/integrations/chat/google_vertex_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GPTRouter\" icon=\"link\" href=\"/oss/python/integrations/chat/gpt_router\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DigitalOcean Gradient\" icon=\"link\" href=\"/oss/python/integrations/chat/gradientai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GreenNode\" icon=\"link\" href=\"/oss/python/integrations/chat/greennode\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Groq\" icon=\"link\" href=\"/oss/python/integrations/chat/groq\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatHuggingFace\" icon=\"link\" href=\"/oss/python/integrations/chat/huggingface\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IBM watsonx.ai\" icon=\"link\" href=\"/oss/python/integrations/chat/ibm_watsonx\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JinaChat\" icon=\"link\" href=\"/oss/python/integrations/chat/jinachat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Kinetica\" icon=\"link\" href=\"/oss/python/integrations/chat/kinetica\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Konko\" icon=\"link\" href=\"/oss/python/integrations/chat/konko\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LiteLLM\" icon=\"link\" href=\"/oss/python/integrations/chat/litellm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Llama 2 Chat\" icon=\"link\" href=\"/oss/python/integrations/chat/llama2_chat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Llama API\" icon=\"link\" href=\"/oss/python/integrations/chat/llama_api\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LlamaEdge\" icon=\"link\" href=\"/oss/python/integrations/chat/llama_edge\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Llama.cpp\" icon=\"link\" href=\"/oss/python/integrations/chat/llamacpp\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"maritalk\" icon=\"link\" href=\"/oss/python/integrations/chat/maritalk\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MiniMax\" icon=\"link\" href=\"/oss/python/integrations/chat/minimax\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MistralAI\" icon=\"link\" href=\"/oss/python/integrations/chat/mistralai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MLX\" icon=\"link\" href=\"/oss/python/integrations/chat/mlx\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ModelScope\" icon=\"link\" href=\"/oss/python/integrations/chat/modelscope_chat_endpoint\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Moonshot\" icon=\"link\" href=\"/oss/python/integrations/chat/moonshot\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Naver\" icon=\"link\" href=\"/oss/python/integrations/chat/naver\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nebius\" icon=\"link\" href=\"/oss/python/integrations/chat/nebius\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Netmind\" icon=\"link\" href=\"/oss/python/integrations/chat/netmind\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NVIDIA AI Endpoints\" icon=\"link\" href=\"/oss/python/integrations/chat/nvidia_ai_endpoints\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatOCIModelDeployment\" icon=\"link\" href=\"/oss/python/integrations/chat/oci_data_science\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OCIGenAI\" icon=\"link\" href=\"/oss/python/integrations/chat/oci_generative_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatOctoAI\" icon=\"link\" href=\"/oss/python/integrations/chat/octoai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Ollama\" icon=\"link\" href=\"/oss/python/integrations/chat/ollama\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenAI\" icon=\"link\" href=\"/oss/python/integrations/chat/openai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Outlines\" icon=\"link\" href=\"/oss/python/integrations/chat/outlines\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Perplexity\" icon=\"link\" href=\"/oss/python/integrations/chat/perplexity\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pipeshift\" icon=\"link\" href=\"/oss/python/integrations/chat/pipeshift\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatPredictionGuard\" icon=\"link\" href=\"/oss/python/integrations/chat/predictionguard\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PremAI\" icon=\"link\" href=\"/oss/python/integrations/chat/premai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PromptLayer ChatOpenAI\" icon=\"link\" href=\"/oss/python/integrations/chat/promptlayer_chatopenai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Qwen QwQ\" icon=\"link\" href=\"/oss/python/integrations/chat/qwq\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Qwen\" icon=\"link\" href=\"/oss/python/integrations/chat/qwen\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Reka\" icon=\"link\" href=\"/oss/python/integrations/chat/reka\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RunPod Chat Model\" icon=\"link\" href=\"/oss/python/integrations/chat/runpod\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SambaNova\" icon=\"link\" href=\"/oss/python/integrations/chat/sambanova\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatSeekrFlow\" icon=\"link\" href=\"/oss/python/integrations/chat/seekrflow\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Snowflake Cortex\" icon=\"link\" href=\"/oss/python/integrations/chat/snowflake\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SparkLLM Chat\" icon=\"link\" href=\"/oss/python/integrations/chat/sparkllm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nebula (Symbl.ai)\" icon=\"link\" href=\"/oss/python/integrations/chat/symblai_nebula\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tencent Hunyuan\" icon=\"link\" href=\"/oss/python/integrations/chat/tencent_hunyuan\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Together\" icon=\"link\" href=\"/oss/python/integrations/chat/together\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tongyi Qwen\" icon=\"link\" href=\"/oss/python/integrations/chat/tongyi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Upstage\" icon=\"link\" href=\"/oss/python/integrations/chat/upstage\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"vLLM Chat\" icon=\"link\" href=\"/oss/python/integrations/chat/vllm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Volc Engine Maas\" icon=\"link\" href=\"/oss/python/integrations/chat/volcengine_maas\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatWriter\" icon=\"link\" href=\"/oss/python/integrations/chat/writer\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"xAI\" icon=\"link\" href=\"/oss/python/integrations/chat/xai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Xinference\" icon=\"link\" href=\"/oss/python/integrations/chat/xinference\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"YandexGPT\" icon=\"link\" href=\"/oss/python/integrations/chat/yandex\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatYI\" icon=\"link\" href=\"/oss/python/integrations/chat/yi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Yuan2.0\" icon=\"link\" href=\"/oss/python/integrations/chat/yuan2\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ZHIPU AI\" icon=\"link\" href=\"/oss/python/integrations/chat/zhipuai\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n<Info>\n  If you'd like to contribute an integration, see [Contributing integrations](/oss/python/contributing#add-a-new-integration).\n</Info>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 19087,
    "word_count": 1328
  },
  {
    "title": "null",
    "source": "https://docs.langchain.com/oss/python/integrations/document_loaders/index",
    "content": "Document loaders provide a **standard interface** for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain’s [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) format.\nThis ensures that data can be handled consistently regardless of the source.\n\nAll document loaders implement the [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader) interface.\n\n## Interface\n\nEach document loader may define its own parameters, but they share a common API:\n\n* `.load()` – Loads all documents at once.\n* `.lazy_load()` – Streams documents lazily, useful for large datasets.\n\n```python  theme={null}\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\n\nloader = CSVLoader(\n    ...  # Integration-specific parameters here\n)\n\n# Load all documents\ndocuments = loader.load()\n\n# For large datasets, lazily load documents\nfor document in loader.lazy_load():\n    print(document)\n```\n\n## By category\n\n### Webpages\n\nThe below document loaders allow you to load webpages.\n\n| Document Loader                                                             | Description                                                                                                          | Package/API |\n| --------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------- |\n| [Web](/oss/python/integrations/document_loaders/web_base)                   | Uses urllib and BeautifulSoup to load and parse HTML web pages                                                       | Package     |\n| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file) | Uses Unstructured to load and parse web pages                                                                        | Package     |\n| [RecursiveURL](/oss/python/integrations/document_loaders/recursive_url)     | Recursively scrapes all child links from a root URL                                                                  | Package     |\n| [Sitemap](/oss/python/integrations/document_loaders/sitemap)                | Scrapes all pages on a given sitemap                                                                                 | Package     |\n| [Spider](/oss/python/integrations/document_loaders/spider)                  | Crawler and scraper that returns LLM-ready data                                                                      | API         |\n| [Firecrawl](/oss/python/integrations/document_loaders/firecrawl)            | API service that can be deployed locally                                                                             | API         |\n| [Docling](/oss/python/integrations/document_loaders/docling)                | Uses Docling to load and parse web pages                                                                             | Package     |\n| [Hyperbrowser](/oss/python/integrations/document_loaders/hyperbrowser)      | Platform for running and scaling headless browsers, can be used to scrape/crawl any site                             | API         |\n| [AgentQL](/oss/python/integrations/document_loaders/agentql)                | Web interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt | API         |\n\n### PDFs\n\nThe below document loaders allow you to load PDF documents.\n\n| Document Loader                                                                    | Description                                          | Package/API |\n| ---------------------------------------------------------------------------------- | ---------------------------------------------------- | ----------- |\n| [PyPDF](/oss/python/integrations/document_loaders/pypdfloader)                     | Uses `pypdf` to load and parse PDFs                  | Package     |\n| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file)        | Uses Unstructured's open source library to load PDFs | Package     |\n| [Amazon Textract](/oss/python/integrations/document_loaders/amazon_textract)       | Uses AWS API to load PDFs                            | API         |\n| [MathPix](/oss/python/integrations/document_loaders/mathpix)                       | Uses MathPix to load PDFs                            | Package     |\n| [PDFPlumber](/oss/python/integrations/document_loaders/pdfplumber)                 | Load PDF files using PDFPlumber                      | Package     |\n| [PyPDFDirectry](/oss/python/integrations/document_loaders/pypdfdirectory)          | Load a directory with PDF files                      | Package     |\n| [PyPDFium2](/oss/python/integrations/document_loaders/pypdfium2)                   | Load PDF files using PyPDFium2                       | Package     |\n| [PyMuPDF](/oss/python/integrations/document_loaders/pymupdf)                       | Load PDF files using PyMuPDF                         | Package     |\n| [PyMuPDF4LLM](/oss/python/integrations/document_loaders/pymupdf4llm)               | Load PDF content to Markdown using PyMuPDF4LLM       | Package     |\n| [PDFMiner](/oss/python/integrations/document_loaders/pdfminer)                     | Load PDF files using PDFMiner                        | Package     |\n| [Upstage Document Parse Loader](/oss/python/integrations/document_loaders/upstage) | Load PDF files using UpstageDocumentParseLoader      | Package     |\n| [Docling](/oss/python/integrations/document_loaders/docling)                       | Load PDF files using Docling                         | Package     |\n| [UnDatasIO](/oss/python/integrations/document_loaders/undatasio)                   | Load PDF files using UnDatasIO                       | Package     |\n| [OpenDataLoader PDF](/oss/python/integrations/document_loaders/opendataloader_pdf) | Load PDF files using OpenDataLoader PDF              | Package     |\n\n### Cloud Providers\n\nThe below document loaders allow you to load documents from your favorite cloud providers.\n\n| Document Loader                                                                                            | Description                                                 | Partner Package | API reference                                                                                                                                                                                  |\n| ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [AWS S3 Directory](/oss/python/integrations/document_loaders/aws_s3_directory)                             | Load documents from an AWS S3 directory                     | ❌               | [`S3DirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_directory.S3DirectoryLoader.html)                          |\n| [AWS S3 File](/oss/python/integrations/document_loaders/aws_s3_file)                                       | Load documents from an AWS S3 file                          | ❌               | [`S3FileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html)                                         |\n| [Azure AI Data](/oss/python/integrations/document_loaders/azure_ai_data)                                   | Load documents from Azure AI services                       | ❌               | [`AzureAIDataLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.azure_ai_data.AzureAIDataLoader.html)                         |\n| [Azure Blob Storage](/oss/python/integrations/document_loaders/azure_blob_storage)                         | Load documents from Azure Blob Storage                      | ✅               | [`AzureBlobStorageLoader`](https://reference.langchain.com/python/integrations/langchain_azure/storage/)                                                                                       |\n| [Dropbox](/oss/python/integrations/document_loaders/dropbox)                                               | Load documents from Dropbox                                 | ❌               | [`DropboxLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.dropbox.DropboxLoader.html)                                       |\n| [Google Cloud Storage Directory](/oss/python/integrations/document_loaders/google_cloud_storage_directory) | Load documents from GCS bucket                              | ✅               | [`GCSDirectoryLoader`](https://python.langchain.com/api_reference/google_community/gcs_directory/langchain_google_community.gcs_directory.GCSDirectoryLoader.html)                             |\n| [Google Cloud Storage File](/oss/python/integrations/document_loaders/google_cloud_storage_file)           | Load documents from GCS file object                         | ✅               | [`GCSFileLoader`](https://python.langchain.com/api_reference/google_community/gcs_file/langchain_google_community.gcs_file.GCSFileLoader.html)                                                 |\n| [Google Drive](/oss/python/integrations/document_loaders/google_drive)                                     | Load documents from Google Drive (Google Docs only)         | ✅               | [`GoogleDriveLoader`](https://python.langchain.com/api_reference/google_community/drive/langchain_google_community.drive.GoogleDriveLoader.html)                                               |\n| [Huawei OBS Directory](/oss/python/integrations/document_loaders/huawei_obs_directory)                     | Load documents from Huawei Object Storage Service Directory | ❌               | [`OBSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_directory.OBSDirectoryLoader.html)                       |\n| [Huawei OBS File](/oss/python/integrations/document_loaders/huawei_obs_file)                               | Load documents from Huawei Object Storage Service File      | ❌               | [`OBSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_file.OBSFileLoader.html)                                      |\n| [Microsoft OneDrive](/oss/python/integrations/document_loaders/microsoft_onedrive)                         | Load documents from Microsoft OneDrive                      | ❌               | [`OneDriveLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.onedrive.OneDriveLoader.html)                                    |\n| [Microsoft SharePoint](/oss/python/integrations/document_loaders/microsoft_sharepoint)                     | Load documents from Microsoft SharePoint                    | ❌               | [`SharePointLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.sharepoint.SharePointLoader.html)                              |\n| [Tencent COS Directory](/oss/python/integrations/document_loaders/tencent_cos_directory)                   | Load documents from Tencent Cloud Object Storage Directory  | ❌               | [`TencentCOSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_directory.TencentCOSDirectoryLoader.html) |\n| [Tencent COS File](/oss/python/integrations/document_loaders/tencent_cos_file)                             | Load documents from Tencent Cloud Object Storage File       | ❌               | [`TencentCOSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_file.TencentCOSFileLoader.html)                |\n\n### Social Platforms\n\nThe below document loaders allow you to load documents from different social media platforms.\n\n| Document Loader                                              | API reference                                                                                                                                                      |\n| ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [Twitter](/oss/python/integrations/document_loaders/twitter) | [`TwitterTweetLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.twitter.TwitterTweetLoader.html) |\n| [Reddit](/oss/python/integrations/document_loaders/reddit)   | [`RedditPostsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.reddit.RedditPostsLoader.html)    |\n\n### Messaging Services\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader                                                          | API reference                                                                                                                                                               |\n| ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Telegram](/oss/python/integrations/document_loaders/telegram)           | [`TelegramChatFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.telegram.TelegramChatFileLoader.html) |\n| [WhatsApp](/oss/python/integrations/document_loaders/whatsapp_chat)      | [`WhatsAppChatLoader`](https://python.langchain.com/api_reference/community/chat_loaders/langchain_community.chat_loaders.whatsapp.WhatsAppChatLoader.html)                 |\n| [Discord](/oss/python/integrations/document_loaders/discord)             | [`DiscordChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.discord.DiscordChatLoader.html)            |\n| [Facebook Chat](/oss/python/integrations/document_loaders/facebook_chat) | [`FacebookChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.facebook_chat.FacebookChatLoader.html)    |\n| [Mastodon](/oss/python/integrations/document_loaders/mastodon)           | [`MastodonTootsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.mastodon.MastodonTootsLoader.html)       |\n\n### Productivity tools\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader                                            | API reference                                                                                                                                                                  |\n| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [Figma](/oss/python/integrations/document_loaders/figma)   | [`FigmaFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.figma.FigmaFileLoader.html)                     |\n| [Notion](/oss/python/integrations/document_loaders/notion) | [`NotionDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.notion.NotionDirectoryLoader.html)        |\n| [Slack](/oss/python/integrations/document_loaders/slack)   | [`SlackDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.slack_directory.SlackDirectoryLoader.html) |\n| [Quip](/oss/python/integrations/document_loaders/quip)     | [`QuipLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.quip.QuipLoader.html)                                |\n| [Trello](/oss/python/integrations/document_loaders/trello) | [`TrelloLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.trello.TrelloLoader.html)                          |\n| [Roam](/oss/python/integrations/document_loaders/roam)     | [`RoamLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.roam.RoamLoader.html)                                |\n| [GitHub](/oss/python/integrations/document_loaders/github) | [`GithubFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.github.GithubFileLoader.html)                  |\n\n### Common File Types\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader                                                                                | Data Type                                                                                                                                                                    |\n| ---------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [CSVLoader](/oss/python/integrations/document_loaders/csv)                                     | CSV files                                                                                                                                                                    |\n| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file)                    | Many file types (see [https://docs.unstructured.io/platform/supported-file-types](https://docs.unstructured.io/platform/supported-file-types))                               |\n| [JSONLoader](/oss/python/integrations/document_loaders/json)                                   | JSON files                                                                                                                                                                   |\n| [BSHTMLLoader](/oss/python/integrations/document_loaders/bshtml)                               | HTML files                                                                                                                                                                   |\n| [DoclingLoader](/oss/python/integrations/document_loaders/docling)                             | Various file types (see [https://ds4sd.github.io/docling/](https://ds4sd.github.io/docling/))                                                                                |\n| [PolarisAIDataInsightLoader](/oss/python/integrations/document_loaders/polaris_ai_datainsight) | Various file types (see [https://datainsight.polarisoffice.com/documentation?docType=doc\\_extract](https://datainsight.polarisoffice.com/documentation?docType=doc_extract)) |\n\n## All document loaders\n\n<Columns cols={3}>\n  <Card title=\"acreom\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/acreom\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AgentQLLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/agentql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AirbyteLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/airbyte\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Airtable\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/airtable\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Alibaba Cloud MaxCompute\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/alibaba_cloud_maxcompute\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Amazon Textract\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/amazon_textract\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Apify Dataset\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/apify_dataset\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ArxivLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/arxiv\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AssemblyAI Audio Transcripts\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/assemblyai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AstraDB\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/astradb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Async Chromium\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/async_chromium\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AsyncHtml\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/async_html\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Athena\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/athena\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AWS S3 Directory\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/aws_s3_directory\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AWS S3 File\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/aws_s3_file\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AZLyrics\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/azlyrics\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure AI Data\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/azure_ai_data\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure Blob Storage\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/azure_blob_storage\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure AI Document Intelligence\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/azure_document_intelligence\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BibTeX\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/bibtex\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BiliBili\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/bilibili\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Blackboard\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/blackboard\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Blockchain\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/blockchain\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Box\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/box\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Brave Search\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/brave_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Browserbase\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/browserbase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Browserless\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/browserless\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BSHTMLLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/bshtml\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cassandra\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/cassandra\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatGPT Data\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/chatgpt_loader\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"College Confidential\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/college_confidential\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Concurrent Loader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/concurrent\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Confluence\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/confluence\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"CoNLL-U\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/conll-u\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Copy Paste\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/copypaste\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Couchbase\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/couchbase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"CSV\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/csv\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cube Semantic Layer\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/cube_semantic\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Datadog Logs\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/datadog_logs\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dedoc\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/dedoc\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Diffbot\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/diffbot\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Discord\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/discord\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Docling\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/docling\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Docugami\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/docugami\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Docusaurus\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/docusaurus\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dropbox\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/dropbox\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Email\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/email\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"EPub\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/epub\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Etherscan\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/etherscan\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"EverNote\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/evernote\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Facebook Chat\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/facebook_chat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Fauna\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/fauna\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Figma\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/figma\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FireCrawl\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/firecrawl\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Geopandas\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/geopandas\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Git\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/git\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GitBook\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/gitbook\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GitHub\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/github\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Glue Catalog\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/glue_catalog\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google AlloyDB for PostgreSQL\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_alloydb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google BigQuery\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_bigquery\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Bigtable\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_bigtable\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud SQL for SQL Server\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_cloud_sql_mssql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud SQL for MySQL\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_cloud_sql_mysql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud SQL for PostgreSQL\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_cloud_sql_pg\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud Storage Directory\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_cloud_storage_directory\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud Storage File\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_cloud_storage_file\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Firestore in Datastore Mode\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_datastore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Drive\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_drive\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google El Carro for Oracle Workloads\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_el_carro\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Firestore (Native Mode)\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_firestore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Memorystore for Redis\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_memorystore_redis\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Spanner\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_spanner\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Speech-to-Text\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/google_speech_to_text\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Grobid\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/grobid\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gutenberg\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/gutenberg\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Hacker News\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/hacker_news\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Huawei OBS Directory\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/huawei_obs_directory\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Huawei OBS File\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/huawei_obs_file\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"HuggingFace Dataset\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/hugging_face_dataset\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"HyperbrowserLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/hyperbrowser\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"iFixit\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/ifixit\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Images\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/image\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Image Captions\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/image_captions\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IMSDb\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/imsdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Iugu\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/iugu\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Joplin\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/joplin\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JSONLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/json\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Jupyter Notebook\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/jupyter_notebook\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Kinetica\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/kinetica\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"lakeFS\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/lakefs\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LangSmith\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/langsmith\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LarkSuite (FeiShu)\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/larksuite\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LLM Sherpa\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/llmsherpa\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Mastodon\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/mastodon\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MathPixPDFLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/mathpix\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MediaWiki Dump\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/mediawikidump\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Merge Documents Loader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/merge_doc\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MHTML\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/mhtml\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Microsoft Excel\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/microsoft_excel\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Microsoft OneDrive\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/microsoft_onedrive\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Microsoft OneNote\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/microsoft_onenote\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Microsoft PowerPoint\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/microsoft_powerpoint\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Microsoft SharePoint\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/microsoft_sharepoint\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Microsoft Word\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/microsoft_word\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Near Blockchain\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/mintbase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Modern Treasury\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/modern_treasury\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MongoDB\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/mongodb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Needle Document Loader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/needle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"News URL\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/news\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Notion DB\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/notion\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nuclia\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/nuclia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Obsidian\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/obsidian\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenDataLoader PDF\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/opendataloader_pdf\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Open Document Format (ODT)\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/odt\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Open City Data\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/open_city_data\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oracle Autonomous Database\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/oracleadb_loader\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oracle AI Vector Search\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/oracleai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Org-mode\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/org_mode\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Outline Document Loader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/outline\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pandas DataFrame\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pandas_dataframe\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PDFMinerLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pdfminer\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PDFPlumber\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pdfplumber\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pebblo Safe DocumentLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pebblo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Polaris AI DataInsight\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/polaris_ai_datainsight\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Polars DataFrame\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/polars_dataframe\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dell PowerScale\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/powerscale\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Psychic\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/psychic\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PubMed\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pubmed\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PullMdLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pull_md\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PyMuPDFLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pymupdf\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PyMuPDF4LLM\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pymupdf4llm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PyPDFDirectoryLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pypdfdirectory\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PyPDFium2Loader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pypdfium2\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PyPDFLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pypdfloader\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PySpark\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/pyspark_dataframe\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Quip\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/quip\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ReadTheDocs Documentation\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/readthedocs_documentation\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Recursive URL\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/recursive_url\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Reddit\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/reddit\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Roam\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/roam\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Rockset\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/rockset\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"rspace\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/rspace\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RSS Feeds\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/rss\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RST\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/rst\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"scrapfly\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/scrapfly\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ScrapingAnt\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/scrapingant\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SingleStore\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/singlestore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Sitemap\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/sitemap\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Slack\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/slack\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Snowflake\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/snowflake\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Source Code\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/source_code\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Spider\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/spider\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Spreedly\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/spreedly\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Stripe\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/stripe\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Subtitle\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/subtitle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SurrealDB\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/surrealdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Telegram\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/telegram\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tencent COS Directory\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/tencent_cos_directory\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tencent COS File\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/tencent_cos_file\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TensorFlow Datasets\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/tensorflow_datasets\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TiDB\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/tidb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"2Markdown\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/tomarkdown\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TOML\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/toml\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Trello\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/trello\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TSV\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/tsv\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Twitter\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/twitter\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"UnDatasIO\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/undatasio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Unstructured\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/unstructured_file\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"UnstructuredMarkdownLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/unstructured_markdown\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"UnstructuredPDFLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/unstructured_pdfloader\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Upstage\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/upstage\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"URL\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/url\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vsdx\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/vsdx\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Weather\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/weather\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"WebBaseLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/web_base\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"WhatsApp Chat\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/whatsapp_chat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"UnstructuredXMLLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/xml\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Xorbits Pandas DataFrame\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/xorbits\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"YouTube Audio\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/youtube_audio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"YouTube Transcripts\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/youtube_transcript\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"YoutubeLoaderDL\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/yt_dlp\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Yuque\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/yuque\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ZeroxPDFLoader\" icon=\"link\" href=\"/oss/python/integrations/document_loaders/zeroxpdfloader\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/document_loaders/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 45415,
    "word_count": 2624
  },
  {
    "title": "All integration providers",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/all_providers",
    "content": "Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.\n\n## Providers\n\n<Columns cols={3}>\n  <Card title=\"Abso\" href=\"/oss/python/integrations/providers/abso\" icon=\"link\">\n    Custom AI integration platform for enterprise workflows.\n  </Card>\n\n  <Card title=\"Acreom\" href=\"/oss/python/integrations/providers/acreom\" icon=\"link\">\n    Knowledge management platform with AI-powered organization.\n  </Card>\n\n  <Card title=\"ActiveLoop DeepLake\" href=\"/oss/python/integrations/providers/activeloop_deeplake\" icon=\"link\">\n    Vector database for AI applications with deep learning focus.\n  </Card>\n\n  <Card title=\"Ads4GPTs\" href=\"/oss/python/integrations/providers/ads4gpts\" icon=\"link\">\n    Advertising platform for GPT applications and AI services.\n  </Card>\n\n  <Card title=\"AgentQL\" href=\"/oss/python/integrations/providers/agentql\" icon=\"link\">\n    Web scraping with natural language queries.\n  </Card>\n\n  <Card title=\"AI21\" href=\"/oss/python/integrations/providers/ai21\" icon=\"link\">\n    AI21 Labs' Jurassic models for text generation.\n  </Card>\n\n  <Card title=\"AIM Tracking\" href=\"/oss/python/integrations/providers/aim_tracking\" icon=\"link\">\n    Experiment tracking and management platform.\n  </Card>\n\n  <Card title=\"AI/ML API\" href=\"/oss/python/integrations/providers/aimlapi\" icon=\"link\">\n    Unified API for multiple AI and ML services.\n  </Card>\n\n  <Card title=\"AI Network\" href=\"/oss/python/integrations/providers/ainetwork\" icon=\"link\">\n    Decentralized AI computing network platform.\n  </Card>\n\n  <Card title=\"Airbyte\" href=\"/oss/python/integrations/providers/airbyte\" icon=\"link\">\n    Data integration platform for ETL and ELT pipelines.\n  </Card>\n\n  <Card title=\"Airtable\" href=\"/oss/python/integrations/providers/airtable\" icon=\"link\">\n    Cloud-based spreadsheet and database platform.\n  </Card>\n\n  <Card title=\"Alchemy\" href=\"/oss/python/integrations/providers/alchemy\" icon=\"link\">\n    Blockchain development platform and APIs.\n  </Card>\n\n  <Card title=\"Aleph Alpha\" href=\"/oss/python/integrations/providers/aleph_alpha\" icon=\"link\">\n    European AI company's multilingual language models.\n  </Card>\n\n  <Card title=\"Alibaba Cloud\" href=\"/oss/python/integrations/providers/alibaba_cloud\" icon=\"link\">\n    Alibaba's cloud computing and AI services.\n  </Card>\n\n  <Card title=\"AnalyticDB\" href=\"/oss/python/integrations/providers/analyticdb\" icon=\"link\">\n    Alibaba Cloud's real-time analytics database.\n  </Card>\n\n  <Card title=\"Anchor Browser\" href=\"/oss/python/integrations/providers/anchor_browser\" icon=\"link\">\n    Browser automation and web scraping tools.\n  </Card>\n\n  <Card title=\"Annoy\" href=\"/oss/python/integrations/providers/annoy\" icon=\"link\">\n    Approximate nearest neighbors search library.\n  </Card>\n\n  <Card title=\"Anthropic\" href=\"/oss/python/integrations/providers/anthropic\" icon=\"anthropic\">\n    Claude models for advanced reasoning and conversation.\n  </Card>\n\n  <Card title=\"Anyscale\" href=\"/oss/python/integrations/providers/anyscale\" icon=\"link\">\n    Distributed computing platform for ML workloads.\n  </Card>\n\n  <Card title=\"Apache Doris\" href=\"/oss/python/integrations/providers/apache_doris\" icon=\"link\">\n    Real-time analytical database management system.\n  </Card>\n\n  <Card title=\"Apache\" href=\"/oss/python/integrations/providers/apache\" icon=\"link\">\n    Apache Software Foundation tools and libraries.\n  </Card>\n\n  <Card title=\"Apify\" href=\"/oss/python/integrations/providers/apify\" icon=\"link\">\n    Web scraping and automation platform.\n  </Card>\n\n  <Card title=\"Apple\" href=\"/oss/python/integrations/providers/apple\" icon=\"link\">\n    Apple's machine learning and AI frameworks.\n  </Card>\n\n  <Card title=\"ArangoDB\" href=\"/oss/python/integrations/providers/arangodb\" icon=\"link\">\n    Multi-model database with graph capabilities.\n  </Card>\n\n  <Card title=\"Arcee\" href=\"/oss/python/integrations/providers/arcee\" icon=\"link\">\n    Domain-specific language model training platform.\n  </Card>\n\n  <Card title=\"ArcGIS\" href=\"/oss/python/integrations/providers/arcgis\" icon=\"link\">\n    Geographic information system platform.\n  </Card>\n\n  <Card title=\"Argilla\" href=\"/oss/python/integrations/providers/argilla\" icon=\"link\">\n    Data labeling and annotation platform for NLP.\n  </Card>\n\n  <Card title=\"Arize\" href=\"/oss/python/integrations/providers/arize\" icon=\"link\">\n    ML observability and performance monitoring.\n  </Card>\n\n  <Card title=\"Arthur Tracking\" href=\"/oss/python/integrations/providers/arthur_tracking\" icon=\"link\">\n    AI model monitoring and governance platform.\n  </Card>\n\n  <Card title=\"arXiv\" href=\"/oss/python/integrations/providers/arxiv\" icon=\"link\">\n    Academic paper repository and search platform.\n  </Card>\n\n  <Card title=\"Ascend\" href=\"/oss/python/integrations/providers/ascend\" icon=\"link\">\n    Data engineering and pipeline automation platform.\n  </Card>\n\n  <Card title=\"Ask News\" href=\"/oss/python/integrations/providers/asknews\" icon=\"link\">\n    Real-time news search and analysis API.\n  </Card>\n\n  <Card title=\"AssemblyAI\" href=\"/oss/python/integrations/providers/assemblyai\" icon=\"link\">\n    Speech-to-text and audio intelligence API.\n  </Card>\n\n  <Card title=\"AstraDB\" href=\"/oss/python/integrations/providers/astradb\" icon=\"link\">\n    DataStax Astra DB vector database platform.\n  </Card>\n\n  <Card title=\"Atlas\" href=\"/oss/python/integrations/providers/atlas\" icon=\"link\">\n    Data visualization and exploration platform.\n  </Card>\n\n  <Card title=\"AwaDB\" href=\"/oss/python/integrations/providers/awadb\" icon=\"link\">\n    Vector database for AI and ML applications.\n  </Card>\n\n  <Card title=\"AWS\" href=\"/oss/python/integrations/providers/aws\" icon=\"aws\">\n    Amazon Web Services cloud platform and AI services.\n  </Card>\n\n  <Card title=\"AZLyrics\" href=\"/oss/python/integrations/providers/azlyrics\" icon=\"link\">\n    Song lyrics database and search platform.\n  </Card>\n\n  <Card title=\"Azure AI\" href=\"/oss/python/integrations/providers/azure_ai\" icon=\"microsoft\">\n    Microsoft Azure AI and cognitive services.\n  </Card>\n\n  <Card title=\"BAAI\" href=\"/oss/python/integrations/providers/baai\" icon=\"link\">\n    Beijing Academy of AI research and models.\n  </Card>\n\n  <Card title=\"Bagel\" href=\"/oss/python/integrations/providers/bagel\" icon=\"link\">\n    Vector database and semantic search platform.\n  </Card>\n\n  <Card title=\"BagelDB\" href=\"/oss/python/integrations/providers/bageldb\" icon=\"link\">\n    Multi-modal AI database and storage system.\n  </Card>\n\n  <Card title=\"Baichuan\" href=\"/oss/python/integrations/providers/baichuan\" icon=\"link\">\n    Chinese language model from Baichuan AI.\n  </Card>\n\n  <Card title=\"Baidu\" href=\"/oss/python/integrations/providers/baidu\" icon=\"link\">\n    Baidu's AI services and language models.\n  </Card>\n\n  <Card title=\"BananaDev\" href=\"/oss/python/integrations/providers/bananadev\" icon=\"link\">\n    Serverless GPU infrastructure for ML models.\n  </Card>\n\n  <Card title=\"Baseten\" href=\"/oss/python/integrations/providers/baseten\" icon=\"link\">\n    ML model deployment and serving platform.\n  </Card>\n\n  <Card title=\"Beam\" href=\"/oss/python/integrations/providers/beam\" icon=\"link\">\n    Serverless GPU computing platform.\n  </Card>\n\n  <Card title=\"Beautiful Soup\" href=\"/oss/python/integrations/providers/beautiful_soup\" icon=\"link\">\n    HTML and XML parsing library for web scraping.\n  </Card>\n\n  <Card title=\"BibTeX\" href=\"/oss/python/integrations/providers/bibtex\" icon=\"link\">\n    Bibliography management and citation format.\n  </Card>\n\n  <Card title=\"Bilibili\" href=\"/oss/python/integrations/providers/bilibili\" icon=\"link\">\n    Chinese video sharing platform integration.\n  </Card>\n\n  <Card title=\"Bittensor\" href=\"/oss/python/integrations/providers/bittensor\" icon=\"link\">\n    Decentralized AI network and incentive protocol.\n  </Card>\n\n  <Card title=\"Blackboard\" href=\"/oss/python/integrations/providers/blackboard\" icon=\"link\">\n    Educational technology and learning management.\n  </Card>\n\n  <Card title=\"Bodo DataFrames\" href=\"/oss/python/integrations/providers/bodo\" icon=\"link\">\n    High-performance analytics and data processing.\n  </Card>\n\n  <Card title=\"BookendAI\" href=\"/oss/python/integrations/providers/bookendai\" icon=\"link\">\n    AI-powered reading and research assistant.\n  </Card>\n\n  <Card title=\"Box\" href=\"/oss/python/integrations/providers/box\" icon=\"link\">\n    Cloud content management and collaboration.\n  </Card>\n\n  <Card title=\"Brave Search\" href=\"/oss/python/integrations/providers/brave_search\" icon=\"link\">\n    Privacy-focused search engine API.\n  </Card>\n\n  <Card title=\"Breebs\" href=\"/oss/python/integrations/providers/breebs\" icon=\"link\">\n    AI knowledge management and retrieval platform.\n  </Card>\n\n  <Card title=\"Brightdata\" href=\"/oss/python/integrations/providers/brightdata\" icon=\"link\">\n    Web data platform and proxy services.\n  </Card>\n\n  <Card title=\"Browserbase\" href=\"/oss/python/integrations/providers/browserbase\" icon=\"link\">\n    Headless browser automation platform.\n  </Card>\n\n  <Card title=\"Browserless\" href=\"/oss/python/integrations/providers/browserless\" icon=\"link\">\n    Serverless browser automation service.\n  </Card>\n\n  <Card title=\"ByteDance\" href=\"/oss/python/integrations/providers/byte_dance\" icon=\"link\">\n    ByteDance's AI models and services.\n  </Card>\n\n  <Card title=\"Cassandra\" href=\"/oss/python/integrations/providers/cassandra\" icon=\"link\">\n    Distributed NoSQL database management system.\n  </Card>\n\n  <Card title=\"Cerebras\" href=\"/oss/python/integrations/providers/cerebras\" icon=\"link\">\n    AI compute platform with specialized processors.\n  </Card>\n\n  <Card title=\"CerebriumAI\" href=\"/oss/python/integrations/providers/cerebriumai\" icon=\"link\">\n    Serverless GPU platform for AI applications.\n  </Card>\n\n  <Card title=\"Chaindesk\" href=\"/oss/python/integrations/providers/chaindesk\" icon=\"link\">\n    No-code AI chatbot and automation platform.\n  </Card>\n\n  <Card title=\"Chroma\" href=\"/oss/python/integrations/providers/chroma\" icon=\"link\">\n    Open-source embedding database for AI apps.\n  </Card>\n\n  <Card title=\"Clarifai\" href=\"/oss/python/integrations/providers/clarifai\" icon=\"link\">\n    Computer vision and AI model platform.\n  </Card>\n\n  <Card title=\"ClearML Tracking\" href=\"/oss/python/integrations/providers/clearml_tracking\" icon=\"link\">\n    ML experiment tracking and automation.\n  </Card>\n\n  <Card title=\"ClickHouse\" href=\"/oss/python/integrations/providers/clickhouse\" icon=\"link\">\n    Fast columnar database for analytics.\n  </Card>\n\n  <Card title=\"ClickUp\" href=\"/oss/python/integrations/providers/clickup\" icon=\"link\">\n    Project management and productivity platform.\n  </Card>\n\n  <Card title=\"Cloudflare\" href=\"/oss/python/integrations/providers/cloudflare\" icon=\"link\">\n    Web infrastructure and security services.\n  </Card>\n\n  <Card title=\"Clova\" href=\"/oss/python/integrations/providers/clova\" icon=\"link\">\n    Naver's AI assistant and NLP platform.\n  </Card>\n\n  <Card title=\"CnosDB\" href=\"/oss/python/integrations/providers/cnosdb\" icon=\"link\">\n    Time series database for IoT and analytics.\n  </Card>\n\n  <Card title=\"Cognee\" href=\"/oss/python/integrations/providers/cognee\" icon=\"link\">\n    Memory layer for AI applications and agents.\n  </Card>\n\n  <Card title=\"CogniSwitch\" href=\"/oss/python/integrations/providers/cogniswitch\" icon=\"link\">\n    AI knowledge management and retrieval system.\n  </Card>\n\n  <Card title=\"Cohere\" href=\"/oss/python/integrations/providers/cohere\" icon=\"link\">\n    Language AI platform for enterprise applications.\n  </Card>\n\n  <Card title=\"College Confidential\" href=\"/oss/python/integrations/providers/college_confidential\" icon=\"link\">\n    College admissions and education platform.\n  </Card>\n\n  <Card title=\"Comet Tracking\" href=\"/oss/python/integrations/providers/comet_tracking\" icon=\"link\">\n    ML experiment tracking and model management.\n  </Card>\n\n  <Card title=\"Confident\" href=\"/oss/python/integrations/providers/confident\" icon=\"link\">\n    AI observability and monitoring platform.\n  </Card>\n\n  <Card title=\"Confluence\" href=\"/oss/python/integrations/providers/confluence\" icon=\"link\">\n    Team collaboration and documentation platform.\n  </Card>\n\n  <Card title=\"Connery\" href=\"/oss/python/integrations/providers/connery\" icon=\"link\">\n    Plugin system for AI agents and applications.\n  </Card>\n\n  <Card title=\"Context\" href=\"/oss/python/integrations/providers/context\" icon=\"link\">\n    Context management for AI applications.\n  </Card>\n\n  <Card title=\"Contextual\" href=\"/oss/python/integrations/providers/contextual\" icon=\"link\">\n    Contextual AI and language understanding.\n  </Card>\n\n  <Card title=\"Couchbase\" href=\"/oss/python/integrations/providers/couchbase\" icon=\"link\">\n    NoSQL cloud database platform.\n  </Card>\n\n  <Card title=\"Coze\" href=\"/oss/python/integrations/providers/coze\" icon=\"link\">\n    Conversational AI platform and chatbot builder.\n  </Card>\n\n  <Card title=\"CrateDB\" href=\"/oss/python/integrations/providers/cratedb\" icon=\"link\">\n    Distributed SQL database for machine data.\n  </Card>\n\n  <Card title=\"CTransformers\" href=\"/oss/python/integrations/providers/ctransformers\" icon=\"link\">\n    Python bindings for transformer models in C/C++.\n  </Card>\n\n  <Card title=\"CTranslate2\" href=\"/oss/python/integrations/providers/ctranslate2\" icon=\"link\">\n    Fast inference engine for Transformer models.\n  </Card>\n\n  <Card title=\"Cube\" href=\"/oss/python/integrations/providers/cube\" icon=\"link\">\n    Semantic layer for building data applications.\n  </Card>\n\n  <Card title=\"Dappier\" href=\"/oss/python/integrations/providers/dappier\" icon=\"link\">\n    Real-time AI data platform and API.\n  </Card>\n\n  <Card title=\"DashVector\" href=\"/oss/python/integrations/providers/dashvector\" icon=\"link\">\n    Alibaba Cloud's vector database service.\n  </Card>\n\n  <Card title=\"Databricks\" href=\"/oss/python/integrations/providers/databricks\" icon=\"link\">\n    Unified analytics platform for big data and ML.\n  </Card>\n\n  <Card title=\"Datadog\" href=\"/oss/python/integrations/providers/datadog\" icon=\"link\">\n    Monitoring and analytics platform for applications.\n  </Card>\n\n  <Card title=\"Datadog Logs\" href=\"/oss/python/integrations/providers/datadog_logs\" icon=\"link\">\n    Log management and analysis platform.\n  </Card>\n\n  <Card title=\"DataForSEO\" href=\"/oss/python/integrations/providers/dataforseo\" icon=\"link\">\n    SEO and SERP data API platform.\n  </Card>\n\n  <Card title=\"DataHerald\" href=\"/oss/python/integrations/providers/dataherald\" icon=\"link\">\n    Natural language to SQL query platform.\n  </Card>\n\n  <Card title=\"Daytona\" href=\"/oss/python/integrations/providers/daytona\" icon=\"link\">\n    Secure and elastic infrastructure for running your AI-generated code.\n  </Card>\n\n  <Card title=\"Dedoc\" href=\"/oss/python/integrations/providers/dedoc\" icon=\"link\">\n    Document analysis and structure detection.\n  </Card>\n\n  <Card title=\"DeepInfra\" href=\"/oss/python/integrations/providers/deepinfra\" icon=\"link\">\n    Serverless inference for deep learning models.\n  </Card>\n\n  <Card title=\"DeepLake\" href=\"/oss/python/integrations/providers/deeplake\" icon=\"link\">\n    Vector database for deep learning applications.\n  </Card>\n\n  <Card title=\"DeepSeek\" href=\"/oss/python/integrations/providers/deepseek\" icon=\"link\">\n    Advanced reasoning and coding AI models.\n  </Card>\n\n  <Card title=\"DeepSparse\" href=\"/oss/python/integrations/providers/deepsparse\" icon=\"link\">\n    Inference runtime for sparse neural networks.\n  </Card>\n\n  <Card title=\"Dell\" href=\"/oss/python/integrations/providers/dell\" icon=\"link\">\n    Dell Technologies AI and computing solutions.\n  </Card>\n\n  <Card title=\"Diffbot\" href=\"/oss/python/integrations/providers/diffbot\" icon=\"link\">\n    Web data extraction and knowledge graph.\n  </Card>\n\n  <Card title=\"Dingo\" href=\"/oss/python/integrations/providers/dingo\" icon=\"link\">\n    Distributed vector database system.\n  </Card>\n\n  <Card title=\"Discord\" href=\"/oss/python/integrations/providers/discord\" icon=\"link\">\n    Communication platform integration and bots.\n  </Card>\n\n  <Card title=\"Discord Shikenso\" href=\"/oss/python/integrations/providers/discord-shikenso\" icon=\"link\">\n    Discord analytics and moderation tools.\n  </Card>\n\n  <Card title=\"DocArray\" href=\"/oss/python/integrations/providers/docarray\" icon=\"link\">\n    Data structure for multimodal AI applications.\n  </Card>\n\n  <Card title=\"Docling\" href=\"/oss/python/integrations/providers/docling\" icon=\"link\">\n    Document processing and AI integration.\n  </Card>\n\n  <Card title=\"Doctran\" href=\"/oss/python/integrations/providers/doctran\" icon=\"link\">\n    Document transformation and processing.\n  </Card>\n\n  <Card title=\"Docugami\" href=\"/oss/python/integrations/providers/docugami\" icon=\"link\">\n    Document AI and semantic processing.\n  </Card>\n\n  <Card title=\"Docusaurus\" href=\"/oss/python/integrations/providers/docusaurus\" icon=\"link\">\n    Documentation website generator and platform.\n  </Card>\n\n  <Card title=\"Dria\" href=\"/oss/python/integrations/providers/dria\" icon=\"link\">\n    Decentralized knowledge retrieval network.\n  </Card>\n\n  <Card title=\"Dropbox\" href=\"/oss/python/integrations/providers/dropbox\" icon=\"link\">\n    Cloud storage and file sharing platform.\n  </Card>\n\n  <Card title=\"DuckDB\" href=\"/oss/python/integrations/providers/duckdb\" icon=\"link\">\n    In-process SQL OLAP database management system.\n  </Card>\n\n  <Card title=\"DuckDuckGo Search\" href=\"/oss/python/integrations/providers/duckduckgo_search\" icon=\"link\">\n    Privacy-focused search engine integration.\n  </Card>\n\n  <Card title=\"E2B\" href=\"/oss/python/integrations/providers/e2b\" icon=\"link\">\n    Cloud development environment platform.\n  </Card>\n\n  <Card title=\"EdenAI\" href=\"/oss/python/integrations/providers/edenai\" icon=\"link\">\n    Unified API for multiple AI services.\n  </Card>\n\n  <Card title=\"Elasticsearch\" href=\"/oss/python/integrations/providers/elasticsearch\" icon=\"link\">\n    Distributed search and analytics engine.\n  </Card>\n\n  <Card title=\"ElevenLabs\" href=\"/oss/python/integrations/providers/elevenlabs\" icon=\"link\">\n    AI voice synthesis and speech platform.\n  </Card>\n\n  <Card title=\"EmbedChain\" href=\"/oss/python/integrations/providers/embedchain\" icon=\"link\">\n    Framework for creating RAG applications.\n  </Card>\n\n  <Card title=\"Epsilla\" href=\"/oss/python/integrations/providers/epsilla\" icon=\"link\">\n    Vector database for AI and ML applications.\n  </Card>\n\n  <Card title=\"Etherscan\" href=\"/oss/python/integrations/providers/etherscan\" icon=\"link\">\n    Ethereum blockchain explorer and analytics.\n  </Card>\n\n  <Card title=\"EverlyAI\" href=\"/oss/python/integrations/providers/everlyai\" icon=\"link\">\n    Serverless AI inference platform.\n  </Card>\n\n  <Card title=\"Evernote\" href=\"/oss/python/integrations/providers/evernote\" icon=\"link\">\n    Note-taking and organization platform.\n  </Card>\n\n  <Card title=\"Exa Search\" href=\"/oss/python/integrations/providers/exa_search\" icon=\"link\">\n    AI-powered search engine for developers.\n  </Card>\n\n  <Card title=\"Facebook\" href=\"/oss/python/integrations/providers/facebook\" icon=\"link\">\n    Meta's social platform integration and APIs.\n  </Card>\n\n  <Card title=\"FalkorDB\" href=\"/oss/python/integrations/providers/falkordb\" icon=\"link\">\n    Graph database with ultra-low latency.\n  </Card>\n\n  <Card title=\"Fauna\" href=\"/oss/python/integrations/providers/fauna\" icon=\"link\">\n    Serverless, globally distributed database.\n  </Card>\n\n  <Card title=\"Featherless AI\" href=\"/oss/python/integrations/providers/featherless-ai\" icon=\"link\">\n    Fast and efficient AI model serving.\n  </Card>\n\n  <Card title=\"Fiddler\" href=\"/oss/python/integrations/providers/fiddler\" icon=\"link\">\n    AI observability and monitoring platform.\n  </Card>\n\n  <Card title=\"Figma\" href=\"/oss/python/integrations/providers/figma\" icon=\"link\">\n    Design collaboration and prototyping platform.\n  </Card>\n\n  <Card title=\"FireCrawl\" href=\"/oss/python/integrations/providers/firecrawl\" icon=\"link\">\n    Web scraping and crawling API service.\n  </Card>\n\n  <Card title=\"Fireworks\" href=\"/oss/python/integrations/providers/fireworks\" icon=\"link\">\n    Fast inference platform for open-source models.\n  </Card>\n\n  <Card title=\"Flyte\" href=\"/oss/python/integrations/providers/flyte\" icon=\"link\">\n    Workflow orchestration for ML and data processing.\n  </Card>\n\n  <Card title=\"FMP Data\" href=\"/oss/python/integrations/providers/fmp-data\" icon=\"link\">\n    Financial market data and analytics API.\n  </Card>\n\n  <Card title=\"ForefrontAI\" href=\"/oss/python/integrations/providers/forefrontai\" icon=\"link\">\n    Fine-tuning platform for language models.\n  </Card>\n\n  <Card title=\"Friendli\" href=\"/oss/python/integrations/providers/friendli\" icon=\"link\">\n    Optimized serving engine for AI models.\n  </Card>\n\n  <Card title=\"Galaxia\" href=\"/oss/python/integrations/providers/galaxia\" icon=\"link\">\n    Prompt-driven engineering assistant.\n  </Card>\n\n  <Card title=\"Gel\" href=\"/oss/python/integrations/providers/gel\" icon=\"link\">\n    Knowledge extraction and NLP platform.\n  </Card>\n\n  <Card title=\"GeoPandas\" href=\"/oss/python/integrations/providers/geopandas\" icon=\"link\">\n    Geographic data analysis with Python.\n  </Card>\n\n  <Card title=\"Git\" href=\"/oss/python/integrations/providers/git\" icon=\"link\">\n    Version control system integration.\n  </Card>\n\n  <Card title=\"GitBook\" href=\"/oss/python/integrations/providers/gitbook\" icon=\"link\">\n    Documentation platform and knowledge base.\n  </Card>\n\n  <Card title=\"GitHub\" href=\"/oss/python/integrations/providers/github\" icon=\"link\">\n    Code hosting and collaboration platform.\n  </Card>\n\n  <Card title=\"GitLab\" href=\"/oss/python/integrations/providers/gitlab\" icon=\"link\">\n    DevOps platform and code repository.\n  </Card>\n\n  <Card title=\"GOAT\" href=\"/oss/python/integrations/providers/goat\" icon=\"link\">\n    Tool use framework for AI agents.\n  </Card>\n\n  <Card title=\"Golden\" href=\"/oss/python/integrations/providers/golden\" icon=\"link\">\n    Knowledge graph and data platform.\n  </Card>\n\n  <Card title=\"Google\" href=\"/oss/python/integrations/providers/google\" icon=\"google\">\n    Google's AI services and cloud platform.\n  </Card>\n\n  <Card title=\"Google Serper\" href=\"/oss/python/integrations/providers/google_serper\" icon=\"google\">\n    Google Search API service.\n  </Card>\n\n  <Card title=\"GooseAI\" href=\"/oss/python/integrations/providers/gooseai\" icon=\"link\">\n    Fully managed NLP-as-a-Service platform.\n  </Card>\n\n  <Card title=\"GPT4All\" href=\"/oss/python/integrations/providers/gpt4all\" icon=\"link\">\n    Open-source LLM ecosystem for local deployment.\n  </Card>\n\n  <Card title=\"Gradient\" href=\"/oss/python/integrations/providers/gradient\" icon=\"link\">\n    AI model training and deployment platform.\n  </Card>\n\n  <Card title=\"DigitalOcean Gradient AI Platform\" href=\"/oss/python/integrations/providers/gradientai\" icon=\"link\">\n    Single endpoint to multiple LLMs via serverless inference.\n  </Card>\n\n  <Card title=\"Graph RAG\" href=\"/oss/python/integrations/providers/graph_rag\" icon=\"link\">\n    Graph-based retrieval augmented generation.\n  </Card>\n\n  <Card title=\"GraphSignal\" href=\"/oss/python/integrations/providers/graphsignal\" icon=\"link\">\n    AI observability and monitoring platform.\n  </Card>\n\n  <Card title=\"GreenNode\" href=\"/oss/python/integrations/providers/greennode\" icon=\"link\">\n    Sustainable AI computing platform.\n  </Card>\n\n  <Card title=\"GROBID\" href=\"/oss/python/integrations/providers/grobid\" icon=\"link\">\n    Machine learning library for bibliographic data.\n  </Card>\n\n  <Card title=\"Groq\" href=\"/oss/python/integrations/providers/groq\" icon=\"link\">\n    Ultra-fast inference with specialized hardware.\n  </Card>\n\n  <Card title=\"Gutenberg\" href=\"/oss/python/integrations/providers/gutenberg\" icon=\"link\">\n    Project Gutenberg digital library access.\n  </Card>\n\n  <Card title=\"Hacker News\" href=\"/oss/python/integrations/providers/hacker_news\" icon=\"link\">\n    Tech news and discussion platform.\n  </Card>\n\n  <Card title=\"Hazy Research\" href=\"/oss/python/integrations/providers/hazy_research\" icon=\"link\">\n    Machine learning research and tools.\n  </Card>\n\n  <Card title=\"Helicone\" href=\"/oss/python/integrations/providers/helicone\" icon=\"link\">\n    LLM observability and monitoring platform.\n  </Card>\n\n  <Card title=\"Hologres\" href=\"/oss/python/integrations/providers/hologres\" icon=\"link\">\n    Real-time interactive analytics service.\n  </Card>\n\n  <Card title=\"HTML2Text\" href=\"/oss/python/integrations/providers/html2text\" icon=\"link\">\n    HTML to plain text conversion utility.\n  </Card>\n\n  <Card title=\"Huawei\" href=\"/oss/python/integrations/providers/huawei\" icon=\"link\">\n    Huawei Cloud AI services and models.\n  </Card>\n\n  <Card title=\"Hugging Face\" href=\"/oss/python/integrations/providers/huggingface\" icon=\"link\">\n    Open platform for ML models and datasets.\n  </Card>\n\n  <Card title=\"HyperBrowser\" href=\"/oss/python/integrations/providers/hyperbrowser\" icon=\"link\">\n    Web automation and scraping platform.\n  </Card>\n\n  <Card title=\"IBM\" href=\"/oss/python/integrations/providers/ibm\" icon=\"link\">\n    IBM Watson AI and enterprise solutions.\n  </Card>\n\n  <Card title=\"IEIT Systems\" href=\"/oss/python/integrations/providers/ieit_systems\" icon=\"link\">\n    Enterprise AI and system integration.\n  </Card>\n\n  <Card title=\"iFixit\" href=\"/oss/python/integrations/providers/ifixit\" icon=\"link\">\n    Repair guides and technical documentation.\n  </Card>\n\n  <Card title=\"iFlytek\" href=\"/oss/python/integrations/providers/iflytek\" icon=\"link\">\n    Chinese speech and language AI platform.\n  </Card>\n\n  <Card title=\"IMSDb\" href=\"/oss/python/integrations/providers/imsdb\" icon=\"link\">\n    Internet Movie Script Database access.\n  </Card>\n\n  <Card title=\"InfinispanVS\" href=\"/oss/python/integrations/providers/infinispanvs\" icon=\"link\">\n    Distributed cache and data grid platform.\n  </Card>\n\n  <Card title=\"Infinity\" href=\"/oss/python/integrations/providers/infinity\" icon=\"link\">\n    High-performance embedding inference server.\n  </Card>\n\n  <Card title=\"Infino\" href=\"/oss/python/integrations/providers/infino\" icon=\"link\">\n    Observability and monitoring platform.\n  </Card>\n\n  <Card title=\"Intel\" href=\"/oss/python/integrations/providers/intel\" icon=\"link\">\n    Intel's AI optimization tools and libraries.\n  </Card>\n\n  <Card title=\"Isaacus\" href=\"/oss/python/integrations/providers/isaacus\" icon=\"link\">\n    Legal AI models, apps, and data.\n  </Card>\n\n  <Card title=\"IUGU\" href=\"/oss/python/integrations/providers/iugu\" icon=\"link\">\n    Brazilian payment processing platform.\n  </Card>\n\n  <Card title=\"Jaguar\" href=\"/oss/python/integrations/providers/jaguar\" icon=\"link\">\n    Vector database and search platform.\n  </Card>\n\n  <Card title=\"Javelin AI Gateway\" href=\"/oss/python/integrations/providers/javelin_ai_gateway\" icon=\"link\">\n    AI model gateway and management platform.\n  </Card>\n\n  <Card title=\"Jenkins\" href=\"/oss/python/integrations/providers/jenkins\" icon=\"link\">\n    Automation server and CI/CD platform.\n  </Card>\n\n  <Card title=\"Jina\" href=\"/oss/python/integrations/providers/jina\" icon=\"link\">\n    Neural search framework and cloud platform.\n  </Card>\n\n  <Card title=\"John Snow Labs\" href=\"/oss/python/integrations/providers/johnsnowlabs\" icon=\"link\">\n    Enterprise NLP and healthcare AI platform.\n  </Card>\n\n  <Card title=\"Joplin\" href=\"/oss/python/integrations/providers/joplin\" icon=\"link\">\n    Open-source note taking and organization.\n  </Card>\n\n  <Card title=\"KDB.AI\" href=\"/oss/python/integrations/providers/kdbai\" icon=\"link\">\n    Time-series vector database platform.\n  </Card>\n\n  <Card title=\"Kinetica\" href=\"/oss/python/integrations/providers/kinetica\" icon=\"link\">\n    Real-time analytics and database platform.\n  </Card>\n\n  <Card title=\"KoboldAI\" href=\"/oss/python/integrations/providers/koboldai\" icon=\"link\">\n    Browser-based AI writing assistant.\n  </Card>\n\n  <Card title=\"Konko\" href=\"/oss/python/integrations/providers/konko\" icon=\"link\">\n    Generative AI platform and model hosting.\n  </Card>\n\n  <Card title=\"KoNLPy\" href=\"/oss/python/integrations/providers/konlpy\" icon=\"link\">\n    Korean natural language processing toolkit.\n  </Card>\n\n  <Card title=\"Kuzu\" href=\"/oss/python/integrations/providers/kuzu\" icon=\"link\">\n    Embedded graph database management system.\n  </Card>\n\n  <Card title=\"Label Studio\" href=\"/oss/python/integrations/providers/labelstudio\" icon=\"link\">\n    Data labeling and annotation platform.\n  </Card>\n\n  <Card title=\"LakeFS\" href=\"/oss/python/integrations/providers/lakefs\" icon=\"link\">\n    Git-like version control for data lakes.\n  </Card>\n\n  <Card title=\"LanceDB\" href=\"/oss/python/integrations/providers/lancedb\" icon=\"link\">\n    Developer-friendly embedded vector database.\n  </Card>\n\n  <Card title=\"LangChain Decorators\" href=\"/oss/python/integrations/providers/langchain_decorators\" icon=\"link\">\n    Syntactic sugar and utilities for LangChain.\n  </Card>\n\n  <Card title=\"LangFair\" href=\"/oss/python/integrations/providers/langfair\" icon=\"link\">\n    Bias testing framework for language models.\n  </Card>\n\n  <Card title=\"LangFuse\" href=\"/oss/python/integrations/providers/langfuse\" icon=\"link\">\n    LLM engineering platform and observability.\n  </Card>\n\n  <Card title=\"Lantern\" href=\"/oss/python/integrations/providers/lantern\" icon=\"link\">\n    PostgreSQL vector database extension.\n  </Card>\n\n  <Card title=\"Lindorm\" href=\"/oss/python/integrations/providers/lindorm\" icon=\"link\">\n    Alibaba Cloud's multi-model database service.\n  </Card>\n\n  <Card title=\"LinkUp\" href=\"/oss/python/integrations/providers/linkup\" icon=\"link\">\n    Real-time job market data and search.\n  </Card>\n\n  <Card title=\"LiteLLM\" href=\"/oss/python/integrations/providers/litellm\" icon=\"link\">\n    Unified interface for 100+ LLM APIs.\n  </Card>\n\n  <Card title=\"LlamaIndex\" href=\"/oss/python/integrations/providers/llama_index\" icon=\"link\">\n    Data framework for LLM applications.\n  </Card>\n\n  <Card title=\"LlamaCPP\" href=\"/oss/python/integrations/providers/llamacpp\" icon=\"link\">\n    Port of Meta's LLaMA model in C/C++.\n  </Card>\n\n  <Card title=\"LlamaEdge\" href=\"/oss/python/integrations/providers/llamaedge\" icon=\"link\">\n    Edge computing platform for LLaMA models.\n  </Card>\n\n  <Card title=\"LlamaFile\" href=\"/oss/python/integrations/providers/llamafile\" icon=\"link\">\n    Single-file executable for running LLMs.\n  </Card>\n\n  <Card title=\"LLMonitor\" href=\"/oss/python/integrations/providers/llmonitor\" icon=\"link\">\n    Observability platform for LLM applications.\n  </Card>\n\n  <Card title=\"LocalAI\" href=\"/oss/python/integrations/providers/localai\" icon=\"link\">\n    Self-hosted OpenAI-compatible API server.\n  </Card>\n\n  <Card title=\"Log10\" href=\"/oss/python/integrations/providers/log10\" icon=\"link\">\n    LLM data management and observability.\n  </Card>\n\n  <Card title=\"MariaDB\" href=\"/oss/python/integrations/providers/mariadb\" icon=\"link\">\n    Open-source relational database management.\n  </Card>\n\n  <Card title=\"MaritALK\" href=\"/oss/python/integrations/providers/maritalk\" icon=\"link\">\n    Brazilian Portuguese language model.\n  </Card>\n\n  <Card title=\"Marqo\" href=\"/oss/python/integrations/providers/marqo\" icon=\"link\">\n    End-to-end vector search engine.\n  </Card>\n\n  <Card title=\"MediaWiki Dump\" href=\"/oss/python/integrations/providers/mediawikidump\" icon=\"link\">\n    Wikipedia and MediaWiki data processing.\n  </Card>\n\n  <Card title=\"Meilisearch\" href=\"/oss/python/integrations/providers/meilisearch\" icon=\"link\">\n    Lightning-fast search engine platform.\n  </Card>\n\n  <Card title=\"Memcached\" href=\"/oss/python/integrations/providers/memcached\" icon=\"link\">\n    Distributed memory caching system.\n  </Card>\n\n  <Card title=\"Memgraph\" href=\"/oss/python/integrations/providers/memgraph\" icon=\"link\">\n    Real-time graph database platform.\n  </Card>\n\n  <Card title=\"Metal\" href=\"/oss/python/integrations/providers/metal\" icon=\"link\">\n    Managed vector search and retrieval.\n  </Card>\n\n  <Card title=\"Microsoft\" href=\"/oss/python/integrations/providers/microsoft\" icon=\"microsoft\">\n    Microsoft Azure AI and enterprise services.\n  </Card>\n\n  <Card title=\"Milvus\" href=\"/oss/python/integrations/providers/milvus\" icon=\"link\">\n    Open-source vector database for AI applications.\n  </Card>\n\n  <Card title=\"MindsDB\" href=\"/oss/python/integrations/providers/mindsdb\" icon=\"link\">\n    AI layer for databases and data platforms.\n  </Card>\n\n  <Card title=\"Minimax\" href=\"/oss/python/integrations/providers/minimax\" icon=\"link\">\n    Chinese AI company's language models.\n  </Card>\n\n  <Card title=\"MistralAI\" href=\"/oss/python/integrations/providers/mistralai\" icon=\"link\">\n    Efficient open-source language models.\n  </Card>\n\n  <Card title=\"MLflow\" href=\"/oss/python/integrations/providers/mlflow\" icon=\"link\">\n    ML lifecycle management platform.\n  </Card>\n\n  <Card title=\"MLflow Tracking\" href=\"/oss/python/integrations/providers/mlflow_tracking\" icon=\"link\">\n    Experiment tracking and model registry.\n  </Card>\n\n  <Card title=\"MLX\" href=\"/oss/python/integrations/providers/mlx\" icon=\"link\">\n    Apple's machine learning framework.\n  </Card>\n\n  <Card title=\"Modal\" href=\"/oss/python/integrations/providers/modal\" icon=\"link\">\n    Serverless cloud computing for data science.\n  </Card>\n\n  <Card title=\"ModelScope\" href=\"/oss/python/integrations/providers/modelscope\" icon=\"link\">\n    Alibaba's open-source model hub.\n  </Card>\n\n  <Card title=\"Modern Treasury\" href=\"/oss/python/integrations/providers/modern_treasury\" icon=\"link\">\n    Payment operations and treasury management.\n  </Card>\n\n  <Card title=\"Momento\" href=\"/oss/python/integrations/providers/momento\" icon=\"link\">\n    Serverless cache and vector index.\n  </Card>\n\n  <Card title=\"MongoDB\" href=\"/oss/python/integrations/providers/mongodb\" icon=\"link\">\n    Document-based NoSQL database platform.\n  </Card>\n\n  <Card title=\"MongoDB Atlas\" href=\"/oss/python/integrations/providers/mongodb_atlas\" icon=\"link\">\n    Cloud-hosted MongoDB with vector search.\n  </Card>\n\n  <Card title=\"MotherDuck\" href=\"/oss/python/integrations/providers/motherduck\" icon=\"link\">\n    Serverless analytics with DuckDB in the cloud.\n  </Card>\n\n  <Card title=\"Motorhead\" href=\"/oss/python/integrations/providers/motorhead\" icon=\"link\">\n    Long-term memory for AI conversations.\n  </Card>\n\n  <Card title=\"MyScale\" href=\"/oss/python/integrations/providers/myscale\" icon=\"link\">\n    SQL-compatible vector database platform.\n  </Card>\n\n  <Card title=\"Naver\" href=\"/oss/python/integrations/providers/naver\" icon=\"link\">\n    Naver's AI services and language models.\n  </Card>\n\n  <Card title=\"Nebius\" href=\"/oss/python/integrations/providers/nebius\" icon=\"link\">\n    AI cloud platform and infrastructure.\n  </Card>\n\n  <Card title=\"Neo4j\" href=\"/oss/python/integrations/providers/neo4j\" icon=\"link\">\n    Native graph database and analytics platform.\n  </Card>\n\n  <Card title=\"NetMind\" href=\"/oss/python/integrations/providers/netmind\" icon=\"link\">\n    Decentralized AI computing network.\n  </Card>\n\n  <Card title=\"Nimble\" href=\"/oss/python/integrations/providers/nimble\" icon=\"link\">\n    Web intelligence and data extraction.\n  </Card>\n\n  <Card title=\"NLP Cloud\" href=\"/oss/python/integrations/providers/nlpcloud\" icon=\"link\">\n    Production-ready NLP API platform.\n  </Card>\n\n  <Card title=\"Nomic\" href=\"/oss/python/integrations/providers/nomic\" icon=\"link\">\n    Open-source embedding models and tools.\n  </Card>\n\n  <Card title=\"Notion\" href=\"/oss/python/integrations/providers/notion\" icon=\"link\">\n    All-in-one workspace and collaboration platform.\n  </Card>\n\n  <Card title=\"Nuclia\" href=\"/oss/python/integrations/providers/nuclia\" icon=\"link\">\n    AI-powered search and understanding platform.\n  </Card>\n\n  <Card title=\"NVIDIA\" href=\"/oss/python/integrations/providers/nvidia\" icon=\"link\">\n    NVIDIA's AI computing platform and models.\n  </Card>\n\n  <Card title=\"Obsidian\" href=\"/oss/python/integrations/providers/obsidian\" icon=\"link\">\n    Connected note-taking and knowledge management.\n  </Card>\n\n  <Card title=\"OceanBase\" href=\"/oss/python/integrations/providers/oceanbase\" icon=\"link\">\n    Distributed relational database system.\n  </Card>\n\n  <Card title=\"OCI\" href=\"/oss/python/integrations/providers/oci\" icon=\"link\">\n    Oracle Cloud Infrastructure AI services.\n  </Card>\n\n  <Card title=\"OctoAI\" href=\"/oss/python/integrations/providers/octoai\" icon=\"link\">\n    Efficient AI compute and model serving.\n  </Card>\n\n  <Card title=\"Ollama\" href=\"/oss/python/integrations/providers/ollama\" icon=\"link\">\n    Run Large Language Models (LLMs) locally.\n  </Card>\n\n  <Card title=\"Ontotext GraphDB\" href=\"/oss/python/integrations/providers/ontotext_graphdb\" icon=\"link\">\n    RDF database and semantic graph platform.\n  </Card>\n\n  <Card title=\"OpenAI\" href=\"/oss/python/integrations/providers/openai\" icon=\"openai\">\n    GPT models and comprehensive AI platform.\n  </Card>\n\n  <Card title=\"OpenDataLoader PDF\" href=\"/oss/python/integrations/providers/opendataloader_pdf\" icon=\"link\">\n    Safe, Open, High-Performance — PDF for AI\n  </Card>\n\n  <Card title=\"OpenGradient\" href=\"/oss/python/integrations/providers/opengradient\" icon=\"link\">\n    AI model training and fine-tuning platform.\n  </Card>\n\n  <Card title=\"OpenLLM\" href=\"/oss/python/integrations/providers/openllm\" icon=\"link\">\n    Operating LLMs in production environment.\n  </Card>\n\n  <Card title=\"OpenSearch\" href=\"/oss/python/integrations/providers/opensearch\" icon=\"link\">\n    Distributed search and analytics suite.\n  </Card>\n\n  <Card title=\"OpenWeatherMap\" href=\"/oss/python/integrations/providers/openweathermap\" icon=\"link\">\n    Weather data and forecasting API.\n  </Card>\n\n  <Card title=\"Oracle AI\" href=\"/oss/python/integrations/providers/oracleai\" icon=\"link\">\n    Oracle's AI and machine learning services.\n  </Card>\n\n  <Card title=\"Outline\" href=\"/oss/python/integrations/providers/outline\" icon=\"link\">\n    Team knowledge base and wiki platform.\n  </Card>\n\n  <Card title=\"Outlines\" href=\"/oss/python/integrations/providers/outlines\" icon=\"link\">\n    Structured generation for language models.\n  </Card>\n\n  <Card title=\"Oxylabs\" href=\"/oss/python/integrations/providers/oxylabs\" icon=\"link\">\n    Web scraping and proxy services.\n  </Card>\n\n  <Card title=\"Pandas\" href=\"/oss/python/integrations/providers/pandas\" icon=\"link\">\n    Data analysis and manipulation library.\n  </Card>\n\n  <Card title=\"Parallel\" href=\"/oss/python/integrations/providers/parallel\" icon=\"link\">\n    AI-powered web search and content extraction for LLMs.\n  </Card>\n\n  <Card title=\"Perigon\" href=\"/oss/python/integrations/providers/perigon\" icon=\"link\">\n    Real-time news and media monitoring.\n  </Card>\n\n  <Card title=\"Permit\" href=\"/oss/python/integrations/providers/permit\" icon=\"link\">\n    Authorization and access control platform.\n  </Card>\n\n  <Card title=\"Perplexity\" href=\"/oss/python/integrations/providers/perplexity\" icon=\"link\">\n    AI-powered search and reasoning engine.\n  </Card>\n\n  <Card title=\"Petals\" href=\"/oss/python/integrations/providers/petals\" icon=\"link\">\n    Distributed inference for Large Language Models.\n  </Card>\n\n  <Card title=\"PG Embedding\" href=\"/oss/python/integrations/providers/pg_embedding\" icon=\"link\">\n    PostgreSQL vector embedding extensions.\n  </Card>\n\n  <Card title=\"pgvector\" href=\"/oss/python/integrations/providers/pgvector\" icon=\"link\">\n    Vector similarity search for PostgreSQL.\n  </Card>\n\n  <Card title=\"Pinecone\" href=\"/oss/python/integrations/providers/pinecone\" icon=\"link\">\n    Managed vector database for ML applications.\n  </Card>\n\n  <Card title=\"PipelineAI\" href=\"/oss/python/integrations/providers/pipelineai\" icon=\"link\">\n    ML pipeline and model deployment platform.\n  </Card>\n\n  <Card title=\"Pipeshift\" href=\"/oss/python/integrations/providers/pipeshift\" icon=\"link\">\n    AI-powered content moderation platform.\n  </Card>\n\n  <Card title=\"PolarisAIDataInsight\" href=\"/oss/python/integrations/providers/polaris_ai_datainsight\" icon=\"link\">\n    Document-loaders for various file formats.\n  </Card>\n\n  <Card title=\"Portkey\" href=\"/oss/python/integrations/providers/portkey/logging_tracing_portkey\" icon=\"link\">\n    AI gateway and observability platform.\n  </Card>\n\n  <Card title=\"Predibase\" href=\"/oss/python/integrations/providers/predibase\" icon=\"link\">\n    Fine-tuning platform for Large Language Models.\n  </Card>\n\n  <Card title=\"PredictionGuard\" href=\"/oss/python/integrations/providers/predictionguard\" icon=\"link\">\n    AI model security and compliance platform.\n  </Card>\n\n  <Card title=\"PreMAI\" href=\"/oss/python/integrations/providers/premai\" icon=\"link\">\n    AI platform for model deployment and management.\n  </Card>\n\n  <Card title=\"Privy\" href=\"/oss/python/integrations/providers/privy\" icon=\"link\">\n    Wallets and payments for AI agents.\n  </Card>\n\n  <Card title=\"Prolog\" href=\"/oss/python/integrations/providers/prolog\" icon=\"link\">\n    Logic programming language integration.\n  </Card>\n\n  <Card title=\"PromptLayer\" href=\"/oss/python/integrations/providers/promptlayer\" icon=\"link\">\n    Prompt engineering and observability platform.\n  </Card>\n\n  <Card title=\"Psychic\" href=\"/oss/python/integrations/providers/psychic\" icon=\"link\">\n    Universal API for SaaS integrations.\n  </Card>\n\n  <Card title=\"PubMed\" href=\"/oss/python/integrations/providers/pubmed\" icon=\"link\">\n    Biomedical literature database access.\n  </Card>\n\n  <Card title=\"Pull MD\" href=\"/oss/python/integrations/providers/pull-md\" icon=\"link\">\n    Markdown content extraction and processing.\n  </Card>\n\n  <Card title=\"PygmalionAI\" href=\"/oss/python/integrations/providers/pygmalionai\" icon=\"link\">\n    Conversational AI model platform.\n  </Card>\n\n  <Card title=\"PyMuPDF4LLM\" href=\"/oss/python/integrations/providers/pymupdf4llm\" icon=\"link\">\n    PDF processing optimized for LLM ingestion.\n  </Card>\n\n  <Card title=\"Qdrant\" href=\"/oss/python/integrations/providers/qdrant\" icon=\"link\">\n    Vector similarity search engine.\n  </Card>\n\n  <Card title=\"Ragatouille\" href=\"/oss/python/integrations/providers/ragatouille\" icon=\"link\">\n    RAG toolkit with ColBERT indexing.\n  </Card>\n\n  <Card title=\"Rank BM25\" href=\"/oss/python/integrations/providers/rank_bm25\" icon=\"link\">\n    BM25 ranking algorithm implementation.\n  </Card>\n\n  <Card title=\"Ray Serve\" href=\"/oss/python/integrations/providers/ray_serve\" icon=\"link\">\n    Scalable model serving framework.\n  </Card>\n\n  <Card title=\"Rebuff\" href=\"/oss/python/integrations/providers/rebuff\" icon=\"link\">\n    Prompt injection detection and prevention.\n  </Card>\n\n  <Card title=\"Reddit\" href=\"/oss/python/integrations/providers/reddit\" icon=\"link\">\n    Social media platform integration and APIs.\n  </Card>\n\n  <Card title=\"Redis\" href=\"/oss/python/integrations/providers/redis\" icon=\"link\">\n    In-memory data structure store and cache.\n  </Card>\n\n  <Card title=\"Remembrall\" href=\"/oss/python/integrations/providers/remembrall\" icon=\"link\">\n    AI memory and context management.\n  </Card>\n\n  <Card title=\"Replicate\" href=\"/oss/python/integrations/providers/replicate\" icon=\"link\">\n    Cloud platform for running ML models.\n  </Card>\n\n  <Card title=\"Roam\" href=\"/oss/python/integrations/providers/roam\" icon=\"link\">\n    Research and note-taking platform.\n  </Card>\n\n  <Card title=\"Robocorp\" href=\"/oss/python/integrations/providers/robocorp\" icon=\"link\">\n    Python automation and RPA platform.\n  </Card>\n\n  <Card title=\"Rockset\" href=\"/oss/python/integrations/providers/rockset\" icon=\"link\">\n    Real-time analytics database platform.\n  </Card>\n\n  <Card title=\"RunPod\" href=\"/oss/python/integrations/providers/runpod\" icon=\"link\">\n    GPU cloud platform for AI workloads.\n  </Card>\n\n  <Card title=\"Salesforce\" href=\"/oss/python/integrations/providers/salesforce\" icon=\"link\">\n    CRM platform and business automation.\n  </Card>\n\n  <Card title=\"SambaNova\" href=\"/oss/python/integrations/providers/sambanova\" icon=\"link\">\n    AI platform with specialized hardware.\n  </Card>\n\n  <Card title=\"SAP\" href=\"/oss/python/integrations/providers/sap\" icon=\"link\">\n    Enterprise software and AI solutions.\n  </Card>\n\n  <Card title=\"ScrapeGraph\" href=\"/oss/python/integrations/providers/scrapegraph\" icon=\"link\">\n    AI-powered web scraping framework.\n  </Card>\n\n  <Card title=\"Scrapeless\" href=\"/oss/python/integrations/providers/scrapeless\" icon=\"link\">\n    Web scraping API and proxy service.\n  </Card>\n\n  <Card title=\"SearchAPI\" href=\"/oss/python/integrations/providers/searchapi\" icon=\"link\">\n    Real-time search engine results API.\n  </Card>\n\n  <Card title=\"SearX\" href=\"/oss/python/integrations/providers/searx\" icon=\"link\">\n    Privacy-respecting metasearch engine.\n  </Card>\n\n  <Card title=\"SemaDB\" href=\"/oss/python/integrations/providers/semadb\" icon=\"link\">\n    Vector database for semantic search.\n  </Card>\n\n  <Card title=\"SerpAPI\" href=\"/oss/python/integrations/providers/serpapi\" icon=\"link\">\n    Google Search results scraping API.\n  </Card>\n\n  <Card title=\"Shale Protocol\" href=\"/oss/python/integrations/providers/shaleprotocol\" icon=\"link\">\n    Decentralized AI inference protocol.\n  </Card>\n\n  <Card title=\"SingleStore\" href=\"/oss/python/integrations/providers/singlestore\" icon=\"link\">\n    Distributed database with vector capabilities.\n  </Card>\n\n  <Card title=\"scikit-learn\" href=\"/oss/python/integrations/providers/sklearn\" icon=\"link\">\n    Machine learning library for Python.\n  </Card>\n\n  <Card title=\"Slack\" href=\"/oss/python/integrations/providers/slack\" icon=\"link\">\n    Business communication and collaboration.\n  </Card>\n\n  <Card title=\"Snowflake\" href=\"/oss/python/integrations/providers/snowflake\" icon=\"link\">\n    Cloud data platform and analytics.\n  </Card>\n\n  <Card title=\"spaCy\" href=\"/oss/python/integrations/providers/spacy\" icon=\"link\">\n    Industrial-strength NLP library.\n  </Card>\n\n  <Card title=\"Spark\" href=\"/oss/python/integrations/providers/spark\" icon=\"link\">\n    Unified analytics engine for big data.\n  </Card>\n\n  <Card title=\"SparkLLM\" href=\"/oss/python/integrations/providers/sparkllm\" icon=\"link\">\n    iFlytek's multilingual language model.\n  </Card>\n\n  <Card title=\"Spreedly\" href=\"/oss/python/integrations/providers/spreedly\" icon=\"link\">\n    Payment orchestration platform.\n  </Card>\n\n  <Card title=\"SQLite\" href=\"/oss/python/integrations/providers/sqlite\" icon=\"link\">\n    Embedded relational database engine.\n  </Card>\n\n  <Card title=\"StackExchange\" href=\"/oss/python/integrations/providers/stackexchange\" icon=\"link\">\n    Q\\&A platform network integration.\n  </Card>\n\n  <Card title=\"StarRocks\" href=\"/oss/python/integrations/providers/starrocks\" icon=\"link\">\n    High-performance analytical database.\n  </Card>\n\n  <Card title=\"StochasticAI\" href=\"/oss/python/integrations/providers/stochasticai\" icon=\"link\">\n    GPU cloud platform for ML acceleration.\n  </Card>\n\n  <Card title=\"Streamlit\" href=\"/oss/python/integrations/providers/streamlit\" icon=\"link\">\n    Web app framework for data science.\n  </Card>\n\n  <Card title=\"Stripe\" href=\"/oss/python/integrations/providers/stripe\" icon=\"link\">\n    Online payment processing platform.\n  </Card>\n\n  <Card title=\"Supabase\" href=\"/oss/python/integrations/providers/supabase\" icon=\"link\">\n    Open-source Firebase alternative.\n  </Card>\n\n  <Card title=\"SurrealDB\" href=\"/oss/python/integrations/providers/surrealdb\" icon=\"link\">\n    Multi-model database for modern applications.\n  </Card>\n\n  <Card title=\"Symbl.ai Nebula\" href=\"/oss/python/integrations/providers/symblai_nebula\" icon=\"link\">\n    Conversation intelligence platform.\n  </Card>\n\n  <Card title=\"Tableau\" href=\"/oss/python/integrations/providers/tableau\" icon=\"link\">\n    Data visualization and business intelligence.\n  </Card>\n\n  <Card title=\"Taiga\" href=\"/oss/python/integrations/providers/taiga\" icon=\"link\">\n    Project management platform for agile teams.\n  </Card>\n\n  <Card title=\"Tair\" href=\"/oss/python/integrations/providers/tair\" icon=\"link\">\n    Alibaba Cloud's in-memory database.\n  </Card>\n\n  <Card title=\"Tavily\" href=\"/oss/python/integrations/providers/tavily\" icon=\"link\">\n    AI-optimized search API for applications.\n  </Card>\n\n  <Card title=\"Telegram\" href=\"/oss/python/integrations/providers/telegram\" icon=\"link\">\n    Messaging platform and bot integration.\n  </Card>\n\n  <Card title=\"Tencent\" href=\"/oss/python/integrations/providers/tencent\" icon=\"link\">\n    Tencent Cloud AI services and models.\n  </Card>\n\n  <Card title=\"TensorFlow Datasets\" href=\"/oss/python/integrations/providers/tensorflow_datasets\" icon=\"link\">\n    Collection of ready-to-use datasets.\n  </Card>\n\n  <Card title=\"TensorLake\" href=\"/oss/python/integrations/providers/tensorlake\" icon=\"link\">\n    Data infrastructure for ML applications.\n  </Card>\n\n  <Card title=\"TiDB\" href=\"/oss/python/integrations/providers/tidb\" icon=\"link\">\n    Distributed SQL database platform.\n  </Card>\n\n  <Card title=\"TigerGraph\" href=\"/oss/python/integrations/providers/tigergraph\" icon=\"link\">\n    Scalable graph database and analytics.\n  </Card>\n\n  <Card title=\"Tigris\" href=\"/oss/python/integrations/providers/tigris\" icon=\"link\">\n    Globally distributed database platform.\n  </Card>\n\n  <Card title=\"Tilores\" href=\"/oss/python/integrations/providers/tilores\" icon=\"link\">\n    Entity resolution and data matching.\n  </Card>\n\n  <Card title=\"Timbr\" href=\"/oss/python/integrations/providers/timbr\" icon=\"link\">\n    Semantic layer for data integration and querying.\n  </Card>\n\n  <Card title=\"Together\" href=\"/oss/python/integrations/providers/together\" icon=\"link\">\n    Fast inference for open-source models.\n  </Card>\n\n  <Card title=\"ToMarkdown\" href=\"/oss/python/integrations/providers/tomarkdown\" icon=\"link\">\n    HTML to Markdown conversion utility.\n  </Card>\n\n  <Card title=\"Toolbox LangChain\" href=\"/oss/python/integrations/providers/toolbox\" icon=\"link\">\n    Extended toolkit for LangChain applications.\n  </Card>\n\n  <Card title=\"Transwarp\" href=\"/oss/python/integrations/providers/transwarp\" icon=\"link\">\n    Big data platform and analytics suite.\n  </Card>\n\n  <Card title=\"Trello\" href=\"/oss/python/integrations/providers/trello\" icon=\"link\">\n    Visual project management and collaboration.\n  </Card>\n\n  <Card title=\"Trubrics\" href=\"/oss/python/integrations/providers/trubrics\" icon=\"link\">\n    LLM evaluation and analytics platform.\n  </Card>\n\n  <Card title=\"TrueFoundry\" href=\"/oss/python/integrations/providers/truefoundry\" icon=\"link\">\n    ML platform for model deployment.\n  </Card>\n\n  <Card title=\"TrueLens\" href=\"/oss/python/integrations/providers/trulens\" icon=\"link\">\n    Evaluation framework for LLM applications.\n  </Card>\n\n  <Card title=\"Twitter\" href=\"/oss/python/integrations/providers/twitter\" icon=\"link\">\n    Social media platform integration.\n  </Card>\n\n  <Card title=\"Typesense\" href=\"/oss/python/integrations/providers/typesense\" icon=\"link\">\n    Fast and typo-tolerant search engine.\n  </Card>\n\n  <Card title=\"UnDatasIO\" href=\"/oss/python/integrations/providers/undatasio\" icon=\"link\">\n    Data extraction and processing platform.\n  </Card>\n\n  <Card title=\"Unstructured\" href=\"/oss/python/integrations/providers/unstructured\" icon=\"link\">\n    Document processing and data extraction.\n  </Card>\n\n  <Card title=\"Upstage\" href=\"/oss/python/integrations/providers/upstage\" icon=\"link\">\n    Document AI and OCR platform.\n  </Card>\n\n  <Card title=\"Upstash\" href=\"/oss/python/integrations/providers/upstash\" icon=\"link\">\n    Serverless data platform for Redis and Kafka.\n  </Card>\n\n  <Card title=\"UpTrain\" href=\"/oss/python/integrations/providers/uptrain\" icon=\"link\">\n    ML observability and evaluation platform.\n  </Card>\n\n  <Card title=\"USearch\" href=\"/oss/python/integrations/providers/usearch\" icon=\"link\">\n    Single-file vector search engine.\n  </Card>\n\n  <Card title=\"Valthera\" href=\"/oss/python/integrations/providers/valthera\" icon=\"link\">\n    AI platform for healthcare applications.\n  </Card>\n\n  <Card title=\"Valyu\" href=\"/oss/python/integrations/providers/valyu\" icon=\"link\">\n    AI-powered data analysis platform.\n  </Card>\n\n  <Card title=\"VDMS\" href=\"/oss/python/integrations/providers/vdms\" icon=\"link\">\n    Visual data management system.\n  </Card>\n\n  <Card title=\"Vearch\" href=\"/oss/python/integrations/providers/vearch\" icon=\"link\">\n    Distributed vector search engine.\n  </Card>\n\n  <Card title=\"Vectara\" href=\"/oss/python/integrations/providers/vectara\" icon=\"link\">\n    Neural search platform with built-in understanding.\n  </Card>\n\n  <Card title=\"Vectorize\" href=\"/oss/python/integrations/providers/vectorize\" icon=\"link\">\n    Vector database and semantic search.\n  </Card>\n\n  <Card title=\"Vespa\" href=\"/oss/python/integrations/providers/vespa\" icon=\"link\">\n    Big data serving engine for vector search.\n  </Card>\n\n  <Card title=\"VLite\" href=\"/oss/python/integrations/providers/vlite\" icon=\"link\">\n    Simple vector database for embeddings.\n  </Card>\n\n  <Card title=\"VoyageAI\" href=\"/oss/python/integrations/providers/voyageai\" icon=\"link\">\n    Embedding models and semantic search.\n  </Card>\n\n  <Card title=\"Weights & Biases\" href=\"/oss/python/integrations/providers/wandb\" icon=\"link\">\n    ML experiment tracking and collaboration.\n  </Card>\n\n  <Card title=\"Weights & Biases Tracking\" href=\"/oss/python/integrations/providers/wandb_tracking\" icon=\"link\">\n    Experiment tracking and model management.\n  </Card>\n\n  <Card title=\"Weights & Biases Tracing\" href=\"/oss/python/integrations/providers/wandb_tracing\" icon=\"link\">\n    LLM tracing and observability.\n  </Card>\n\n  <Card title=\"Weather\" href=\"/oss/python/integrations/providers/weather\" icon=\"link\">\n    Weather data and forecasting services.\n  </Card>\n\n  <Card title=\"Weaviate\" href=\"/oss/python/integrations/providers/weaviate\" icon=\"link\">\n    Open-source vector database with GraphQL.\n  </Card>\n\n  <Card title=\"WhatsApp\" href=\"/oss/python/integrations/providers/whatsapp\" icon=\"link\">\n    Messaging platform integration and automation.\n  </Card>\n\n  <Card title=\"WhyLabs Profiling\" href=\"/oss/python/integrations/providers/whylabs_profiling\" icon=\"link\">\n    AI observability and data monitoring.\n  </Card>\n\n  <Card title=\"Wikipedia\" href=\"/oss/python/integrations/providers/wikipedia\" icon=\"link\">\n    Wikipedia content access and search.\n  </Card>\n\n  <Card title=\"Wolfram Alpha\" href=\"/oss/python/integrations/providers/wolfram_alpha\" icon=\"link\">\n    Computational knowledge engine.\n  </Card>\n\n  <Card title=\"WRITER\" href=\"/oss/python/integrations/providers/writer\" icon=\"link\">\n    Enterprise models and tools for building, activating, and supervising AI agents.\n  </Card>\n\n  <Card title=\"XAI\" href=\"/oss/python/integrations/providers/xai\" icon=\"link\">\n    xAI's Grok models for conversational AI.\n  </Card>\n\n  <Card title=\"Xata\" href=\"/oss/python/integrations/providers/xata\" icon=\"link\">\n    Serverless database with vector search.\n  </Card>\n\n  <Card title=\"Xinference\" href=\"/oss/python/integrations/providers/xinference\" icon=\"link\">\n    Distributed inference framework for LLMs.\n  </Card>\n\n  <Card title=\"Yahoo\" href=\"/oss/python/integrations/providers/yahoo\" icon=\"link\">\n    Yahoo services and data integration.\n  </Card>\n\n  <Card title=\"Yandex\" href=\"/oss/python/integrations/providers/yandex\" icon=\"link\">\n    Yandex AI services and language models.\n  </Card>\n\n  <Card title=\"YDB\" href=\"/oss/python/integrations/providers/ydb\" icon=\"link\">\n    Yandex Database distributed storage system.\n  </Card>\n\n  <Card title=\"YeagerAI\" href=\"/oss/python/integrations/providers/yeagerai\" icon=\"link\">\n    AI agent framework and development platform.\n  </Card>\n\n  <Card title=\"Yellowbrick\" href=\"/oss/python/integrations/providers/yellowbrick\" icon=\"link\">\n    Data warehouse and analytics platform.\n  </Card>\n\n  <Card title=\"Yi\" href=\"/oss/python/integrations/providers/yi\" icon=\"link\">\n    01.AI's bilingual language models.\n  </Card>\n\n  <Card title=\"You\" href=\"/oss/python/integrations/providers/you\" icon=\"link\">\n    You.com search engine and AI platform.\n  </Card>\n\n  <Card title=\"YouTube\" href=\"/oss/python/integrations/providers/youtube\" icon=\"link\">\n    Video platform integration and content access.\n  </Card>\n\n  <Card title=\"Zep\" href=\"/oss/python/integrations/providers/zep\" icon=\"link\">\n    Long-term memory for AI assistants.\n  </Card>\n\n  <Card title=\"ZeusDB\" href=\"/oss/python/integrations/providers/zeusdb\" icon=\"link\">\n    High-performance vector database.\n  </Card>\n\n  <Card title=\"ZhipuAI\" href=\"/oss/python/integrations/providers/zhipuai\" icon=\"link\">\n    ChatGLM and other Chinese language models.\n  </Card>\n\n  <Card title=\"Zilliz\" href=\"/oss/python/integrations/providers/zilliz\" icon=\"link\">\n    Managed Milvus vector database service.\n  </Card>\n\n  <Card title=\"Zotero\" href=\"/oss/python/integrations/providers/zotero\" icon=\"link\">\n    Reference management and research tool.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/all_providers.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 57018,
    "word_count": 4099
  },
  {
    "title": "Anthropic (Claude)",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/anthropic",
    "content": "This page covers all LangChain integrations with [Anthropic](https://www.anthropic.com/), the makers of Claude.\n\n## Model interfaces\n\n<Columns cols={2}>\n  <Card title=\"ChatAnthropic\" href=\"/oss/python/integrations/chat/anthropic\" cta=\"Get started\" icon=\"message\" arrow>\n    Anthropic chat models.\n  </Card>\n\n  <Card title=\"AnthropicLLM\" href=\"/oss/python/integrations/llms/anthropic\" cta=\"Get started\" icon=\"i-cursor\" arrow>\n    (Legacy) Anthropic text completion models.\n  </Card>\n</Columns>\n\n## Middleware\n\nMiddleware specifically designed for Anthropic's Claude models. Learn more about [middleware](/oss/python/langchain/middleware/overview).\n\n| Middleware                        | Description                                                    |\n| --------------------------------- | -------------------------------------------------------------- |\n| [Prompt caching](#prompt-caching) | Reduce costs by caching repetitive prompt prefixes             |\n| [Bash tool](#bash-tool)           | Execute Claude's native bash tool with local command execution |\n| [Text editor](#text-editor)       | Provide Claude's text editor tool for file editing             |\n| [Memory](#memory)                 | Provide Claude's memory tool for persistent agent memory       |\n| [File search](#file-search)       | Search tools for state-based file systems                      |\n\n### Prompt caching\n\nReduce costs and latency by caching static or repetitive prompt content (like system prompts, tool definitions, and conversation history) on Anthropic's servers. This middleware implements a **conversational caching strategy** that places cache breakpoints after the most recent message, allowing the entire conversation history (including the latest user message) to be cached and reused in subsequent API calls. Prompt caching is useful for the following:\n\n* Applications with long, static system prompts that don't change between requests\n* Agents with many tool definitions that remain constant across invocations\n* Conversations where early message history is reused across multiple turns\n* High-volume deployments where reducing API costs and latency is critical\n\n<Info>\n  Learn more about [Anthropic prompt caching](https://platform.claude.com/docs/en/build-with-claude/prompt-caching#cache-limitations) strategies and limitations.\n</Info>\n\n**API reference:** [`AnthropicPromptCachingMiddleware`](https://reference.langchain.com/python/integrations/langchain_anthropic/middleware/#langchain_anthropic.middleware.AnthropicPromptCachingMiddleware)\n\n```python  theme={null}\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_anthropic.middleware import AnthropicPromptCachingMiddleware\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n    system_prompt=\"<Your long system prompt here>\",\n    middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"type\" type=\"string\" default=\"ephemeral\">\n    Cache type. Only `'ephemeral'` is currently supported.\n  </ParamField>\n\n  <ParamField body=\"ttl\" type=\"string\" default=\"5m\">\n    Time to live for cached content. Valid values: `'5m'` or `'1h'`\n  </ParamField>\n\n  <ParamField body=\"min_messages_to_cache\" type=\"number\" default=\"0\">\n    Minimum number of messages before caching starts\n  </ParamField>\n\n  <ParamField body=\"unsupported_model_behavior\" type=\"string\" default=\"warn\">\n    Behavior when using non-Anthropic models. Options: `'ignore'`, `'warn'`, or `'raise'`\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware caches content up to and including the latest message in each request. On subsequent requests within the TTL window (5 minutes or 1 hour), previously seen content is retrieved from cache rather than reprocessed, significantly reducing costs and latency.\n\n  **How it works:**\n\n  1. First request: System prompt, tools, and the user message \"Hi, my name is Bob\" are sent to the API and cached\n  2. Second request: The cached content (system prompt, tools, and first message) is retrieved from cache. Only the new message \"What's my name?\" needs to be processed, plus the model's response from the first request\n  3. This pattern continues for each turn, with each request reusing the cached conversation history\n\n  ```python  theme={null}\n  from langchain_anthropic import ChatAnthropic\n  from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware\n  from langchain.agents import create_agent\n  from langchain.messages import HumanMessage\n\n\n  LONG_PROMPT = \"\"\"\n  Please be a helpful assistant.\n\n  <Lots more context ...>\n  \"\"\"\n\n  agent = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      system_prompt=LONG_PROMPT,\n      middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")],\n  )\n\n  # First invocation: Creates cache with system prompt, tools, and \"Hi, my name is Bob\"\n  agent.invoke({\"messages\": [HumanMessage(\"Hi, my name is Bob\")]})\n\n  # Second invocation: Reuses cached system prompt, tools, and previous messages\n  # Only processes the new message \"What's my name?\" and the previous AI response\n  agent.invoke({\"messages\": [HumanMessage(\"What's my name?\")]})\n  ```\n</Accordion>\n\n### Bash tool\n\nExecute Claude's native `bash_20250124` tool with local command execution. The bash tool middleware is useful for the following:\n\n* Using Claude's built-in bash tool with local execution\n* Leveraging Claude's optimized bash tool interface\n* Agents that need persistent shell sessions with Anthropic models\n\n<Info>\n  This middleware wraps `ShellToolMiddleware` and exposes it as Claude's native bash tool.\n</Info>\n\n**API reference:** [`ClaudeBashToolMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClaudeBashToolMiddleware)\n\n```python  theme={null}\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_anthropic.middleware import ClaudeBashToolMiddleware\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n    tools=[],\n    middleware=[\n        ClaudeBashToolMiddleware(\n            workspace_root=\"/workspace\",\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  `ClaudeBashToolMiddleware` accepts all parameters from [`ShellToolMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ShellToolMiddleware), including:\n\n  <ParamField body=\"workspace_root\" type=\"str | Path | None\">\n    Base directory for the shell session\n  </ParamField>\n\n  <ParamField body=\"startup_commands\" type=\"tuple[str, ...] | list[str] | str | None\">\n    Commands to run when the session starts\n  </ParamField>\n\n  <ParamField body=\"execution_policy\" type=\"BaseExecutionPolicy | None\">\n    Execution policy (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`)\n  </ParamField>\n\n  <ParamField body=\"redaction_rules\" type=\"tuple[RedactionRule, ...] | list[RedactionRule] | None\">\n    Rules for sanitizing command output\n  </ParamField>\n\n  See [Shell tool](/oss/python/langchain/middleware/built-in#shell-tool) for full configuration details.\n</Accordion>\n\n<Accordion title=\"Full example\">\n  ```python  theme={null}\n  from langchain_anthropic import ChatAnthropic\n  from langchain_anthropic.middleware import ClaudeBashToolMiddleware\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import DockerExecutionPolicy\n\n\n  agent = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      tools=[],\n      middleware=[\n          ClaudeBashToolMiddleware(\n              workspace_root=\"/workspace\",\n              startup_commands=[\"pip install requests\"],\n              execution_policy=DockerExecutionPolicy(\n                  image=\"python:3.11-slim\",\n              ),\n          ),\n      ],\n  )\n\n  # Claude can now use its native bash tool\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"List files in the workspace\"}]\n  })\n  ```\n</Accordion>\n\n### Text editor\n\nProvide Claude's text editor tool (`text_editor_20250728`) for file creation and editing. The text editor middleware is useful for the following:\n\n* File-based agent workflows\n* Code editing and refactoring tasks\n* Multi-file project work\n* Agents that need persistent file storage\n\n<Note>\n  Available in two variants: **State-based** (files in LangGraph state) and **Filesystem-based** (files on disk).\n</Note>\n\n**API reference:** [`StateClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeTextEditorMiddleware), [`FilesystemClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeTextEditorMiddleware)\n\n```python  theme={null}\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_anthropic.middleware import StateClaudeTextEditorMiddleware\nfrom langchain.agents import create_agent\n\n# State-based (files in LangGraph state)\nagent = create_agent(\n    model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n    tools=[],\n    middleware=[\n        StateClaudeTextEditorMiddleware(),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  **[`StateClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeTextEditorMiddleware) (state-based)**\n\n  <ParamField body=\"allowed_path_prefixes\" type=\"Sequence[str] | None\">\n    Optional list of allowed path prefixes. If specified, only paths starting with these prefixes are allowed.\n  </ParamField>\n\n  **[`FilesystemClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeTextEditorMiddleware) (filesystem-based)**\n\n  <ParamField body=\"root_path\" type=\"str\" required>\n    Root directory for file operations\n  </ParamField>\n\n  <ParamField body=\"allowed_prefixes\" type=\"list[str] | None\">\n    Optional list of allowed virtual path prefixes (default: `[\"/\"]`)\n  </ParamField>\n\n  <ParamField body=\"max_file_size_mb\" type=\"int\" default=\"10\">\n    Maximum file size in MB\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  Claude's text editor tool supports the following commands:\n\n  * `view` - View file contents or list directory\n  * `create` - Create a new file\n  * `str_replace` - Replace string in file\n  * `insert` - Insert text at line number\n  * `delete` - Delete a file\n  * `rename` - Rename/move a file\n\n  ```python  theme={null}\n  from langchain_anthropic import ChatAnthropic\n  from langchain_anthropic.middleware import (\n      StateClaudeTextEditorMiddleware,\n      FilesystemClaudeTextEditorMiddleware,\n  )\n  from langchain.agents import create_agent\n\n\n  # State-based: Files persist in LangGraph state\n  agent_state = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      tools=[],\n      middleware=[\n          StateClaudeTextEditorMiddleware(\n              allowed_path_prefixes=[\"/project\"],\n          ),\n      ],\n  )\n\n  # Filesystem-based: Files persist on disk\n  agent_fs = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      tools=[],\n      middleware=[\n          FilesystemClaudeTextEditorMiddleware(\n              root_path=\"/workspace\",\n              allowed_prefixes=[\"/src\"],\n              max_file_size_mb=10,\n          ),\n      ],\n  )\n  ```\n</Accordion>\n\n### Memory\n\nProvide Claude's memory tool (`memory_20250818`) for persistent agent memory across conversation turns. The memory middleware is useful for the following:\n\n* Long-running agent conversations\n* Maintaining context across interruptions\n* Task progress tracking\n* Persistent agent state management\n\n<Info>\n  Claude's memory tool uses a `/memories` directory and automatically injects a system prompt encouraging the agent to check and update memory.\n</Info>\n\n**API reference:** [`StateClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeMemoryMiddleware), [`FilesystemClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeMemoryMiddleware)\n\n```python  theme={null}\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_anthropic.middleware import StateClaudeMemoryMiddleware\nfrom langchain.agents import create_agent\n\n# State-based memory\nagent = create_agent(\n    model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n    tools=[],\n    middleware=[\n        StateClaudeMemoryMiddleware(),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  **[`StateClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeMemoryMiddleware) (state-based)**\n\n  <ParamField body=\"allowed_path_prefixes\" type=\"Sequence[str] | None\">\n    Optional list of allowed path prefixes. Defaults to `[\"/memories\"]`.\n  </ParamField>\n\n  <ParamField body=\"system_prompt\" type=\"str\">\n    System prompt to inject. Defaults to Anthropic's recommended memory prompt that encourages the agent to check and update memory.\n  </ParamField>\n\n  **[`FilesystemClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeMemoryMiddleware) (filesystem-based)**\n\n  <ParamField body=\"root_path\" type=\"str\" required>\n    Root directory for file operations\n  </ParamField>\n\n  <ParamField body=\"allowed_prefixes\" type=\"list[str] | None\">\n    Optional list of allowed virtual path prefixes. Defaults to `[\"/memories\"]`.\n  </ParamField>\n\n  <ParamField body=\"max_file_size_mb\" type=\"int\" default=\"10\">\n    Maximum file size in MB\n  </ParamField>\n\n  <ParamField body=\"system_prompt\" type=\"str\">\n    System prompt to inject\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  ```python  theme={null}\n  from langchain_anthropic import ChatAnthropic\n  from langchain_anthropic.middleware import (\n      StateClaudeMemoryMiddleware,\n      FilesystemClaudeMemoryMiddleware,\n  )\n  from langchain.agents import create_agent\n\n\n  # State-based: Memory persists in LangGraph state\n  agent_state = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      tools=[],\n      middleware=[\n          StateClaudeMemoryMiddleware(),\n      ],\n  )\n\n  # Filesystem-based: Memory persists on disk\n  agent_fs = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      tools=[],\n      middleware=[\n          FilesystemClaudeMemoryMiddleware(\n              root_path=\"/workspace\",\n          ),\n      ],\n  )\n\n  # The agent will automatically:\n  # 1. Check /memories directory at start\n  # 2. Record progress and thoughts during execution\n  # 3. Update memory files as work progresses\n  ```\n</Accordion>\n\n### File search\n\nProvide Glob and Grep search tools for files stored in LangGraph state. File search middleware is useful for the following:\n\n* Searching through state-based virtual file systems\n* Works with text editor and memory tools\n* Finding files by patterns\n* Content search with regex\n\n**API reference:** [`StateFileSearchMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateFileSearchMiddleware)\n\n```python  theme={null}\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_anthropic.middleware import (\n    StateClaudeTextEditorMiddleware,\n    StateFileSearchMiddleware,\n)\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n    tools=[],\n    middleware=[\n        StateClaudeTextEditorMiddleware(),\n        StateFileSearchMiddleware(),  # Search text editor files\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"state_key\" type=\"str\" default=\"text_editor_files\">\n    State key containing files to search. Use `\"text_editor_files\"` for text editor files or `\"memory_files\"` for memory files.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware adds Glob and Grep search tools that work with state-based files.\n\n  ```python  theme={null}\n  from langchain_anthropic import ChatAnthropic\n  from langchain_anthropic.middleware import (\n      StateClaudeTextEditorMiddleware,\n      StateClaudeMemoryMiddleware,\n      StateFileSearchMiddleware,\n  )\n  from langchain.agents import create_agent\n\n\n  # Search text editor files\n  agent = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      tools=[],\n      middleware=[\n          StateClaudeTextEditorMiddleware(),\n          StateFileSearchMiddleware(state_key=\"text_editor_files\"),\n      ],\n  )\n\n  # Search memory files\n  agent_memory = create_agent(\n      model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n      tools=[],\n      middleware=[\n          StateClaudeMemoryMiddleware(),\n          StateFileSearchMiddleware(state_key=\"memory_files\"),\n      ],\n  )\n  ```\n</Accordion>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/anthropic.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 17466,
    "word_count": 1517
  },
  {
    "title": "AWS (Amazon)",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/aws",
    "content": "This page covers all LangChain integrations with the [Amazon Web Services (AWS)](https://aws.amazon.com/) platform.\n\n## Chat models\n\n### Bedrock Chat\n\n> [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of\n> high-performing foundation models (FMs) from leading AI companies like `AI21 Labs`, `Anthropic`, `Cohere`,\n> `Meta`, `Stability AI`, and `Amazon` via a single API, along with a broad set of capabilities you need to\n> build generative AI applications with security, privacy, and responsible AI. Using `Amazon Bedrock`,\n> you can easily experiment with and evaluate top FMs for your use case, privately customize them with\n> your data using techniques such as fine-tuning and `Retrieval Augmented Generation` (`RAG`), and build\n> agents that execute tasks using your enterprise systems and data sources. Since `Amazon Bedrock` is\n> serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy\n> generative AI capabilities into your applications using the AWS services you are already familiar with.\n\nSee a [usage example](/oss/python/integrations/chat/bedrock).\n\n```python  theme={null}\nfrom langchain_aws import ChatBedrock\n```\n\n### Bedrock Converse\n\nAWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)\nthat provides a unified conversational interface for Bedrock models. This API does not\nyet support custom models. You can see a list of all\n[models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n\n<Info>\n  **We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**\n</Info>\n\nSee a [usage example](/oss/python/integrations/chat/bedrock).\n\n```python  theme={null}\nfrom langchain_aws import ChatBedrockConverse\n```\n\n## LLMs\n\n### Bedrock\n\nSee a [usage example](/oss/python/integrations/llms/bedrock).\n\n```python  theme={null}\nfrom langchain_aws import BedrockLLM\n```\n\n### Amazon API Gateway\n\n> [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for\n> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\"\n> for applications to access data, business logic, or functionality from your backend services. Using\n> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication\n> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.\n>\n> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of\n> concurrent API calls, including traffic management, CORS support, authorization and access control,\n> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs.\n> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway`\n> tiered pricing model, you can reduce your cost as your API usage scales.\n\nSee a [usage example](/oss/python/integrations/llms/amazon_api_gateway).\n\n```python  theme={null}\nfrom langchain_community.llms import AmazonAPIGateway\n```\n\n### SageMaker Endpoint\n\n> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy\n> machine learning (ML) models with fully managed infrastructure, tools, and workflows.\n\nWe use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.\n\nSee a [usage example](/oss/python/integrations/llms/sagemaker).\n\n```python  theme={null}\nfrom langchain_aws import SagemakerEndpoint\n```\n\n## Embedding Models\n\n### Bedrock\n\nSee a [usage example](/oss/python/integrations/text_embedding/bedrock).\n\n```python  theme={null}\nfrom langchain_aws import BedrockEmbeddings\n```\n\n### SageMaker Endpoint\n\nSee a [usage example](/oss/python/integrations/text_embedding/sagemaker-endpoint).\n\n```python  theme={null}\nfrom langchain_community.embeddings import SagemakerEndpointEmbeddings\nfrom langchain_community.llms.sagemaker_endpoint import ContentHandlerBase\n```\n\n## Document loaders\n\n### AWS S3 Directory and File\n\n> [Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)\n> is an object storage service.\n> [AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)\n> [AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)\n\nSee a [usage example for S3DirectoryLoader](/oss/python/integrations/document_loaders/aws_s3_directory).\n\nSee a [usage example for S3FileLoader](/oss/python/integrations/document_loaders/aws_s3_file).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n```\n\n### Amazon Textract\n\n> [Amazon Textract](https://docs.aws.amazon.com/managedservices/latest/userguide/textract.html) is a machine\n> learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.\n\nSee a [usage example](/oss/python/integrations/document_loaders/amazon_textract).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import AmazonTextractPDFLoader\n```\n\n### Amazon Athena\n\n> [Amazon Athena](https://aws.amazon.com/athena/) is a serverless, interactive analytics service built\n> on open-source frameworks, supporting open-table and file formats.\n\nSee a [usage example](/oss/python/integrations/document_loaders/athena).\n\n```python  theme={null}\nfrom langchain_community.document_loaders.athena import AthenaLoader\n```\n\n### AWS Glue\n\n> The [AWS Glue Data Catalog](https://docs.aws.amazon.com/en_en/glue/latest/dg/catalog-and-crawler.html) is a centralized metadata\n> repository that allows you to manage, access, and share metadata about\n> your data stored in AWS. It acts as a metadata store for your data assets,\n> enabling various AWS services and your applications to query and connect\n> to the data they need efficiently.\n\nSee a [usage example](/oss/python/integrations/document_loaders/glue_catalog).\n\n```python  theme={null}\nfrom langchain_community.document_loaders.glue_catalog import GlueCatalogLoader\n```\n\n## Vector stores\n\n### Amazon OpenSearch Service\n\n> [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) performs\n> interactive log analytics, real-time application monitoring, website search, and more. `OpenSearch` is\n> an open source,\n> distributed search and analytics suite derived from `Elasticsearch`. `Amazon OpenSearch Service` offers the\n> latest versions of `OpenSearch`, support for many versions of `Elasticsearch`, as well as\n> visualization capabilities powered by `OpenSearch Dashboards` and `Kibana`.\n\nWe need to install several python libraries.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install boto3 requests requests-aws4auth\n  ```\n\n  ```bash uv theme={null}\n  uv add boto3 requests requests-aws4auth\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/vectorstores/opensearch#using-aos-amazon-opensearch-service).\n\n```python  theme={null}\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\n```\n\n### Amazon DocumentDB Vector Search\n\n> [Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\n> With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.\n> Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.\n\n#### Installation and Setup\n\nSee [detail configuration instructions](/oss/python/integrations/vectorstores/documentdb).\n\nWe need to install the `pymongo` python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install pymongo\n  ```\n\n  ```bash uv theme={null}\n  uv add pymongo\n  ```\n</CodeGroup>\n\n#### Deploy DocumentDB on AWS\n\n[Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\n\nAWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see [Cloud Computing with Amazon Web Services](https://aws.amazon.com/what-is-aws/).\n\nSee a [usage example](/oss/python/integrations/vectorstores/documentdb).\n\n```python  theme={null}\nfrom langchain_community.vectorstores import DocumentDBVectorSearch\n```\n\n### Amazon MemoryDB\n\n[Amazon MemoryDB](https://aws.amazon.com/memorydb/) is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store,\nenabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.\n\nInMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.\n\n```python  theme={null}\nfrom langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore\n\nvds = InMemoryVectorStore.from_documents(\n            chunks,\n            embeddings,\n            redis_url=\"rediss://cluster_endpoint:6379/ssl=True ssl_cert_reqs=none\",\n            vector_schema=vector_schema,\n            index_name=INDEX_NAME,\n        )\n```\n\nSee a [usage example](/oss/python/integrations/vectorstores/memorydb).\n\n## Retrievers\n\n### Amazon Kendra\n\n> [Amazon Kendra](https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html) is an intelligent search service\n> provided by `Amazon Web Services` (`AWS`). It utilizes advanced natural language processing (NLP) and machine\n> learning algorithms to enable powerful search capabilities across various data sources within an organization.\n> `Kendra` is designed to help users find the information they need quickly and accurately,\n> improving productivity and decision-making.\n\n> With `Kendra`, we can search across a wide range of content types, including documents, FAQs, knowledge bases,\n> manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and\n> contextual meanings to provide highly relevant search results.\n\nWe need to install the `langchain-aws` library.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-aws\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-aws\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/retrievers/amazon_kendra_retriever).\n\n```python  theme={null}\nfrom langchain_aws import AmazonKendraRetriever\n```\n\n### Amazon Bedrock (Knowledge Bases)\n\n> [Knowledge bases for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) is an\n> `Amazon Web Services` (`AWS`) offering which lets you quickly build RAG applications by using your\n> private data to customize foundation model response.\n\nWe need to install the `langchain-aws` library.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-aws\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-aws\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/retrievers/bedrock).\n\n```python  theme={null}\nfrom langchain_aws import AmazonKnowledgeBasesRetriever\n```\n\n## Tools\n\n### AWS Lambda\n\n> [`Amazon AWS Lambda`](https://aws.amazon.com/pm/lambda/) is a serverless computing service provided by\n> `Amazon Web Services` (`AWS`). It helps developers to build and run applications and services without\n> provisioning or managing servers. This serverless architecture enables you to focus on writing and\n> deploying code, while AWS automatically takes care of scaling, patching, and managing the\n> infrastructure required to run your applications.\n\nWe need to install `boto3` python library.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install boto3\n  ```\n\n  ```bash uv theme={null}\n  uv add boto3\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/awslambda).\n\n```python  theme={null}\nfrom langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n```\n\n## Graphs\n\n### Amazon Neptune\n\n> [Amazon Neptune](https://aws.amazon.com/neptune/)\n> is a high-performance graph analytics and serverless database for superior scalability and availability.\n\nFor the Cypher and SPARQL integrations below, we need to install the `langchain-aws` library.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-aws\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-aws\n  ```\n</CodeGroup>\n\n### Amazon Neptune with Cypher\n\nSee a [usage example](/oss/python/integrations/graphs/amazon_neptune_open_cypher).\n\n```python  theme={null}\nfrom langchain_aws.graphs import NeptuneGraph\nfrom langchain_aws.graphs import NeptuneAnalyticsGraph\nfrom langchain_aws.chains import create_neptune_opencypher_qa_chain\n```\n\n### Amazon Neptune with SPARQL\n\n```python  theme={null}\nfrom langchain_aws.graphs import NeptuneRdfGraph\nfrom langchain_aws.chains import create_neptune_sparql_qa_chain\n```\n\n## Callbacks\n\n### Bedrock token usage\n\n```python  theme={null}\nfrom langchain_community.callbacks.bedrock_anthropic_callback import BedrockAnthropicTokenUsageCallbackHandler\n```\n\n### SageMaker Tracking\n\n> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service that is used to quickly\n> and easily build, train and deploy machine learning (ML) models.\n\n> [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) is a capability\n> of `Amazon SageMaker` that lets you organize, track,\n> compare and evaluate ML experiments and model versions.\n\nWe need to install several python libraries.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install google-search-results sagemaker\n  ```\n\n  ```bash uv theme={null}\n  uv add google-search-results sagemaker\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/callbacks/sagemaker_tracking).\n\n```python  theme={null}\nfrom langchain_community.callbacks import SageMakerCallbackHandler\n```\n\n## Chains\n\n### Amazon Comprehend Moderation Chain\n\n> [Amazon Comprehend](https://aws.amazon.com/comprehend/) is a natural-language processing (NLP) service that\n> uses machine learning to uncover valuable insights and connections in text.\n\nWe need to install the `boto3` and `nltk` libraries.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install boto3 nltk\n  ```\n\n  ```bash uv theme={null}\n  uv add boto3 nltk\n  ```\n</CodeGroup>\n\nSee a [usage example](https://python.langchain.com/v0.1/docs/guides/productionization/safety/amazon_comprehend_chain/).\n\n```python  theme={null}\nfrom langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/aws.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 15286,
    "word_count": 1672
  },
  {
    "title": "Google",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/google",
    "content": "This page covers all LangChain integrations with [Google Gemini](https://ai.google.dev/gemini-api/docs), [Google Cloud](https://cloud.google.com/), and other Google products (such as Google Maps, YouTube, and [more](#other-google-products)).\n\nNot sure which to use?\n\n<AccordionGroup>\n  <Accordion title=\"Google Generative AI (Gemini API & AI Studio)\">\n    Access Google Gemini models directly via the [Gemini Developer API](https://ai.google.dev/). This is often the best starting point for individual developers.\n\n    [See integrations.](#google-generative-ai)\n  </Accordion>\n\n  <Accordion title=\"Google Cloud (Vertex AI & other services)\">\n    Access Gemini models, Vertex AI Model Garden and a wide range of cloud services (databases, storage, document AI, etc.) via the [Google Cloud Platform](https://cloud.google.com/). Use the `langchain-google-vertexai` package for Vertex AI models and specific packages (e.g., `langchain-google-community`, `langchain-google-cloud-sql-pg`) for other cloud services. This is ideal for developers already using Google Cloud or needing enterprise features like MLOps, specific model tuning or enterprise support.\n\n    [See integrations.](#google-cloud)\n  </Accordion>\n</AccordionGroup>\n\nSee Google's guide on [migrating from the Gemini API to Vertex AI](https://ai.google.dev/gemini-api/docs/migrate-to-cloud) for more details on the differences.\n\n<Note>\n  Integration packages for Gemini models and the Vertex AI platform are maintained in the [`langchain-google`](https://github.com/langchain-ai/langchain-google) repository.\n\n  You can find a host of LangChain integrations with other Google APIs and services in the `langchain-google-community` package (listed on this page) and the [`googleapis`](https://github.com/orgs/googleapis/repositories?q=langchain) Github organization.\n</Note>\n\n***\n\n## Google Generative AI\n\nAccess Google Gemini models directly using the [Gemini Developer API](https://ai.google.dev/gemini-api/docs), best suited for rapid development and experimentation.\n\n### Chat models\n\n<Columns cols={1}>\n  <Card title=\"ChatGoogleGenerativeAI\" href=\"/oss/python/integrations/chat/google_generative_ai\" cta=\"Get started\" icon=\"message\" arrow>\n    Google Gemini chat models via the Gemini Developer API.\n  </Card>\n</Columns>\n\n### LLMs\n\n<Columns cols={1}>\n  <Card title=\"GoogleGenerativeAI\" href=\"/oss/python/integrations/llms/google_ai\" cta=\"Get started\" icon=\"i-cursor\" arrow>\n    Access the same Gemini models using the (legacy) LLM text completion interface.\n  </Card>\n</Columns>\n\n### Embedding models\n\n<Columns cols={1}>\n  <Card title=\"GoogleGenerativeAIEmbeddings\" href=\"/oss/python/integrations/text_embedding/google_generative_ai\" cta=\"Get started\" icon=\"layer-group\" arrow>\n    Gemini embedding models.\n  </Card>\n</Columns>\n\n***\n\n## Google Cloud\n\nAccess Gemini models, Vertex AI Model Garden and other Google Cloud services via [Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs) and specific cloud integrations.\n\n### Chat models\n\n<Columns cols={2}>\n  <Card title=\"Vertex AI\" icon=\"comments\" href=\"/oss/python/integrations/chat/google_vertex_ai\" cta=\"Get started\" arrow>\n    Access chat models like Gemini via the Vertex AI platform.\n  </Card>\n\n  <Card title=\"Anthropic on Vertex AI Model Garden\" icon=\"comments\" href=\"/oss/python/integrations/llms/google_vertex_ai\" cta=\"Get started\" arrow />\n</Columns>\n\n<AccordionGroup>\n  <Accordion title=\"Llama on Vertex AI Model Garden\">\n    ```python wrap theme={null}\n    from langchain_google_vertexai.model_garden_maas.llama import VertexModelGardenLlama\n    ```\n  </Accordion>\n\n  <Accordion title=\"Mistral on Vertex AI Model Garden\">\n    ```python wrap theme={null}\n    from langchain_google_vertexai.model_garden_maas.mistral import VertexModelGardenMistral\n    ```\n  </Accordion>\n\n  <Accordion title=\"Gemma local from Hugging Face\">\n    Local Gemma model loaded from HuggingFace.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.gemma import GemmaChatLocalHF\n    ```\n  </Accordion>\n\n  <Accordion title=\"Gemma local from Kaggle\">\n    Local Gemma model loaded from Kaggle.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.gemma import GemmaChatLocalKaggle\n    ```\n  </Accordion>\n\n  <Accordion title=\"Gemma on Vertex AI Model Garden\">\n    ```python wrap theme={null}\n    from langchain_google_vertexai.gemma import GemmaChatVertexAIModelGarden\n    ```\n  </Accordion>\n\n  <Accordion title=\"Vertex AI image captioning\">\n    Implementation of the Image Captioning model as a chat.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.vision_models import VertexAIImageCaptioningChat\n    ```\n  </Accordion>\n\n  <Accordion title=\"Vertex AI image editor\">\n    Given an image and a prompt, edit the image. Currently only supports mask-free editing.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.vision_models import VertexAIImageEditorChat\n    ```\n  </Accordion>\n\n  <Accordion title=\"Vertex AI image generator\">\n    Generates an image from a prompt.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat\n    ```\n  </Accordion>\n\n  <Accordion title=\"Vertex AI visual QnA\">\n    Chat implementation of a visual QnA model.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.vision_models import VertexAIVisualQnAChat\n    ```\n  </Accordion>\n</AccordionGroup>\n\n### LLMs\n\n(legacy) string-in, string-out LLM interface.\n\n<Columns cols={2}>\n  <Card title=\"Vertex AI Model Garden\" icon=\"i-cursor\" href=\"/oss/python/integrations/llms/google_vertex_ai#vertex-model-garden\" cta=\"Get started\" arrow>\n    Access Gemini, and hundreds of OSS models via Vertex AI Model Garden service.\n  </Card>\n</Columns>\n\nGemma:\n\n<AccordionGroup>\n  <Accordion title=\"Gemma local from Hugging Face\">\n    Local Gemma model loaded from HuggingFace.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.gemma import GemmaLocalHF\n    ```\n  </Accordion>\n\n  <Accordion title=\"Gemma local from Kaggle\">\n    Local Gemma model loaded from Kaggle.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.gemma import GemmaLocalKaggle\n    ```\n  </Accordion>\n\n  <Accordion title=\"Gemma on Vertex AI Model Garden\">\n    ```python wrap theme={null}\n    from langchain_google_vertexai.gemma import GemmaVertexAIModelGarden\n    ```\n  </Accordion>\n\n  <Accordion title=\"Vertex AI image captioning\">\n    Implementation of the Image Captioning model as an LLM.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.vision_models import VertexAIImageCaptioning\n    ```\n  </Accordion>\n</AccordionGroup>\n\n### Embedding models\n\n<Columns cols={2}>\n  <Card title=\"Vertex AI\" icon=\"layer-group\" href=\"/oss/python/integrations/text_embedding/google_vertex_ai\" cta=\"Get started\" arrow>\n    Generate embeddings using models deployed on Vertex AI.\n  </Card>\n</Columns>\n\n### Document loaders\n\nLoad documents from various Google Cloud sources.\n\n<Columns cols={2}>\n  <Card title=\"AlloyDB for PostgreSQL\" href=\"/oss/python/integrations/document_loaders/google_alloydb\" cta=\"Get started\" arrow>\n    Google Cloud AlloyDB is a fully managed PostgreSQL-compatible database service.\n  </Card>\n\n  <Card title=\"BigQuery\" href=\"/oss/python/integrations/document_loaders/google_bigquery\" cta=\"Get started\" arrow>\n    Google Cloud BigQuery is a serverless data warehouse.\n  </Card>\n\n  <Card title=\"Bigtable\" href=\"/oss/python/integrations/document_loaders/google_bigtable\" cta=\"Get started\" arrow>\n    Google Cloud Bigtable is a fully managed NoSQL Big Data database service.\n  </Card>\n\n  <Card title=\"Cloud SQL for MySQL\" href=\"/oss/python/integrations/document_loaders/google_cloud_sql_mysql\" cta=\"Get started\" arrow>\n    Google Cloud SQL for MySQL is a fully-managed MySQL database service.\n  </Card>\n\n  <Card title=\"Cloud SQL for SQL Server\" href=\"/oss/python/integrations/document_loaders/google_cloud_sql_mssql\" cta=\"Get started\" arrow>\n    Google Cloud SQL for SQL Server is a fully-managed SQL Server database service.\n  </Card>\n\n  <Card title=\"Cloud SQL for PostgreSQL\" href=\"/oss/python/integrations/document_loaders/google_cloud_sql_pg\" cta=\"Get started\" arrow>\n    Google Cloud SQL for PostgreSQL is a fully-managed PostgreSQL database service.\n  </Card>\n\n  <Card title=\"Cloud Storage (directory)\" href=\"/oss/python/integrations/document_loaders/google_cloud_storage_directory\" cta=\"Get started\" arrow>\n    Google Cloud Storage is a managed service for storing unstructured data.\n  </Card>\n\n  <Card title=\"Cloud Storage (file)\" href=\"/oss/python/integrations/document_loaders/google_cloud_storage_file\" cta=\"Get started\" arrow>\n    Google Cloud Storage is a managed service for storing unstructured data.\n  </Card>\n\n  <Card title=\"El Carro for Oracle Workloads\" href=\"/oss/python/integrations/document_loaders/google_el_carro\" cta=\"Get started\" arrow>\n    Google El Carro Oracle Operator runs Oracle databases in Kubernetes.\n  </Card>\n\n  <Card title=\"Firestore (Native Mode)\" href=\"/oss/python/integrations/document_loaders/google_firestore\" cta=\"Get started\" arrow>\n    Google Cloud Firestore is a NoSQL document database.\n  </Card>\n\n  <Card title=\"Firestore (Datastore Mode)\" href=\"/oss/python/integrations/document_loaders/google_datastore\" cta=\"Get started\" arrow>\n    Google Cloud Firestore in Datastore mode\n  </Card>\n\n  <Card title=\"Memorystore for Redis\" href=\"/oss/python/integrations/document_loaders/google_memorystore_redis\" cta=\"Get started\" arrow>\n    Google Cloud Memorystore for Redis is a fully managed Redis service.\n  </Card>\n\n  <Card title=\"Spanner\" href=\"/oss/python/integrations/document_loaders/google_spanner\" cta=\"Get started\" arrow>\n    Google Cloud Spanner is a fully managed, globally distributed relational database service.\n  </Card>\n\n  <Card title=\"Speech-to-Text\" href=\"/oss/python/integrations/document_loaders/google_speech_to_text\" cta=\"Get started\" arrow>\n    Google Cloud Speech-to-Text transcribes audio files.\n  </Card>\n</Columns>\n\n<Card title=\"Cloud Vision loader\">\n  Load data using Google Cloud Vision API.\n\n  ```python  theme={null}\n  from langchain_google_community.vision import CloudVisionLoader\n  ```\n</Card>\n\n### Document transformers\n\nTransform documents using Google Cloud services.\n\n<Columns cols={2}>\n  <Card title=\"Document AI\" href=\"/oss/python/integrations/document_transformers/google_docai\" cta=\"Get started\" arrow>\n    Transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume.\n  </Card>\n\n  <Card title=\"Google Translate\" href=\"/oss/python/integrations/document_transformers/google_translate\" cta=\"Get started\" arrow>\n    Translate text and HTML with the Google Cloud Translation API.\n  </Card>\n</Columns>\n\n### Vector stores\n\nStore and search vectors using Google Cloud databases and Vertex AI Vector Search.\n\n<Columns cols={2}>\n  <Card title=\"AlloyDB for PostgreSQL\" href=\"/oss/python/integrations/vectorstores/google_alloydb\" cta=\"Get started\" arrow>\n    Google Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.\n  </Card>\n\n  <Card title=\"BigQuery Vector Search\" href=\"/oss/python/integrations/vectorstores/google_bigquery_vector_search\" cta=\"Get started\" arrow>\n    BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.\n  </Card>\n\n  <Card title=\"Memorystore for Redis\" href=\"/oss/python/integrations/vectorstores/google_memorystore_redis\" cta=\"Get started\" arrow>\n    Vector store using Memorystore for Redis\n  </Card>\n\n  <Card title=\"Spanner\" href=\"/oss/python/integrations/vectorstores/google_spanner\" cta=\"Get started\" arrow>\n    Vector store using Cloud Spanner\n  </Card>\n\n  <Card title=\"Firestore (Native Mode)\" href=\"/oss/python/integrations/vectorstores/google_firestore\" cta=\"Get started\" arrow>\n    Vector store using Firestore\n  </Card>\n\n  <Card title=\"Cloud SQL for MySQL\" href=\"/oss/python/integrations/vectorstores/google_cloud_sql_mysql\" cta=\"Get started\" arrow>\n    Vector store using Cloud SQL for MySQL\n  </Card>\n\n  <Card title=\"Cloud SQL for PostgreSQL\" href=\"/oss/python/integrations/vectorstores/google_cloud_sql_pg\" cta=\"Get started\" arrow>\n    Vector store using Cloud SQL for PostgreSQL.\n  </Card>\n\n  <Card title=\"Vertex AI Vector Search\" href=\"/oss/python/integrations/vectorstores/google_vertex_ai_vector_search\" cta=\"Get started\" arrow>\n    Formerly known as Vertex AI Matching Engine, provides a low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\n  </Card>\n\n  <Card title=\"With DataStore Backend\" href=\"/oss/python/integrations/vectorstores/google_vertex_ai_vector_search/#optional--you-can-also-create-vectore-and-store-chunks-in-a-datastore\" cta=\"Get started\" arrow>\n    Vector search using Datastore for document storage.\n  </Card>\n</Columns>\n\n### Retrievers\n\nRetrieve information using Google Cloud services.\n\n<Columns cols={2}>\n  <Card title=\"Vertex AI Search\" icon=\"magnifying-glass\" href=\"/oss/python/integrations/retrievers/google_vertex_ai_search\" cta=\"Get started\" arrow>\n    Build generative AI powered search engines using Vertex AI Search\n  </Card>\n\n  <Card title=\"Document AI Warehouse\" icon=\"warehouse\" href=\"https://cloud.google.com/document-ai-warehouse\" cta=\"Get started\" arrow>\n    Search, store, and manage documents using Document AI Warehouse.\n  </Card>\n</Columns>\n\n```python Other retrievers theme={null}\nfrom langchain_google_community import VertexAIMultiTurnSearchRetriever\nfrom langchain_google_community import VertexAISearchRetriever\nfrom langchain_google_community import VertexAISearchSummaryTool\n```\n\n### Tools\n\nIntegrate agents with various Google Cloud services.\n\n<Columns cols={2}>\n  <Card title=\"Text-to-Speech\" icon=\"volume-high\" href=\"/oss/python/integrations/tools/google_cloud_texttospeech\" cta=\"Get started\" arrow>\n    Google Cloud Text-to-Speech synthesizes natural-sounding speech with 100+ voices in multiple languages.\n  </Card>\n</Columns>\n\n### Callbacks\n\nTrack LLM/Chat model usage.\n\n<AccordionGroup>\n  <Accordion title=\"Vertex AI callback handler\">\n    Callback Handler that tracks `VertexAI` usage info.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.callbacks import VertexAICallbackHandler\n    ```\n  </Accordion>\n</AccordionGroup>\n\n### Evaluators\n\nEvaluate model outputs using Vertex AI.\n\n<AccordionGroup>\n  <Accordion title=\"VertexPairWiseStringEvaluator\">\n    Pair-wise evaluation using Vertex AI models.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.evaluators.evaluation import VertexPairWiseStringEvaluator\n    ```\n  </Accordion>\n\n  <Accordion title=\"VertexStringEvaluator\">\n    Evaluate a single prediction string using Vertex AI models.\n\n    ```python wrap theme={null}\n    from langchain_google_vertexai.evaluators.evaluation import VertexStringEvaluator\n    ```\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Other Google products\n\nIntegrations with various Google services beyond the core Cloud Platform.\n\n### Document loaders\n\n<Columns cols={1}>\n  <Card title=\"Google Drive\" href=\"/oss/python/integrations/document_loaders/google_drive\" cta=\"Get started\" arrow>\n    Google Drive file storage. Currently supports Google Docs.\n  </Card>\n</Columns>\n\n### Vector stores\n\n<Columns cols={1}>\n  <Card title=\"ScaNN (Local Index)\" href=\"/oss/python/integrations/vectorstores/google_scann\" cta=\"Get started\" arrow>\n    ScaNN is a method for efficient vector similarity search at scale.\n  </Card>\n</Columns>\n\n### Retrievers\n\n<Columns cols={1}>\n  <Card title=\"Google Drive\" href=\"/oss/python/integrations/retrievers/google_drive\" cta=\"Get started\" arrow>\n    Retrieve documents from Google Drive.\n  </Card>\n</Columns>\n\n### Tools\n\n<Columns cols={2}>\n  <Card title=\"Google Search\" href=\"/oss/python/integrations/tools/google_search\" cta=\"Get started\" arrow>\n    Perform web searches using Google Custom Search Engine (CSE).\n  </Card>\n\n  <Card title=\"Google Drive\" href=\"/oss/python/integrations/tools/google_drive\" cta=\"Get started\" arrow>\n    Tools for interacting with Google Drive.\n  </Card>\n\n  <Card title=\"Google Finance\" href=\"/oss/python/integrations/tools/google_finance\" cta=\"Get started\" arrow>\n    Query financial data.\n  </Card>\n\n  <Card title=\"Google Jobs\" href=\"/oss/python/integrations/tools/google_jobs\" cta=\"Get started\" arrow>\n    Query job listings.\n  </Card>\n\n  <Card title=\"Google Lens\" href=\"/oss/python/integrations/tools/google_lens\" cta=\"Get started\" arrow>\n    Perform visual searches.\n  </Card>\n\n  <Card title=\"Google Places\" href=\"/oss/python/integrations/tools/google_places\" cta=\"Get started\" arrow>\n    Search for places information.\n  </Card>\n\n  <Card title=\"Google Scholar\" href=\"/oss/python/integrations/tools/google_scholar\" cta=\"Get started\" arrow>\n    Search academic papers.\n  </Card>\n\n  <Card title=\"Google Trends\" href=\"/oss/python/integrations/tools/google_trends\" cta=\"Get started\" arrow>\n    Query Google Trends data.\n  </Card>\n</Columns>\n\n### MCP\n\n<Columns cols={1}>\n  <Card title=\"MCP Toolbox\" href=\"/oss/python/integrations/tools/mcp_toolbox\" cta=\"Get started\" arrow>\n    Simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB\n  </Card>\n</Columns>\n\n### Toolkits\n\nCollections of tools for specific Google services.\n\n<Columns cols={2}>\n  <Card title=\"Gmail\" icon=\"envelope\" href=\"/oss/python/integrations/tools/google_gmail\" cta=\"Get started\" arrow>\n    Toolkit to create, get, search, and send emails using the Gmail API.\n  </Card>\n</Columns>\n\n### Chat loaders\n\n<Columns cols={2}>\n  <Card title=\"Gmail\" icon=\"envelope\" href=\"/oss/python/integrations/chat_loaders/google_gmail\" cta=\"Get started\" arrow>\n    Load chat history from Gmail threads.\n  </Card>\n</Columns>\n\n***\n\n## 3rd party integrations\n\nAccess Google services via unofficial third-party APIs.\n\n### Search\n\n<Columns cols={2}>\n  <Card title=\"SearchApi\" icon=\"magnifying-glass\" href=\"/oss/python/integrations/tools/searchapi\" cta=\"Get started\" arrow>\n    searchapi.io provides API access to Google search results, YouTube, and more.\n  </Card>\n\n  <Card title=\"SerpApi\" icon=\"magnifying-glass\" href=\"/oss/python/integrations/tools/serpapi\" cta=\"Get started\" arrow>\n    SerpApi provides API access to Google search results.\n  </Card>\n\n  <Card title=\"Serper.dev\" icon=\"magnifying-glass\" href=\"/oss/python/integrations/tools/google_serper\" cta=\"Get started\" arrow>\n    serper.dev provides API access to Google search results.\n  </Card>\n</Columns>\n\n### YouTube\n\n<Columns cols={2}>\n  <Card title=\"Search tool\" icon=\"youtube\" href=\"/oss/python/integrations/tools/youtube\" cta=\"Get started\" arrow>\n    Search YouTube videos without the official API.\n  </Card>\n\n  <Card title=\"Audio loader\" icon=\"youtube\" href=\"/oss/python/integrations/document_loaders/youtube_audio\" cta=\"Get started\" arrow>\n    Download audio from YouTube videos.\n  </Card>\n\n  <Card title=\"Transcripts loader\" icon=\"youtube\" href=\"/oss/python/integrations/document_loaders/youtube_transcript\" cta=\"Get started\" arrow>\n    Load video transcripts.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/google.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 19705,
    "word_count": 1830
  },
  {
    "title": "Groq",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/groq",
    "content": "<Warning>\n  This page makes reference to [Groq](https://console.groq.com/docs/overview), an AI hardware and software company. For information on how to use Grok models (provided by [xAI](https://docs.x.ai/docs/overview)), see the [xAI provider page](/oss/python/integrations/providers/xai).\n</Warning>\n\n> [Groq](https://groq.com) developed the world's first Language Processing Unit™, or `LPU`.\n> The `Groq LPU` has a deterministic, single core streaming architecture that sets the standard\n> for GenAI inference speed with predictable and repeatable performance for any given workload.\n>\n> Beyond the architecture, `Groq` software is designed to empower developers like you with\n> the tools you need to create innovative, powerful AI applications.\n>\n> With Groq as your engine, you can:\n>\n> * Achieve uncompromised low latency and performance for real-time AI and HPC inferences\n> * Know the exact performance and compute time for any given workload\n> * Take advantage of our cutting-edge technology to stay ahead of the competition\n\n## Installation and Setup\n\nInstall the integration package:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-groq\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-groq\n  ```\n</CodeGroup>\n\nRequest an [API key](https://console.groq.com/login?utm_source=langchain\\&utm_content=provider_page) and set it as an environment variable:\n\n```bash  theme={null}\nexport GROQ_API_KEY=gsk_...\n```\n\n## Chat models\n\nSee a [usage example](/oss/python/integrations/chat/groq).\n\n```python  theme={null}\nfrom langchain_groq import ChatGroq\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/groq.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 1947,
    "word_count": 232
  },
  {
    "title": "Hugging Face",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/huggingface",
    "content": "This page covers all LangChain integrations with [Hugging Face Hub](https://huggingface.co/) and libraries like [transformers](https://huggingface.co/docs/transformers/index), [sentence transformers](https://sbert.net/), and [datasets](https://huggingface.co/docs/datasets/index).\n\n## Chat models\n\n### ChatHuggingFace\n\nWe can use the `Hugging Face` LLM classes or directly use the `ChatHuggingFace` class.\n\nSee a [usage example](/oss/python/integrations/chat/huggingface).\n\n```python  theme={null}\nfrom langchain_huggingface import ChatHuggingFace\n```\n\n## LLMs\n\n### HuggingFaceEndpoint\n\nWe can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).\n\nSee a [usage example](/oss/python/integrations/llms/huggingface_endpoint).\n\n```python  theme={null}\nfrom langchain_huggingface import HuggingFaceEndpoint\n```\n\n### HuggingFacePipeline\n\nWe can use the `HuggingFacePipeline` class to run open source models locally.\n\nSee a [usage example](/oss/python/integrations/llms/huggingface_pipelines).\n\n```python  theme={null}\nfrom langchain_huggingface import HuggingFacePipeline\n```\n\n## Embedding Models\n\n### HuggingFaceEmbeddings\n\nWe can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.\n\nSee a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).\n\n```python  theme={null}\nfrom langchain_huggingface import HuggingFaceEmbeddings\n```\n\n### HuggingFaceEndpointEmbeddings\n\nWe can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).\n\nSee a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).\n\n```python  theme={null}\nfrom langchain_huggingface import HuggingFaceEndpointEmbeddings\n```\n\n### HuggingFaceInferenceAPIEmbeddings\n\nWe can use the `HuggingFaceInferenceAPIEmbeddings` class to run open source embedding models via [Inference Providers](https://huggingface.co/docs/inference-providers).\n\nSee a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).\n\n```python  theme={null}\nfrom langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n```\n\n### HuggingFaceInstructEmbeddings\n\nWe can use the `HuggingFaceInstructEmbeddings` class to run open source embedding models locally.\n\nSee a [usage example](/oss/python/integrations/text_embedding/instruct_embeddings).\n\n```python  theme={null}\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\n```\n\n### HuggingFaceBgeEmbeddings\n\n> [BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard).\n> BGE model is created by the [Beijing Academy of Artificial Intelligence (BAAI)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence). `BAAI` is a private non-profit organization engaged in AI research and development.\n\nSee a [usage example](/oss/python/integrations/text_embedding/bge_huggingface).\n\n```python  theme={null}\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\n```\n\n## Document loaders\n\n### Hugging Face dataset\n\n> [Hugging Face Hub](https://huggingface.co/docs/hub/index) is home to over 75,000\n> [datasets](https://huggingface.co/docs/hub/index#datasets) in more than 100 languages\n> that can be used for a broad range of tasks across NLP, Computer Vision, and Audio.\n> They used for a diverse range of tasks such as translation, automatic speech\n> recognition, and image classification.\n\nWe need to install `datasets` python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install datasets\n  ```\n\n  ```bash uv theme={null}\n  uv add datasets\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/document_loaders/hugging_face_dataset).\n\n```python  theme={null}\nfrom langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader\n```\n\n### Hugging Face model loader\n\n> Load model information from `Hugging Face Hub`, including README content.\n>\n> This loader interfaces with the `Hugging Face Models API` to fetch\n> and load model metadata and README files.\n> The API allows you to search and filter models based on\n> specific criteria such as model tags, authors, and more.\n\n```python  theme={null}\nfrom langchain_community.document_loaders import HuggingFaceModelLoader\n```\n\n### Image captions\n\nIt uses the Hugging Face models to generate image captions.\n\nWe need to install several python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install transformers pillow\n  ```\n\n  ```bash uv theme={null}\n  uv add transformers pillow\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/document_loaders/image_captions).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import ImageCaptionLoader\n```\n\n## Tools\n\n### Hugging Face Hub Tools\n\n> [Hugging Face Tools](https://huggingface.co/docs/transformers/v4.29.0/en/custom_tools)\n> support text I/O and are loaded using the `load_huggingface_tool` function.\n\nWe need to install several python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install transformers huggingface_hub\n  ```\n\n  ```bash uv theme={null}\n  uv add transformers huggingface_hub\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/huggingface_tools).\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits.load_tools import load_huggingface_tool\n```\n\n### Hugging Face Text-to-Speech Model Inference.\n\n> It is a wrapper around `OpenAI Text-to-Speech API`.\n\n```python  theme={null}\nfrom langchain_community.tools.audio import HuggingFaceTextToSpeechModelInference\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/huggingface.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 6218,
    "word_count": 592
  },
  {
    "title": "Microsoft",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/microsoft",
    "content": "This page covers all LangChain integrations with [Microsoft Azure](https://portal.azure.com) and other [Microsoft](https://www.microsoft.com) products.\n\n## Chat models\n\nMicrosoft offers three main options for accessing chat models through Azure:\n\n1. [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) - Provides access to OpenAI's powerful models like o3, 4.1, and other models through Microsoft Azure's secure enterprise platform.\n2. [Azure AI](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models) - Offers access to a variety of models from different providers including Anthropic, DeepSeek, Cohere, Phi and Mistral through a unified API.\n3. [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/) - Allows deployment and management of your own custom models or fine-tuned open-source models with Azure Machine Learning.\n\n### Azure OpenAI\n\n> [Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.\n\n> [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/) is an `Azure` service with powerful language models from OpenAI including the `GPT-3`, `Codex` and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-openai\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-openai\n  ```\n</CodeGroup>\n\nSet the environment variables to get access to the `Azure OpenAI` service.\n\n```python  theme={null}\nimport os\n\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://<your-endpoint.openai.azure.com/\"\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\n```\n\nSee a [usage example](/oss/python/integrations/chat/azure_chat_openai)\n\n```python  theme={null}\nfrom langchain_openai import AzureChatOpenAI\n```\n\n### Azure AI\n\n> [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langchain-azure-ai\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-azure-ai\n  ```\n</CodeGroup>\n\nConfigure your API key and Endpoint.\n\n```bash  theme={null}\nexport AZURE_AI_CREDENTIAL=your-api-key\nexport AZURE_AI_ENDPOINT=your-endpoint\n```\n\n```python  theme={null}\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n\nllm = AzureAIChatCompletionsModel(\n    model_name=\"gpt-4o\",\n    api_version=\"2024-05-01-preview\",\n)\n```\n\nSee a [usage example](/oss/python/integrations/chat/azure_ai)\n\n### Azure ML Chat Online Endpoint\n\nSee the documentation [here](/oss/python/integrations/chat/azureml_chat_endpoint) for accessing chat\nmodels hosted with [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/).\n\n## LLMs\n\n### Azure ML\n\nSee a [usage example](/oss/python/integrations/llms/azure_ml).\n\n```python  theme={null}\nfrom langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint\n```\n\n### Azure OpenAI\n\nSee a [usage example](/oss/python/integrations/llms/azure_openai).\n\n```python  theme={null}\nfrom langchain_openai import AzureOpenAI\n```\n\n## Embedding Models\n\nMicrosoft offers two main options for accessing embedding models through Azure:\n\n### Azure OpenAI\n\nSee a [usage example](/oss/python/integrations/text_embedding/azure_openai)\n\n```python  theme={null}\nfrom langchain_openai import AzureOpenAIEmbeddings\n```\n\n### Azure AI\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langchain-azure-ai\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-azure-ai\n  ```\n</CodeGroup>\n\nConfigure your API key and Endpoint.\n\n```bash  theme={null}\nexport AZURE_AI_CREDENTIAL=your-api-key\nexport AZURE_AI_ENDPOINT=your-endpoint\n```\n\n```python  theme={null}\nfrom langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\n\nembed_model = AzureAIEmbeddingsModel(\n    model_name=\"text-embedding-ada-002\"\n)\n```\n\n## Document loaders\n\n### Azure AI Data\n\n> [Azure AI Foundry (formerly Azure AI Studio](https://ai.azure.com/) provides the capability to upload data assets\n> to cloud storage and register existing data assets from the following sources:\n>\n> * `Microsoft OneLake`\n> * `Azure Blob Storage`\n> * `Azure Data Lake gen 2`\n\nFirst, you need to install several python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install azureml-fsspec, azure-ai-generative\n  ```\n\n  ```bash uv theme={null}\n  uv add azureml-fsspec, azure-ai-generative\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/document_loaders/azure_ai_data).\n\n```python  theme={null}\nfrom langchain.document_loaders import AzureAIDataLoader\n```\n\n### Azure AI Document Intelligence\n\n> [Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known\n> as `Azure Form Recognizer`) is machine-learning\n> based service that extracts texts (including handwriting), tables, document structures,\n> and key-value-pairs\n> from digital or scanned PDFs, images, Office and HTML files.\n>\n> Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.\n\nFirst, you need to install a python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install azure-ai-documentintelligence\n  ```\n\n  ```bash uv theme={null}\n  uv add azure-ai-documentintelligence\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/document_loaders/azure_document_intelligence).\n\n```python  theme={null}\nfrom langchain.document_loaders import AzureAIDocumentIntelligenceLoader\n```\n\n### Azure Blob Storage\n\n> [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.\n\n`Azure Blob Storage` is designed for:\n\n* Serving images or documents directly to a browser.\n* Storing files for distributed access.\n* Streaming video and audio.\n* Writing to log files.\n* Storing data for backup and restore, disaster recovery, and archiving.\n* Storing data for analysis by an on-premises or Azure-hosted service.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-azure-storage\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-azure-storage\n  ```\n</CodeGroup>\n\nSee [usage examples for the Azure Blob Storage Loader](/oss/python/integrations/document_loaders/azure_blob_storage).\n\n```python  theme={null}\nfrom langchain_azure_storage.document_loaders import AzureBlobStorageLoader\n```\n\n### Microsoft OneDrive\n\n> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.\n\nFirst, you need to install a python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install o365\n  ```\n\n  ```bash uv theme={null}\n  uv add o365\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/document_loaders/microsoft_onedrive).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import OneDriveLoader\n```\n\n### Microsoft OneDrive File\n\n> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.\n\nFirst, you need to install a python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install o365\n  ```\n\n  ```bash uv theme={null}\n  uv add o365\n  ```\n</CodeGroup>\n\n```python  theme={null}\nfrom langchain_community.document_loaders import OneDriveFileLoader\n```\n\n### Microsoft Word\n\n> [Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft.\n\nSee a [usage example](/oss/python/integrations/document_loaders/microsoft_word).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import UnstructuredWordDocumentLoader\n```\n\n### Microsoft Excel\n\n> [Microsoft Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) is a spreadsheet editor developed by\n> Microsoft for Windows, macOS, Android, iOS and iPadOS.\n> It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming\n> language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.\n\nThe `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files.\nThe page content will be the raw text of the Excel file. If you use the loader in `\"elements\"` mode, an HTML\nrepresentation of the Excel file will be available in the document metadata under the `text_as_html` key.\n\nSee a [usage example](/oss/python/integrations/document_loaders/microsoft_excel).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import UnstructuredExcelLoader\n```\n\n### Microsoft SharePoint\n\n> [Microsoft SharePoint](https://en.wikipedia.org/wiki/SharePoint) is a website-based collaboration system\n> that uses workflow applications, “list” databases, and other web parts and security features to\n> empower business teams to work together developed by Microsoft.\n\nSee a [usage example](/oss/python/integrations/document_loaders/microsoft_sharepoint).\n\n```python  theme={null}\nfrom langchain_community.document_loaders.sharepoint import SharePointLoader\n```\n\n### Microsoft PowerPoint\n\n> [Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft_PowerPoint) is a presentation program by Microsoft.\n\nSee a [usage example](/oss/python/integrations/document_loaders/microsoft_powerpoint).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import UnstructuredPowerPointLoader\n```\n\n### Microsoft OneNote\n\nFirst, let's install dependencies:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install bs4 msal\n  ```\n\n  ```bash uv theme={null}\n  uv add bs4 msal\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/document_loaders/microsoft_onenote).\n\n```python  theme={null}\nfrom langchain_community.document_loaders.onenote import OneNoteLoader\n```\n\n### Playwright URL Loader\n\n> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool\n> developed by `Microsoft` that allows you to programmatically control and automate\n> web browsers. It is designed for end-to-end testing, scraping, and automating\n> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.\n\nFirst, let's install dependencies:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install playwright unstructured\n  ```\n\n  ```bash uv theme={null}\n  uv add playwright unstructured\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/document_loaders/url/#playwright-url-loader).\n\n```python  theme={null}\nfrom langchain_community.document_loaders.onenote import OneNoteLoader\n```\n\n## Memory\n\n### Azure Cosmos DB Chat Message History\n\n> [Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/) provides chat message history storage for conversational AI applications, enabling you to persist and retrieve conversation history with low latency and high availability.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-azure-ai\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-azure-ai\n  ```\n</CodeGroup>\n\nConfigure your Azure Cosmos DB connection:\n\n```python  theme={null}\nfrom langchain_azure_ai.chat_message_histories import CosmosDBChatMessageHistory\n\nhistory = CosmosDBChatMessageHistory(\n    cosmos_endpoint=\"https://<your-account>.documents.azure.com:443/\",\n    cosmos_database=\"<your-database>\",\n    cosmos_container=\"<your-container>\",\n    session_id=\"<session-id>\",\n    user_id=\"<user-id>\",\n    credential=\"<your-credential>\"  # or use connection_string\n)\n```\n\n## Vector Stores\n\n### Azure Cosmos DB\n\nAI agents can rely on Azure Cosmos DB as a unified [memory system](https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents#memory-can-make-or-break-agents) solution, enjoying speed, scale, and simplicity. This service successfully [enabled OpenAI's ChatGPT service](https://www.youtube.com/watch?v=6IIUtEFKJec\\&t) to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world's first globally distributed [NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql), [relational](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-relational), and [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) service that offers a serverless mode.\n\nBelow are two available Azure Cosmos DB APIs that can provide vector store functionalities.\n\n#### Azure Cosmos DB for MongoDB (vCore)\n\n> [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support.\n> You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string.\n> Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB.\n\n##### Installation and Setup\n\nSee [detailed configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).\n\nWe need to install `langchain-azure-ai` and `pymongo` python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-azure-ai pymongo\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-azure-ai pymongo\n  ```\n</CodeGroup>\n\n##### Deploy Azure Cosmos DB on Microsoft Azure\n\nAzure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.\n\nWith Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.\n\n[Sign Up](https://azure.microsoft.com/en-us/free/) for free to get started today.\n\nSee a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).\n\n```python  theme={null}\nfrom langchain_azure_ai.vectorstores import AzureCosmosDBMongoVCoreVectorSearch\n```\n\n#### Azure Cosmos DB NoSQL\n\n> [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search) now offers vector indexing and search in preview.\n> This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors\n> directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data,\n> but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching,\n> as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the\n> efficiency of vector-based operations.\n\n##### Installation and Setup\n\nSee [detail configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).\n\nWe need to install `langchain-azure-ai` and `azure-cosmos` python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-azure-ai azure-cosmos\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-azure-ai azure-cosmos\n  ```\n</CodeGroup>\n\n##### Deploy Azure Cosmos DB on Microsoft Azure\n\nAzure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available\nin every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.\n\n[Sign Up](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-python?pivots=devcontainer-codespace) for free to get started today.\n\nSee a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).\n\n```python  theme={null}\nfrom langchain_azure_ai.vectorstores import AzureCosmosDBNoSqlVectorSearch\n```\n\n### Azure Database for PostgreSQL\n\n> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\n\nSee [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.\n\nSimply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.\n\nSince Azure Database for PostgreSQL is open-source Postgres, you can use the [LangChain's Postgres support](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.\n\n### Azure SQL Database\n\n> [Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview?view=azuresql) is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution.  It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.\n\nBy leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.\n\n##### Installation and Setup\n\nSee [detail configuration instructions](/oss/python/integrations/vectorstores/sqlserver).\n\nWe need to install the `langchain-sqlserver` python package.\n\n```bash  theme={null}\n!pip install langchain-sqlserver==0.1.1\n```\n\n##### Deploy Azure SQL DB on Microsoft Azure\n\n[Sign Up](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql) for free to get started today.\n\nSee a [usage example](/oss/python/integrations/vectorstores/sqlserver).\n\n```python  theme={null}\nfrom langchain_sqlserver import SQLServer_VectorStore\n```\n\n### Azure AI Search\n\n[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) is a cloud search service\nthat gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid\nqueries at scale. See [here](/oss/python/integrations/vectorstores/azuresearch) for usage examples.\n\n```python  theme={null}\nfrom langchain_community.vectorstores.azuresearch import AzureSearch\n```\n\n## Retrievers\n\n### Azure AI Search\n\n> [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as `Azure Search` or `Azure Cognitive Search` ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.\n\n> Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities:\n>\n> * A search engine for full text search over a search index containing user-owned content\n> * Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation\n> * Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more\n> * Programmability through REST APIs and client libraries in Azure SDKs\n> * Azure integration at the data layer, machine learning layer, and AI (AI Services)\n\nSee [set up instructions](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).\n\nSee a [usage example](/oss/python/integrations/retrievers/azure_ai_search).\n\n```python  theme={null}\nfrom langchain_community.retrievers import AzureAISearchRetriever\n```\n\n## Vector Store\n\n### Azure Database for PostgreSQL\n\n> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\n\nSee [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.\n\nYou need to [enable pgvector extension](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector) in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the [PGVector in LangChain](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.\n\nSee a [usage example](/oss/python/integrations/vectorstores/pgvector/). Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.\n\n## Tools\n\n### Azure Container Apps dynamic sessions\n\nWe need to get the `POOL_MANAGEMENT_ENDPOINT` environment variable from the Azure Container Apps service.\nSee the instructions [here](/oss/python/integrations/tools/azure_dynamic_sessions/#setup).\n\nWe need to install a python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-azure-dynamic-sessions\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-azure-dynamic-sessions\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/azure_dynamic_sessions).\n\n```python  theme={null}\nfrom langchain_azure_dynamic_sessions import SessionsPythonREPLTool\n```\n\n### Bing Search\n\nFollow the documentation [here](/oss/python/integrations/tools/bing_search) to get a detail explanations and instructions of this tool.\n\nThe environment variable `BING_SUBSCRIPTION_KEY` and `BING_SEARCH_URL` are required from Bing Search resource.\n\n```python  theme={null}\nfrom langchain_community.tools.bing_search import BingSearchResults\nfrom langchain_community.utilities import BingSearchAPIWrapper\n\napi_wrapper = BingSearchAPIWrapper()\ntool = BingSearchResults(api_wrapper=api_wrapper)\n```\n\n## Toolkits\n\n### Azure AI Services\n\nWe need to install several python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n  ```\n\n  ```bash uv theme={null}\n  uv add azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/azure_ai_services).\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import azure_ai_services\n```\n\n#### Azure AI Services individual tools\n\nThe `azure_ai_services` toolkit includes the following tools:\n\n* Image Analysis: [AzureAiServicesImageAnalysisTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.image_analysis.AzureAiServicesImageAnalysisTool.html)\n* Document Intelligence: [AzureAiServicesDocumentIntelligenceTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.document_intelligence.AzureAiServicesDocumentIntelligenceTool.html)\n* Speech to Text: [AzureAiServicesSpeechToTextTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.speech_to_text.AzureAiServicesSpeechToTextTool.html)\n* Text to Speech: [AzureAiServicesTextToSpeechTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_to_speech.AzureAiServicesTextToSpeechTool.html)\n* Text Analytics for Health: [AzureAiServicesTextAnalyticsForHealthTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_analytics_for_health.AzureAiServicesTextAnalyticsForHealthTool.html)\n\n### Azure Cognitive Services\n\nWe need to install several python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n  ```\n\n  ```bash uv theme={null}\n  uv add azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/azure_cognitive_services).\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import AzureCognitiveServicesToolkit\n```\n\n#### Azure AI Services individual tools\n\nThe `azure_ai_services` toolkit includes the tools that queries the `Azure Cognitive Services`:\n\n* `AzureCogsFormRecognizerTool`: Form Recognizer API\n* `AzureCogsImageAnalysisTool`: Image Analysis API\n* `AzureCogsSpeech2TextTool`: Speech2Text API\n* `AzureCogsText2SpeechTool`: Text2Speech API\n* `AzureCogsTextAnalyticsHealthTool`: Text Analytics for Health API\n\n```python  theme={null}\nfrom langchain_community.tools.azure_cognitive_services import (\n    AzureCogsFormRecognizerTool,\n    AzureCogsImageAnalysisTool,\n    AzureCogsSpeech2TextTool,\n    AzureCogsText2SpeechTool,\n    AzureCogsTextAnalyticsHealthTool,\n)\n```\n\n### Microsoft Office 365 email and calendar\n\nWe need to install `O365` python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install O365\n  ```\n\n  ```bash uv theme={null}\n  uv add O365\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/office365).\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import O365Toolkit\n```\n\n#### Office 365 individual tools\n\nYou can use individual tools from the Office 365 Toolkit:\n\n* `O365CreateDraftMessage`: creating a draft email in Office 365\n* `O365SearchEmails`: searching email messages in Office 365\n* `O365SearchEvents`: searching calendar events in Office 365\n* `O365SendEvent`: sending calendar events in Office 365\n* `O365SendMessage`: sending an email in Office 365\n\n```python  theme={null}\nfrom langchain_community.tools.office365 import O365CreateDraftMessage\nfrom langchain_community.tools.office365 import O365SearchEmails\nfrom langchain_community.tools.office365 import O365SearchEvents\nfrom langchain_community.tools.office365 import O365SendEvent\nfrom langchain_community.tools.office365 import O365SendMessage\n```\n\n### Microsoft Azure PowerBI\n\nWe need to install `azure-identity` python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install azure-identity\n  ```\n\n  ```bash uv theme={null}\n  uv add azure-identity\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/powerbi).\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import PowerBIToolkit\nfrom langchain_community.utilities.powerbi import PowerBIDataset\n```\n\n#### PowerBI individual tools\n\nYou can use individual tools from the Azure PowerBI Toolkit:\n\n* `InfoPowerBITool`: getting metadata about a PowerBI Dataset\n* `ListPowerBITool`: getting tables names\n* `QueryPowerBITool`: querying a PowerBI Dataset\n\n```python  theme={null}\nfrom langchain_community.tools.powerbi.tool import InfoPowerBITool\nfrom langchain_community.tools.powerbi.tool import ListPowerBITool\nfrom langchain_community.tools.powerbi.tool import QueryPowerBITool\n```\n\n### PlayWright Browser Toolkit\n\n> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool\n> developed by `Microsoft` that allows you to programmatically control and automate\n> web browsers. It is designed for end-to-end testing, scraping, and automating\n> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.\n\nWe need to install several python packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install playwright lxml\n  ```\n\n  ```bash uv theme={null}\n  uv add playwright lxml\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/tools/playwright).\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n```\n\n#### PlayWright Browser individual tools\n\nYou can use individual tools from the PlayWright Browser Toolkit.\n\n```python  theme={null}\nfrom langchain_community.tools.playwright import ClickTool\nfrom langchain_community.tools.playwright import CurrentWebPageTool\nfrom langchain_community.tools.playwright import ExtractHyperlinksTool\nfrom langchain_community.tools.playwright import ExtractTextTool\nfrom langchain_community.tools.playwright import GetElementsTool\nfrom langchain_community.tools.playwright import NavigateTool\nfrom langchain_community.tools.playwright import NavigateBackTool\n```\n\n## Graphs\n\n### Azure Cosmos DB for Apache Gremlin\n\nWe need to install a python package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install gremlinpython\n  ```\n\n  ```bash uv theme={null}\n  uv add gremlinpython\n  ```\n</CodeGroup>\n\nSee a [usage example](/oss/python/integrations/graphs/azure_cosmosdb_gremlin).\n\n```python  theme={null}\nfrom langchain_community.graphs import GremlinGraph\nfrom langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n```\n\n## Utilities\n\n### Bing Search API\n\n> [Microsoft Bing](https://www.bing.com/), commonly referred to as `Bing` or `Bing Search`,\n> is a web search engine owned and operated by `Microsoft`.\n\nSee a [usage example](/oss/python/integrations/tools/bing_search).\n\n```python  theme={null}\nfrom langchain_community.utilities import BingSearchAPIWrapper\n```\n\n## More\n\n### Microsoft Presidio\n\n> [Presidio](https://microsoft.github.io/presidio/) (Origin from Latin praesidium ‘protection, garrison’)\n> helps to ensure sensitive data is properly managed and governed. It provides fast identification and\n> anonymization modules for private entities in text and images such as credit card numbers, names,\n> locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\n\nFirst, you need to install several python packages and download a `SpaCy` model.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker\n  python -m spacy download en_core_web_lg\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker\n  python -m spacy download en_core_web_lg\n  ```\n</CodeGroup>\n\nSee [usage examples](https://python.langchain.com/v0.1/docs/guides/productionization/safety/presidio_data_anonymization).\n\n```python  theme={null}\nfrom langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/microsoft.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 31977,
    "word_count": 3202
  },
  {
    "title": "Ollama",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/ollama",
    "content": "This page covers all LangChain integrations with [Ollama](https://ollama.com/).\n\nOllama allows you to run open-source models (like [`gpt-oss`](https://ollama.com/library/gpt-oss)) locally.\n\nFor a complete list of supported models and variants, see the [Ollama model library](https://ollama.ai/library).\n\n## Model interfaces\n\n<Columns cols={2}>\n  <Card title=\"ChatOllama\" href=\"/oss/python/integrations/chat/ollama\" cta=\"Get started\" icon=\"message\" arrow>\n    Ollama chat models.\n  </Card>\n\n  <Card title=\"OllamaLLM\" href=\"/oss/python/integrations/llms/ollama\" cta=\"Get started\" icon=\"i-cursor\" arrow>\n    (Legacy) Ollama text completion models.\n  </Card>\n\n  <Card title=\"OllamaEmbeddings\" href=\"/oss/python/integrations/text_embedding/ollama\" cta=\"Get started\" icon=\"microsoft\" arrow>\n    Ollama embedding models.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/ollama.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 1207,
    "word_count": 104
  },
  {
    "title": "OpenAI",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/openai",
    "content": "This page covers all LangChain integrations with [OpenAI](https://en.wikipedia.org/wiki/OpenAI)\n\n## Model interfaces\n\n<Columns cols={2}>\n  <Card title=\"ChatOpenAI\" href=\"/oss/python/integrations/chat/openai\" cta=\"Get started\" icon=\"message\" arrow>\n    OpenAI chat models.\n  </Card>\n\n  <Card title=\"AzureChatOpenAI\" href=\"/oss/python/integrations/chat/azure_chat_openai\" cta=\"Get started\" icon=\"microsoft\" arrow>\n    Wrapper for OpenAI chat models hosted on Azure.\n  </Card>\n\n  <Card title=\"OpenAI\" href=\"/oss/python/integrations/llms/openai\" cta=\"Get started\" icon=\"i-cursor\" arrow>\n    (Legacy) OpenAI text completion models.\n  </Card>\n\n  <Card title=\"AzureOpenAI\" href=\"/oss/python/integrations/llms/azure_openai\" cta=\"Get started\" icon=\"microsoft\" arrow>\n    Wrapper for (legacy) OpenAI text completion models hosted on Azure.\n  </Card>\n\n  <Card title=\"OpenAIEmbeddings\" href=\"/oss/python/integrations/text_embedding/openai\" cta=\"Get started\" icon=\"layer-group\" arrow>\n    OpenAI embedding models.\n  </Card>\n\n  <Card title=\"AzureOpenAIEmbeddings\" href=\"/oss/python/integrations/text_embedding/azure_openai\" cta=\"Get started\" icon=\"microsoft\" arrow>\n    Wrapper for OpenAI embedding models hosted on Azure.\n  </Card>\n</Columns>\n\n## Tools and toolkits\n\n<Columns cols={2}>\n  <Card title=\"Dall-E Image Generator\" href=\"/oss/python/integrations/tools/dalle_image_generator\" cta=\"Get started\" icon=\"image\" arrow>\n    Text-to-image generation using OpenAI's Dall-E models.\n  </Card>\n</Columns>\n\n## Retrievers\n\n<Columns cols={2}>\n  <Card title=\"ChatGPTPluginRetriever\" href=\"/oss/python/integrations/retrievers/chatgpt-plugin\" cta=\"Get started\" icon=\"download\" arrow>\n    Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.\n  </Card>\n</Columns>\n\n## Document loaders\n\n<Columns cols={2}>\n  <Card title=\"ChatGPTLoader\" href=\"/oss/python/integrations/document_loaders/chatgpt_loader\" cta=\"Get started\" icon=\"file\" arrow>\n    Load `conversations.json` from your ChatGPT data export folder.\n  </Card>\n</Columns>\n\n## Middleware\n\nMiddleware specifically designed for OpenAI models. Learn more about [middleware](/oss/python/langchain/middleware/overview).\n\n| Middleware                                | Description                                               |\n| ----------------------------------------- | --------------------------------------------------------- |\n| [Content moderation](#content-moderation) | Moderate agent traffic using OpenAI's moderation endpoint |\n\n### Content moderation\n\nModerate agent traffic (user input, model output, and tool results) using OpenAI's moderation endpoint to detect and handle unsafe content. Content moderation is useful for the following:\n\n* Applications requiring content safety and compliance\n* Filtering harmful, hateful, or inappropriate content\n* Customer-facing agents that need safety guardrails\n* Meeting platform moderation requirements\n\n<Info>\n  Learn more about [OpenAI's moderation models](https://platform.openai.com/docs/guides/moderation) and categories.\n</Info>\n\n**API reference:** [`OpenAIModerationMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.OpenAIModerationMiddleware)\n\n```python  theme={null}\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai.middleware import OpenAIModerationMiddleware\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=ChatOpenAI(model=\"gpt-4o\"),\n    tools=[search_tool, database_tool],\n    middleware=[\n        OpenAIModerationMiddleware(\n            model=\"omni-moderation-latest\",\n            check_input=True,\n            check_output=True,\n            exit_behavior=\"end\",\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"model\" type=\"ModerationModel\" default=\"omni-moderation-latest\">\n    OpenAI moderation model to use. Options: `'omni-moderation-latest'`, `'omni-moderation-2024-09-26'`, `'text-moderation-latest'`, `'text-moderation-stable'`\n  </ParamField>\n\n  <ParamField body=\"check_input\" type=\"bool\" default=\"True\">\n    Whether to check user input messages before the model is called\n  </ParamField>\n\n  <ParamField body=\"check_output\" type=\"bool\" default=\"True\">\n    Whether to check model output messages after the model is called\n  </ParamField>\n\n  <ParamField body=\"check_tool_results\" type=\"bool\" default=\"False\">\n    Whether to check tool result messages before the model is called\n  </ParamField>\n\n  <ParamField body=\"exit_behavior\" type=\"string\" default=\"end\">\n    How to handle violations when content is flagged. Options:\n\n    * `'end'` - End agent execution immediately with a violation message\n    * `'error'` - Raise `OpenAIModerationError` exception\n    * `'replace'` - Replace the flagged content with the violation message and continue\n  </ParamField>\n\n  <ParamField body=\"violation_message\" type=\"str | None\">\n    Custom template for violation messages. Supports template variables:\n\n    * `{categories}` - Comma-separated list of flagged categories\n    * `{category_scores}` - JSON string of category scores\n    * `{original_content}` - The original flagged content\n\n    Default: `\"I'm sorry, but I can't comply with that request. It was flagged for {categories}.\"`\n  </ParamField>\n\n  <ParamField body=\"client\" type=\"OpenAI | None\">\n    Optional pre-configured OpenAI client to reuse. If not provided, a new client will be created.\n  </ParamField>\n\n  <ParamField body=\"async_client\" type=\"AsyncOpenAI | None\">\n    Optional pre-configured AsyncOpenAI client to reuse. If not provided, a new async client will be created.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware integrates OpenAI's moderation endpoint to check content at different stages:\n\n  **Moderation stages:**\n\n  * `check_input` - User messages before model call\n  * `check_output` - AI messages after model call\n  * `check_tool_results` - Tool outputs before model call\n\n  **Exit behaviors:**\n\n  * `'end'` (default) - Stop execution with violation message\n  * `'error'` - Raise exception for application handling\n  * `'replace'` - Replace flagged content and continue\n\n  ```python  theme={null}\n  from langchain_openai import ChatOpenAI\n  from langchain_openai.middleware import OpenAIModerationMiddleware\n  from langchain.agents import create_agent\n\n\n  # Basic moderation\n  agent = create_agent(\n      model=ChatOpenAI(model=\"gpt-4o\"),\n      tools=[search_tool, customer_data_tool],\n      middleware=[\n          OpenAIModerationMiddleware(\n              model=\"omni-moderation-latest\",\n              check_input=True,\n              check_output=True,\n          ),\n      ],\n  )\n\n  # Strict moderation with custom message\n  agent_strict = create_agent(\n      model=ChatOpenAI(model=\"gpt-4o\"),\n      tools=[search_tool, customer_data_tool],\n      middleware=[\n          OpenAIModerationMiddleware(\n              model=\"omni-moderation-latest\",\n              check_input=True,\n              check_output=True,\n              check_tool_results=True,\n              exit_behavior=\"error\",\n              violation_message=(\n                  \"Content policy violation detected: {categories}. \"\n                  \"Please rephrase your request.\"\n              ),\n          ),\n      ],\n  )\n\n  # Moderation with replacement behavior\n  agent_replace = create_agent(\n      model=ChatOpenAI(model=\"gpt-4o\"),\n      tools=[search_tool],\n      middleware=[\n          OpenAIModerationMiddleware(\n              check_input=True,\n              exit_behavior=\"replace\",\n              violation_message=\"[Content removed due to safety policies]\",\n          ),\n      ],\n  )\n  ```\n</Accordion>\n\n## Other\n\n<Columns cols={2}>\n  <Card title=\"Adapter\" href=\"/oss/python/integrations/adapters/openai\" cta=\"Get started\" icon=\"arrows-left-right\" arrow>\n    Adapt LangChain models to OpenAI APIs.\n  </Card>\n\n  <Card title=\"OpenAIModerationChain\" href=\"https://python.langchain.com/v0.1/docs/guides/productionization/safety/moderation\" cta=\"Get started\" icon=\"link\" arrow>\n    Detect text that could be hateful, violent, etc.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/openai.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 8456,
    "word_count": 732
  },
  {
    "title": "Integration packages",
    "source": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "content": "{/* Do not manually edit */}\n\nLangChain Python offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.\n\n<Columns cols={3}>\n  <Card title=\"Chat models\" icon=\"message\" href=\"/oss/python/integrations/chat\" arrow />\n\n  <Card title=\"Embedding models\" icon=\"layer-group\" href=\"/oss/python/integrations/text_embedding\" arrow />\n\n  <Card title=\"Tools and toolkits\" icon=\"screwdriver-wrench\" href=\"/oss/python/integrations/tools\" arrow />\n</Columns>\n\nTo see a full list of integrations by component type, refer to the categories in the sidebar.\n\n## Popular providers\n\n| Provider                                                            | Package                                                                                                               | Downloads                                                                                                                                                                                                                         | Latest version                                                                                                                                                                                                                                           | <Tooltip tip=\"Whether an equivalent version exists in the TypeScript version of LangChain. Click the checkmark to visit the respective package.\">JS/TS support</Tooltip> |\n| :------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [OpenAI](/oss/python/integrations/providers/openai/)                | [`langchain-openai`](https://reference.langchain.com/python/integrations/langchain_openai/)                           | <a href=\"https://pypi.org/project/langchain-openai/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-openai/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                           | <a href=\"https://pypi.org/project/langchain-openai/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-openai?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/openai)                                                                                                                     |\n| [Google (Vertex AI)](/oss/python/integrations/providers/google)     | [`langchain-google-vertexai`](https://reference.langchain.com/python/integrations/langchain_google_vertexai/)         | <a href=\"https://pypi.org/project/langchain-google-vertexai/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-google-vertexai/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>         | <a href=\"https://pypi.org/project/langchain-google-vertexai/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-google-vertexai?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>         | [✅](https://www.npmjs.com/package/@langchain/google-vertexai)                                                                                                            |\n| [Anthropic (Claude)](/oss/python/integrations/providers/anthropic/) | [`langchain-anthropic`](https://reference.langchain.com/python/integrations/langchain_anthropic/)                     | <a href=\"https://pypi.org/project/langchain-anthropic/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-anthropic/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                     | <a href=\"https://pypi.org/project/langchain-anthropic/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-anthropic?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/anthropic)                                                                                                                  |\n| [AWS](/oss/python/integrations/providers/aws/)                      | [`langchain-aws`](https://reference.langchain.com/python/integrations/langchain_aws/)                                 | <a href=\"https://pypi.org/project/langchain-aws/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-aws/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                                 | <a href=\"https://pypi.org/project/langchain-aws/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-aws?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/aws)                                                                                                                        |\n| [Google (GenAI)](/oss/python/integrations/providers/google)         | [`langchain-google-genai`](https://reference.langchain.com/python/integrations/langchain_google_genai/)               | <a href=\"https://pypi.org/project/langchain-google-genai/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-google-genai/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>               | <a href=\"https://pypi.org/project/langchain-google-genai/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-google-genai?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>               | [✅](https://www.npmjs.com/package/@langchain/google-genai)                                                                                                               |\n| [Groq](/oss/python/integrations/providers/groq/)                    | [`langchain-groq`](https://reference.langchain.com/python/integrations/langchain_groq/)                               | <a href=\"https://pypi.org/project/langchain-groq/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-groq/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                               | <a href=\"https://pypi.org/project/langchain-groq/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-groq?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                               | [✅](https://www.npmjs.com/package/@langchain/groq)                                                                                                                       |\n| [Ollama](/oss/python/integrations/providers/ollama/)                | [`langchain-ollama`](https://reference.langchain.com/python/integrations/langchain_ollama/)                           | <a href=\"https://pypi.org/project/langchain-ollama/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-ollama/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                           | <a href=\"https://pypi.org/project/langchain-ollama/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-ollama?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/ollama)                                                                                                                     |\n| [Chroma](/oss/python/integrations/providers/chroma/)                | [`langchain-chroma`](https://reference.langchain.com/python/integrations/langchain_chroma/)                           | <a href=\"https://pypi.org/project/langchain-chroma/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-chroma/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                           | <a href=\"https://pypi.org/project/langchain-chroma/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-chroma?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Huggingface](/oss/python/integrations/providers/huggingface/)      | [`langchain-huggingface`](https://reference.langchain.com/python/integrations/langchain_huggingface/)                 | <a href=\"https://pypi.org/project/langchain-huggingface/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-huggingface/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                 | <a href=\"https://pypi.org/project/langchain-huggingface/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-huggingface?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                 | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Pinecone](/oss/python/integrations/providers/pinecone/)            | [`langchain-pinecone`](https://reference.langchain.com/python/integrations/langchain_pinecone/)                       | <a href=\"https://pypi.org/project/langchain-pinecone/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-pinecone/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                       | <a href=\"https://pypi.org/project/langchain-pinecone/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-pinecone?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/pinecone)                                                                                                                   |\n| [Cohere](/oss/python/integrations/providers/cohere/)                | [`langchain-cohere`](https://reference.langchain.com/python/integrations/langchain_cohere/)                           | <a href=\"https://pypi.org/project/langchain-cohere/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-cohere/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                           | <a href=\"https://pypi.org/project/langchain-cohere/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-cohere?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/cohere)                                                                                                                     |\n| [Postgres](/oss/python/integrations/providers/pgvector)             | [`langchain-postgres`](https://reference.langchain.com/python/integrations/langchain_postgres/)                       | <a href=\"https://pypi.org/project/langchain-postgres/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-postgres/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                       | <a href=\"https://pypi.org/project/langchain-postgres/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-postgres?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Fireworks](/oss/python/integrations/providers/fireworks/)          | [`langchain-fireworks`](https://reference.langchain.com/python/integrations/langchain_fireworks/)                     | <a href=\"https://pypi.org/project/langchain-fireworks/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-fireworks/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                     | <a href=\"https://pypi.org/project/langchain-fireworks/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-fireworks?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [MistralAI](/oss/python/integrations/providers/mistralai/)          | [`langchain-mistralai`](https://reference.langchain.com/python/integrations/langchain_mistralai/)                     | <a href=\"https://pypi.org/project/langchain-mistralai/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-mistralai/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                     | <a href=\"https://pypi.org/project/langchain-mistralai/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-mistralai?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/mistralai)                                                                                                                  |\n| [Databricks](/oss/python/integrations/providers/databricks/)        | [`databricks-langchain`](https://pypi.org/project/databricks-langchain/)                                              | <a href=\"https://pypi.org/project/databricks-langchain/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/databricks-langchain/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                   | <a href=\"https://pypi.org/project/databricks-langchain/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/databricks-langchain?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                   | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Perplexity](/oss/python/integrations/providers/perplexity/)        | [`langchain-perplexity`](https://reference.langchain.com/python/integrations/langchain_perplexity/)                   | <a href=\"https://pypi.org/project/langchain-perplexity/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-perplexity/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                   | <a href=\"https://pypi.org/project/langchain-perplexity/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-perplexity?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                   | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [IBM](/oss/python/integrations/providers/ibm/)                      | [`langchain-ibm`](https://reference.langchain.com/python/integrations/langchain_ibm/)                                 | <a href=\"https://pypi.org/project/langchain-ibm/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-ibm/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                                 | <a href=\"https://pypi.org/project/langchain-ibm/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-ibm?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/ibm)                                                                                                                        |\n| [Nvidia AI Endpoints](/oss/python/integrations/providers/nvidia)    | [`langchain-nvidia-ai-endpoints`](https://reference.langchain.com/python/integrations/langchain_nvidia_ai_endpoints/) | <a href=\"https://pypi.org/project/langchain-nvidia-ai-endpoints/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-nvidia-ai-endpoints/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a> | <a href=\"https://pypi.org/project/langchain-nvidia-ai-endpoints/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-nvidia-ai-endpoints?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a> | ❌                                                                                                                                                                        |\n| [MongoDB](/oss/python/integrations/providers/mongodb_atlas)         | [`langchain-mongodb`](https://reference.langchain.com/python/integrations/langchain_mongodb/)                         | <a href=\"https://pypi.org/project/langchain-mongodb/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-mongodb/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                         | <a href=\"https://pypi.org/project/langchain-mongodb/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-mongodb?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                         | [✅](https://www.npmjs.com/package/@langchain/mongodb)                                                                                                                    |\n| [Deepseek](/oss/python/integrations/providers/deepseek/)            | [`langchain-deepseek`](https://reference.langchain.com/python/integrations/langchain_deepseek/)                       | <a href=\"https://pypi.org/project/langchain-deepseek/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-deepseek/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                       | <a href=\"https://pypi.org/project/langchain-deepseek/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-deepseek?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/deepseek)                                                                                                                   |\n| [Qdrant](/oss/python/integrations/providers/qdrant/)                | [`langchain-qdrant`](https://reference.langchain.com/python/integrations/langchain_qdrant/)                           | <a href=\"https://pypi.org/project/langchain-qdrant/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-qdrant/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                           | <a href=\"https://pypi.org/project/langchain-qdrant/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-qdrant?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/qdrant)                                                                                                                     |\n| [Milvus](/oss/python/integrations/providers/milvus/)                | [`langchain-milvus`](https://reference.langchain.com/python/integrations/langchain_milvus/)                           | <a href=\"https://pypi.org/project/langchain-milvus/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-milvus/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                           | <a href=\"https://pypi.org/project/langchain-milvus/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-milvus?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Tavily](/oss/python/integrations/providers/tavily/)                | [`langchain-tavily`](https://pypi.org/project/langchain-tavily/)                                                      | <a href=\"https://pypi.org/project/langchain-tavily/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-tavily/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                           | <a href=\"https://pypi.org/project/langchain-tavily/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-tavily?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/tavily)                                                                                                                     |\n| [Elasticsearch](/oss/python/integrations/providers/elasticsearch/)  | [`langchain-elasticsearch`](https://reference.langchain.com/python/integrations/langchain_elasticsearch/)             | <a href=\"https://pypi.org/project/langchain-elasticsearch/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-elasticsearch/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>             | <a href=\"https://pypi.org/project/langchain-elasticsearch/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-elasticsearch?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>             | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Together](/oss/python/integrations/providers/together/)            | [`langchain-together`](https://reference.langchain.com/python/integrations/langchain_together/)                       | <a href=\"https://pypi.org/project/langchain-together/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-together/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                       | <a href=\"https://pypi.org/project/langchain-together/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-together?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Redis](/oss/python/integrations/providers/redis/)                  | [`langchain-redis`](https://reference.langchain.com/python/integrations/langchain_redis/)                             | <a href=\"https://pypi.org/project/langchain-redis/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-redis/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                             | <a href=\"https://pypi.org/project/langchain-redis/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-redis?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                             | [✅](https://www.npmjs.com/package/@langchain/redis)                                                                                                                      |\n| [LiteLLM](/oss/python/integrations/providers/litellm/)              | [`langchain-litellm`](https://pypi.org/project/langchain-litellm/)                                                    | <a href=\"https://pypi.org/project/langchain-litellm/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-litellm/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                         | <a href=\"https://pypi.org/project/langchain-litellm/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-litellm?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                         | N/A                                                                                                                                                                      |\n| [xAI (Grok)](/oss/python/integrations/providers/xai/)               | [`langchain-xai`](https://reference.langchain.com/python/integrations/langchain_xai/)                                 | <a href=\"https://pypi.org/project/langchain-xai/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-xai/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                                 | <a href=\"https://pypi.org/project/langchain-xai/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-xai?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/xai)                                                                                                                        |\n| [DataStax Astra DB](/oss/python/integrations/providers/astradb/)    | [`langchain-astradb`](https://reference.langchain.com/python/integrations/langchain_astradb/)                         | <a href=\"https://pypi.org/project/langchain-astradb/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-astradb/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                         | <a href=\"https://pypi.org/project/langchain-astradb/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-astradb?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                         | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Azure AI](/oss/python/integrations/providers/azure_ai)             | [`langchain-azure-ai`](https://reference.langchain.com/python/integrations/langchain_azure_ai/)                       | <a href=\"https://pypi.org/project/langchain-azure-ai/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-azure-ai/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                       | <a href=\"https://pypi.org/project/langchain-azure-ai/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-azure-ai?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/openai)                                                                                                                     |\n| [MCP Toolbox (Google)](/oss/python/integrations/providers/toolbox/) | [`toolbox-langchain`](https://pypi.org/project/toolbox-langchain/)                                                    | <a href=\"https://pypi.org/project/toolbox-langchain/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/toolbox-langchain/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                         | <a href=\"https://pypi.org/project/toolbox-langchain/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/toolbox-langchain?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                         | ❌                                                                                                                                                                        |\n| [Google (Community)](/oss/python/integrations/providers/google)     | [`langchain-google-community`](https://reference.langchain.com/python/integrations/langchain_google_community/)       | <a href=\"https://pypi.org/project/langchain-google-community/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-google-community/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>       | <a href=\"https://pypi.org/project/langchain-google-community/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-google-community?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>       | ❌                                                                                                                                                                        |\n| [Unstructured](/oss/python/integrations/providers/unstructured/)    | [`langchain-unstructured`](https://reference.langchain.com/python/integrations/langchain_unstructured/)               | <a href=\"https://pypi.org/project/langchain-unstructured/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-unstructured/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>               | <a href=\"https://pypi.org/project/langchain-unstructured/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-unstructured?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>               | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Neo4J](/oss/python/integrations/providers/neo4j/)                  | [`langchain-neo4j`](https://reference.langchain.com/python/integrations/langchain_neo4j/)                             | <a href=\"https://pypi.org/project/langchain-neo4j/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-neo4j/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>                             | <a href=\"https://pypi.org/project/langchain-neo4j/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-neo4j?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>                             | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |\n| [Graph RAG](/oss/python/integrations/providers/graph_rag)           | [`langchain-graph-retriever`](https://pypi.org/project/langchain-graph-retriever/)                                    | <a href=\"https://pypi.org/project/langchain-graph-retriever/\" target=\"_blank\"><img src=\"https://static.pepy.tech/badge/langchain-graph-retriever/month\" alt=\"Downloads per month\" noZoom class=\"rounded not-prose\" /></a>         | <a href=\"https://pypi.org/project/langchain-graph-retriever/\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain-graph-retriever?style=flat-square&label=%20\" alt=\"PyPI - Latest version\" noZoom class=\"rounded not-prose\" /></a>         | ❌                                                                                                                                                                        |\n\n## All providers\n\n[See all providers](/oss/python/integrations/providers/all_providers) or search for a provider using the search field.\n\nCommunity integrations can be found in [`langchain-community`](https://github.com/langchain-ai/langchain-community).\n\n<Info>\n  If you'd like to contribute an integration, see the [contributing guide](/oss/python/contributing).\n</Info>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 32552,
    "word_count": 1317
  },
  {
    "title": "Retrievers",
    "source": "https://docs.langchain.com/oss/python/integrations/retrievers/index",
    "content": "A [retriever](/oss/python/langchain/retrieval#building-blocks) is an interface that returns documents given an unstructured query.\nIt is more general than a vector store.\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\nRetrievers can be created from vector stores, but are also broad enough to include [Wikipedia search](/oss/python/integrations/retrievers/wikipedia/) and [Amazon Kendra](/oss/python/integrations/retrievers/amazon_kendra_retriever/).\n\nRetrievers accept a string query as input and return a list of [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) as output.\n\nNote that all [vector stores](/oss/python/integrations/vectorstores) can be cast to retrievers. Refer to the vector store [integration docs](/oss/python/integrations/vectorstores/) for available vector stores.\nThis page lists custom retrievers, implemented via subclassing BaseRetriever.\n\n## Bring-your-own documents\n\nThe below retrievers allow you to index and search a custom corpus of documents.\n\n| Retriever                                                                                | Self-host | Cloud offering | Package                                                                                                                                                                               |\n| ---------------------------------------------------------------------------------------- | --------- | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AmazonKnowledgeBasesRetriever`](/oss/python/integrations/retrievers/bedrock)           | ❌         | ✅              | [`langchain-aws`](https://python.langchain.com/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.html)                                      |\n| [`AzureAISearchRetriever`](/oss/python/integrations/retrievers/azure_ai_search)          | ❌         | ✅              | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.azure_ai_search.AzureAISearchRetriever.html)                   |\n| [`ElasticsearchRetriever`](/oss/python/integrations/retrievers/elasticsearch_retriever)  | ✅         | ✅              | [`langchain-elasticsearch`](https://python.langchain.com/api_reference/elasticsearch/retrievers/langchain_elasticsearch.retrievers.ElasticsearchRetriever.html)                       |\n| [`VertexAISearchRetriever`](/oss/python/integrations/retrievers/google_vertex_ai_search) | ❌         | ✅              | [`langchain-google-community`](https://python.langchain.com/api_reference/google_community/vertex_ai_search/langchain_google_community.vertex_ai_search.VertexAISearchRetriever.html) |\n\n## External index\n\nThe below retrievers will search over an external index (e.g., constructed from Internet data or similar).\n\n| Retriever                                                                | Source                                                | Package                                                                                                                                                                 |\n| ------------------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`ArxivRetriever`](/oss/python/integrations/retrievers/arxiv)            | Scholarly articles on [arxiv.org](https://arxiv.org/) | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html)                       |\n| [`TavilySearchAPIRetriever`](/oss/python/integrations/retrievers/tavily) | Internet search                                       | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.tavily_search_api.TavilySearchAPIRetriever.html) |\n| [`WikipediaRetriever`](/oss/python/integrations/retrievers/wikipedia)    | [Wikipedia](https://www.wikipedia.org/) articles      | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html)               |\n\n## All retrievers\n\n> **Note:** The descriptions in the table below are truncated for readability.\n\n<Columns cols={3}>\n  <Card title=\"Activeloop Deep Memory\" icon=\"link\" href=\"/oss/python/integrations/retrievers/activeloop\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Amazon Kendra\" icon=\"link\" href=\"/oss/python/integrations/retrievers/amazon_kendra_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Arcee\" icon=\"link\" href=\"/oss/python/integrations/retrievers/arcee\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Arxiv\" icon=\"link\" href=\"/oss/python/integrations/retrievers/arxiv\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AskNews\" icon=\"link\" href=\"/oss/python/integrations/retrievers/asknews\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure AI Search\" icon=\"link\" href=\"/oss/python/integrations/retrievers/azure_ai_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bedrock (Knowledge Bases)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/bedrock\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BM25\" icon=\"link\" href=\"/oss/python/integrations/retrievers/bm25\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Box\" icon=\"link\" href=\"/oss/python/integrations/retrievers/box\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BREEBS (Open Knowledge)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/breebs\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Chaindesk\" icon=\"link\" href=\"/oss/python/integrations/retrievers/chaindesk\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatGPT plugin\" icon=\"link\" href=\"/oss/python/integrations/retrievers/chatgpt-plugin\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cognee\" icon=\"link\" href=\"/oss/python/integrations/retrievers/cognee\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cohere reranker\" icon=\"link\" href=\"/oss/python/integrations/retrievers/cohere-reranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cohere RAG\" icon=\"link\" href=\"/oss/python/integrations/retrievers/cohere\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Contextual AI Reranker\" icon=\"link\" href=\"/oss/python/integrations/retrievers/contextual\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dappier\" icon=\"link\" href=\"/oss/python/integrations/retrievers/dappier\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DocArray\" icon=\"link\" href=\"/oss/python/integrations/retrievers/docarray_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dria\" icon=\"link\" href=\"/oss/python/integrations/retrievers/dria_index\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ElasticSearch BM25\" icon=\"link\" href=\"/oss/python/integrations/retrievers/elastic_search_bm25\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Elasticsearch\" icon=\"link\" href=\"/oss/python/integrations/retrievers/elasticsearch_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Embedchain\" icon=\"link\" href=\"/oss/python/integrations/retrievers/embedchain\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FlashRank reranker\" icon=\"link\" href=\"/oss/python/integrations/retrievers/flashrank-reranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Fleet AI Context\" icon=\"link\" href=\"/oss/python/integrations/retrievers/fleet_context\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Galaxia\" icon=\"link\" href=\"/oss/python/integrations/retrievers/galaxia-retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Drive\" icon=\"link\" href=\"/oss/python/integrations/retrievers/google_drive\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Vertex AI Search\" icon=\"link\" href=\"/oss/python/integrations/retrievers/google_vertex_ai_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Graph RAG\" icon=\"link\" href=\"/oss/python/integrations/retrievers/graph_rag\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GreenNode\" icon=\"link\" href=\"/oss/python/integrations/retrievers/greennode_reranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IBM watsonx.ai\" icon=\"link\" href=\"/oss/python/integrations/retrievers/ibm_watsonx_ranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JaguarDB Vector Database\" icon=\"link\" href=\"/oss/python/integrations/retrievers/jaguar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Kay.ai\" icon=\"link\" href=\"/oss/python/integrations/retrievers/kay\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Kinetica Vectorstore\" icon=\"link\" href=\"/oss/python/integrations/retrievers/kinetica\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"kNN\" icon=\"link\" href=\"/oss/python/integrations/retrievers/knn\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LinkupSearchRetriever\" icon=\"link\" href=\"/oss/python/integrations/retrievers/linkup_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LLMLingua Document Compressor\" icon=\"link\" href=\"/oss/python/integrations/retrievers/llmlingua\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LOTR (Merger Retriever)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/merger_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Metal\" icon=\"link\" href=\"/oss/python/integrations/retrievers/metal\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NanoPQ (Product Quantization)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/nanopq\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nebius\" icon=\"link\" href=\"/oss/python/integrations/retrievers/nebius\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"needle\" icon=\"link\" href=\"/oss/python/integrations/retrievers/needle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nimble\" icon=\"link\" href=\"/oss/python/integrations/retrievers/nimble\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Outline\" icon=\"link\" href=\"/oss/python/integrations/retrievers/outline\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Permit\" icon=\"link\" href=\"/oss/python/integrations/retrievers/permit\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pinecone Hybrid Search\" icon=\"link\" href=\"/oss/python/integrations/retrievers/pinecone_hybrid_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pinecone Rerank\" icon=\"link\" href=\"/oss/python/integrations/retrievers/pinecone_rerank\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PubMed\" icon=\"link\" href=\"/oss/python/integrations/retrievers/pubmed\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Qdrant Sparse Vector\" icon=\"link\" href=\"/oss/python/integrations/retrievers/qdrant-sparse\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RAGatouille\" icon=\"link\" href=\"/oss/python/integrations/retrievers/ragatouille\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RePhraseQuery\" icon=\"link\" href=\"/oss/python/integrations/retrievers/re_phrase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Rememberizer\" icon=\"link\" href=\"/oss/python/integrations/retrievers/rememberizer\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SEC filing\" icon=\"link\" href=\"/oss/python/integrations/retrievers/sec_filings\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SVM\" icon=\"link\" href=\"/oss/python/integrations/retrievers/svm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TavilySearchAPI\" icon=\"link\" href=\"/oss/python/integrations/retrievers/tavily\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TF-IDF\" icon=\"link\" href=\"/oss/python/integrations/retrievers/tf_idf\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NeuralDB\" icon=\"link\" href=\"/oss/python/integrations/retrievers/thirdai_neuraldb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ValyuContext\" icon=\"link\" href=\"/oss/python/integrations/retrievers/valyu\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vectorize\" icon=\"link\" href=\"/oss/python/integrations/retrievers/vectorize\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vespa\" icon=\"link\" href=\"/oss/python/integrations/retrievers/vespa\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia\" icon=\"link\" href=\"/oss/python/integrations/retrievers/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"You.com\" icon=\"link\" href=\"/oss/python/integrations/retrievers/you-retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zep Cloud\" icon=\"link\" href=\"/oss/python/integrations/retrievers/zep_cloud_memorystore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zep Open Source\" icon=\"link\" href=\"/oss/python/integrations/retrievers/zep_memorystore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zotero\" icon=\"link\" href=\"/oss/python/integrations/retrievers/zotero\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/retrievers/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 13358,
    "word_count": 835
  },
  {
    "title": "Text splitters",
    "source": "https://docs.langchain.com/oss/python/integrations/splitters/index",
    "content": "**Text splitters** break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.\n\nThere are several strategies for splitting documents, each with its own advantages.\n\n<Tip>\n  For most use cases, start with the [RecursiveCharacterTextSplitter](/oss/python/integrations/splitters/recursive_text_splitter). It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.\n</Tip>\n\n## Text structure-based\n\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain's `RecursiveCharacterTextSplitter` implements this concept:\n\n* The [RecursiveCharacterTextSplitter](/oss/python/integrations/splitters/recursive_text_splitter) attempts to keep larger units (e.g., paragraphs) intact.\n* If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n* This process continues down to the word level if necessary.\n\nExample usage:\n\n```python  theme={null}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\ntexts = text_splitter.split_text(document)\n```\n\n**Available text splitters**:\n\n* [Recursively split text](/oss/python/integrations/splitters/recursive_text_splitter)\n\n## Length-based\n\nAn intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:\n\n* Straightforward implementation\n* Consistent chunk sizes\n* Easily adaptable to different model requirements\n\nTypes of length-based splitting:\n\n* Token-based: Splits text based on the number of tokens, which is useful when working with language models.\n* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.\n\nExample implementation using LangChain's CharacterTextSplitter with token-based splitting:\n\n```python  theme={null}\nfrom langchain_text_splitters import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n)\ntexts = text_splitter.split_text(document)\n```\n\n**Available text splitters**:\n\n* [Split by tokens](/oss/python/integrations/splitters/split_by_token)\n* [Split by characters](/oss/python/integrations/splitters/character_text_splitter)\n\n## Document structure-based\n\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:\n\n* Preserves the logical organization of the document\n* Maintains context within each chunk\n* Can be more effective for downstream tasks like retrieval or summarization\n\nExamples of structure-based splitting:\n\n* Markdown: Split based on headers (e.g., #, ##, ###)\n* HTML: Split using tags\n* JSON: Split by object or array elements\n* Code: Split by functions, classes, or logical blocks\n\n**Available text splitters**:\n\n* [Split Markdown](/oss/python/integrations/splitters/markdown_header_metadata_splitter)\n* [Split JSON](/oss/python/integrations/splitters/recursive_json_splitter)\n* [Split code](/oss/python/integrations/splitters/code_splitter)\n* [Split HTML](/oss/python/integrations/splitters/split_html)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/integrations/splitters/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 4180,
    "word_count": 469
  },
  {
    "title": "Key-value stores",
    "source": "https://docs.langchain.com/oss/python/integrations/stores/index",
    "content": "## Overview\n\nLangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching [embeddings](/oss/python/integrations/text_embedding).\n\n## Interface\n\nAll [`BaseStores`](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface:\n\n* `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n* `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n* `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n* `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n<Note>\n  Base stores are designed to work **multiple** key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.\n</Note>\n\n## Built-in stores for local development\n\n<Columns cols={2}>\n  <Card title=\"InMemoryByteStore\" icon=\"link\" href=\"/oss/python/integrations/stores/in_memory\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LocalFileStore\" icon=\"link\" href=\"/oss/python/integrations/stores/file_system\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n## Custom stores\n\nYou can also implement your own custom store by extending the [`BaseStore`](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) class. See the [store interface documentation](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) for more details.\n\n## All key-value stores\n\n<Columns cols={3}>\n  <Card title=\"AstraDBByteStore\" icon=\"link\" href=\"/oss/python/integrations/stores/astradb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"CassandraByteStore\" icon=\"link\" href=\"/oss/python/integrations/stores/cassandra\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ElasticsearchEmbeddingsCache\" icon=\"link\" href=\"/oss/python/integrations/stores/elasticsearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RedisStore\" icon=\"link\" href=\"/oss/python/integrations/stores/redis\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"UpstashRedisByteStore\" icon=\"link\" href=\"/oss/python/integrations/stores/upstash_redis\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/stores/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 2768,
    "word_count": 251
  },
  {
    "title": "Embedding models",
    "source": "https://docs.langchain.com/oss/python/integrations/text_embedding/index",
    "content": "## Overview\n\n<Note>\n  This overview covers **text-based embedding models**. LangChain does not currently support multimodal embeddings.\n\n  See [top embedding models](#top-integrations).\n</Note>\n\nEmbedding models transform raw text—such as a sentence, paragraph, or tweet—into a fixed-length vector of numbers that captures its **semantic meaning**. These vectors allow machines to compare and search text based on meaning rather than exact words.\n\nIn practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase *\"machine learning\"*, embeddings can surface documents that discuss related concepts even when different wording is used.\n\n### How it works\n\n1. **Vectorization** — The model encodes each input string as a high-dimensional vector.\n2. **Similarity scoring** — Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.\n\n### Similarity metrics\n\nSeveral metrics are commonly used to compare embeddings:\n\n* **Cosine similarity** — measures the angle between two vectors.\n* **Euclidean distance** — measures the straight-line distance between points.\n* **Dot product** — measures how much one vector projects onto another.\n\nHere's an example of computing cosine similarity between two vectors:\n\n```python  theme={null}\nimport numpy as np\n\ndef cosine_similarity(vec1, vec2):\n    dot = np.dot(vec1, vec2)\n    return dot / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\nsimilarity = cosine_similarity(query_embedding, document_embedding)\nprint(\"Cosine Similarity:\", similarity)\n```\n\n## Interface\n\nLangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the [Embeddings](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) interface.\n\nTwo main methods are available:\n\n* `embed_documents(texts: List[str]) → List[List[float]]`: Embeds a list of documents.\n* `embed_query(text: str) → List[float]`: Embeds a single query.\n\n<Note>\n  The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.\n</Note>\n\n## Top integrations\n\n| Model                                                                                          | Package                                                                                                                                                            |\n| ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`OpenAIEmbeddings`](/oss/python/integrations/text_embedding/openai)                           | [`langchain-openai`](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)                              |\n| [`AzureOpenAIEmbeddings`](/oss/python/integrations/text_embedding/azure_openai)                | [`langchain-openai`](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html)                    |\n| [`GoogleGenerativeAIEmbeddings`](/oss/python/integrations/text_embedding/google_generative_ai) | [`langchain-google-genai`](https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html) |\n| [`OllamaEmbeddings`](/oss/python/integrations/text_embedding/ollama)                           | [`langchain-ollama`](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)                               |\n| [`TogetherEmbeddings`](/oss/python/integrations/text_embedding/together)                       | [`langchain-together`](https://python.langchain.com/api_reference/together/embeddings/langchain_together.embeddings.TogetherEmbeddings.html)                       |\n| [`FireworksEmbeddings`](/oss/python/integrations/text_embedding/fireworks)                     | [`langchain-fireworks`](https://python.langchain.com/api_reference/fireworks/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html)                   |\n| [`MistralAIEmbeddings`](/oss/python/integrations/text_embedding/mistralai)                     | [`langchain-mistralai`](https://python.langchain.com/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html)                   |\n| [`CohereEmbeddings`](/oss/python/integrations/text_embedding/cohere)                           | [`langchain-cohere`](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.cohere.Cohere.html)                                        |\n| [`NomicEmbeddings`](/oss/python/integrations/text_embedding/nomic)                             | [`langchain-nomic`](https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html)                                   |\n| [`FakeEmbeddings`](/oss/python/integrations/text_embedding/fake)                               | [`langchain-core`](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.fake.FakeEmbeddings.html)                                  |\n| [`DatabricksEmbeddings`](/oss/python/integrations/text_embedding/databricks)                   | [`databricks-langchain`](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.DatabricksEmbeddings)   |\n| [`WatsonxEmbeddings`](/oss/python/integrations/text_embedding/ibm_watsonx)                     | [`langchain-ibm`](https://python.langchain.com/api_reference/ibm/embeddings/langchain_ibm.embeddings.WatsonxEmbeddings.html)                                       |\n| [`NVIDIAEmbeddings`](/oss/python/integrations/text_embedding/nvidia_ai_endpoints)              | [`langchain-nvidia`](https://python.langchain.com/api_reference/nvidia_ai_endpoints/embeddings/langchain_nvidia_ai_endpoints.embeddings.NVIDIAEmbeddings.html)     |\n| [`AimlapiEmbeddings`](/oss/python/integrations/text_embedding/aimlapi)                         | [`langchain-aimlapi`](https://python.langchain.com/api_reference/aimlapi/embeddings/langchain_aimlapi.embeddings.AimlapiEmbeddings.html)                           |\n\n## Caching\n\nEmbeddings can be stored or temporarily cached to avoid needing to recompute them.\n\nCaching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.\n\nThe main supported way to initialize a `CacheBackedEmbeddings` is `from_bytes_store`. It takes the following parameters:\n\n* **`underlying_embedder`**: The embedder to use for embedding.\n* **`document_embedding_cache`**: Any [`ByteStore`](/oss/python/integrations/stores/) for caching document embeddings.\n* **`batch_size`**: (optional, defaults to `None`) The number of documents to embed between store updates.\n* **`namespace`**: (optional, defaults to `\"\"`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).\n* **`query_embedding_cache`**: (optional, defaults to `None`) A [`ByteStore`](/oss/python/integrations/stores/) for caching query embeddings, or `True` to reuse the same store as `document_embedding_cache`.\n\n<Important>\n  - Always set the `namespace` parameter to avoid collisions when using different embedding models.\n  - `CacheBackedEmbeddings` does not cache query embeddings by default. To enable this, specify a `query_embedding_cache`.\n</Important>\n\n```python  theme={null}\nimport time\nfrom langchain_classic.embeddings import CacheBackedEmbeddings  # [!code highlight]\nfrom langchain_classic.storage import LocalFileStore # [!code highlight]\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\n# Create your underlying embeddings model\nunderlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.\n\n# Store persists embeddings to the local filesystem\n# This isn't for production use, but is useful for local\nstore = LocalFileStore(\"./cache/\") # [!code highlight]\n\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings,\n    store,\n    namespace=underlying_embeddings.model\n)\n\n# Example: caching a query embedding\ntic = time.time()\nprint(cached_embedder.embed_query(\"Hello, world!\"))\nprint(f\"First call took: {time.time() - tic:.2f} seconds\")\n\n# Subsequent calls use the cache\ntic = time.time()\nprint(cached_embedder.embed_query(\"Hello, world!\"))\nprint(f\"Second call took: {time.time() - tic:.2f} seconds\")\n```\n\nIn production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see [stores integrations](/oss/python/integrations/stores/) for options.\n\n## All embedding models\n\n<Columns cols={3}>\n  <Card title=\"Aleph Alpha\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/aleph_alpha\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Anyscale\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/anyscale\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Ascend\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/ascend\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AI/ML API\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/aimlapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AwaDB\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/awadb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AzureOpenAI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/azure_openai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baichuan Text Embeddings\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/baichuan\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baidu Qianfan\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/baidu_qianfan_endpoint\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baseten\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/baseten\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bedrock\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/bedrock\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BGE on Hugging Face\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/bge_huggingface\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bookend AI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/bookend\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Clarifai\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/clarifai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cloudflare Workers AI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/cloudflare_workersai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Clova Embeddings\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/clova\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cohere\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/cohere\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DashScope\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/dashscope\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Databricks\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/databricks\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DeepInfra\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/deepinfra\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"EDEN AI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/edenai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Elasticsearch\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/elasticsearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Embaas\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/embaas\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Fake Embeddings\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/fake\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FastEmbed by Qdrant\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/fastembed\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Fireworks\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/fireworks\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Gemini\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/google_generative_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Vertex AI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/google_vertex_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GPT4All\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/gpt4all\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gradient\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/gradient\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GreenNode\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/greennode\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Hugging Face\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/huggingfacehub\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IBM watsonx.ai\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/ibm_watsonx\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Infinity\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/infinity\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Instruct Embeddings\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/instruct_embeddings\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IPEX-LLM CPU\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/ipex_llm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IPEX-LLM GPU\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/ipex_llm_gpu\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Isaacus\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/isaacus\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Intel Extension for Transformers\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/itrex\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Jina\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/jina\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"John Snow Labs\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/johnsnowlabs_embedding\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LASER\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/laser\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Lindorm\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/lindorm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Llama.cpp\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/llamacpp\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LLMRails\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/llm_rails\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LocalAI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/localai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MiniMax\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/minimax\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MistralAI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/mistralai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Model2Vec\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/model2vec\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ModelScope\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/modelscope_embedding\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MosaicML\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/mosaicml\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Naver\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/naver\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nebius\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/nebius\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Netmind\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/netmind\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NLP Cloud\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/nlp_cloud\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nomic\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/nomic\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NVIDIA NIMs\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/nvidia_ai_endpoints\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oracle Cloud Infrastructure\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/oci_generative_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Ollama\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/ollama\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenClip\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/open_clip\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenAI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/openai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenVINO\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/openvino\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Optimum Intel\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/optimum_intel\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oracle AI Vector Search\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/oracleai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OVHcloud\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/ovhcloud\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pinecone Embeddings\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/pinecone\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PredictionGuard\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/predictionguard\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PremAI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/premai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SageMaker\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/sagemaker-endpoint\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SambaNova\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/sambanova\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Self Hosted\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/self-hosted\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Sentence Transformers\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/sentence_transformers\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Solar\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/solar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SpaCy\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/spacy_embedding\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SparkLLM\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/sparkllm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TensorFlow Hub\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/tensorflowhub\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Text Embeddings Inference\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/text_embeddings_inference\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TextEmbed\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/textembed\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Titan Takeoff\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/titan_takeoff\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Together AI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/together\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Upstage\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/upstage\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Volc Engine\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/volcengine\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Voyage AI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/voyageai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Xinference\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/xinference\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"YandexGPT\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/yandex\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ZhipuAI\" icon=\"link\" href=\"/oss/python/integrations/text_embedding/zhipuai\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 20635,
    "word_count": 1430
  },
  {
    "title": "Tools and toolkits",
    "source": "https://docs.langchain.com/oss/python/integrations/tools/index",
    "content": "[Tools](/oss/python/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA toolkit is a collection of tools meant to be used together.\n\n## Search\n\nThe following table shows tools that execute online searches in some shape or form:\n\n| Tool/Toolkit                                                      | Free/Paid                    | Return Data                                           |\n| ----------------------------------------------------------------- | ---------------------------- | ----------------------------------------------------- |\n| [Bing Search](/oss/python/integrations/tools/bing_search)         | Paid                         | URL, Snippet, Title                                   |\n| [Brave Search](/oss/python/integrations/tools/brave_search)       | Free                         | URL, Snippet, Title                                   |\n| [DuckDuckgoSearch](/oss/python/integrations/tools/ddg)            | Free                         | URL, Snippet, Title                                   |\n| [Exa Search](/oss/python/integrations/tools/exa_search)           | 1000 free searches/month     | URL, Author, Title, Published Date                    |\n| [Google Search](/oss/python/integrations/tools/google_search)     | Paid                         | URL, Snippet, Title                                   |\n| [Google Serper](/oss/python/integrations/tools/google_serper)     | Free                         | URL, Snippet, Title, Search Rank, Site Links          |\n| [Jina Search](/oss/python/integrations/tools/jina_search)         | 1M Response Tokens Free      | URL, Snippet, Title, Page Content                     |\n| [Mojeek Search](/oss/python/integrations/tools/mojeek_search)     | Paid                         | URL, Snippet, Title                                   |\n| [Parallel Search](/oss/python/integrations/tools/parallel_search) | Paid                         | URL, Title, Excerpts                                  |\n| [SearchApi](/oss/python/integrations/tools/searchapi)             | 100 Free Searches on Sign Up | URL, Snippet, Title, Search Rank, Site Links, Authors |\n| [SearxNG Search](/oss/python/integrations/tools/searx_search)     | Free                         | URL, Snippet, Title, Category                         |\n| [SerpAPI](/oss/python/integrations/tools/serpapi)                 | 100 Free Searches/Month      | Answer                                                |\n| [Tavily Search](/oss/python/integrations/tools/tavily_search)     | 1000 free searches/month     | URL, Content, Title, Images, Answer                   |\n| [You.com Search](/oss/python/integrations/tools/you)              | Free for 60 days             | URL, Title, Page Content                              |\n\n## Code Interpreter\n\nThe following table shows tools that can be used as code interpreters:\n\n| Tool/Toolkit                                                                                   | Supported Languages           | Sandbox Lifetime    | Supports File Uploads | Return Types | Supports Self-Hosting |\n| ---------------------------------------------------------------------------------------------- | ----------------------------- | ------------------- | --------------------- | ------------ | --------------------- |\n| [Azure Container Apps dynamic sessions](/oss/python/integrations/tools/azure_dynamic_sessions) | Python                        | 1 Hour              | ✅                     | Text, Images | ❌                     |\n| [Bearly Code Interpreter](/oss/python/integrations/tools/bearly)                               | Python                        | Resets on Execution | ✅                     | Text         | ❌                     |\n| [Riza Code Interpreter](/oss/python/integrations/tools/riza)                                   | Python, JavaScript, PHP, Ruby | Resets on Execution | ✅                     | Text         | ✅                     |\n\n## Productivity\n\nThe following table shows tools that can be used to automate tasks in productivity tools:\n\n| Tool/Toolkit                                                  | Pricing                                                                                                |\n| ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n| [Github Toolkit](/oss/python/integrations/tools/github)       | Free                                                                                                   |\n| [Gitlab Toolkit](/oss/python/integrations/tools/gitlab)       | Free for personal project                                                                              |\n| [Gmail Toolkit](/oss/python/integrations/tools/google_gmail)  | Free, with limit of 250 quota units per user per second                                                |\n| [Infobip Tool](/oss/python/integrations/tools/infobip)        | Free trial, with variable pricing after                                                                |\n| [Jira Toolkit](/oss/python/integrations/tools/jira)           | Free, with [rate limits](https://developer.atlassian.com/cloud/jira/platform/rate-limiting/)           |\n| [Office365 Toolkit](/oss/python/integrations/tools/office365) | Free with Office365, includes [rate limits](https://learn.microsoft.com/en-us/graph/throttling-limits) |\n| [Slack Toolkit](/oss/python/integrations/tools/slack)         | Free                                                                                                   |\n| [Twilio Tool](/oss/python/integrations/tools/twilio)          | Free trial, with [pay-as-you-go pricing](https://www.twilio.com/en-us/pricing) after                   |\n\n## Web Browsing\n\nThe following table shows tools that can be used to automate tasks in web browsers:\n\n| Tool/Toolkit                                                                                        | Pricing                                                     | Supports Interacting with the Browser |\n| --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | ------------------------------------- |\n| [AgentQL Toolkit](/oss/python/integrations/tools/agentql)                                           | Free trial, with pay-as-you-go and flat rate plans after    | ✅                                     |\n| [Hyperbrowser Browser Agent Tools](/oss/python/integrations/tools/hyperbrowser_browser_agent_tools) | Free trial, with flat rate plans and pre-paid credits after | ✅                                     |\n| [Hyperbrowser Web Scraping Tools](/oss/python/integrations/tools/hyperbrowser_web_scraping_tools)   | Free trial, with flat rate plans and pre-paid credits after | ❌                                     |\n| [MultiOn Toolkit](/oss/python/integrations/tools/multion)                                           | 40 free requests/day                                        | ✅                                     |\n| [Oxylabs Web Scraper API](/oss/python/integrations/tools/oxylabs)                                   | Free trial, with flat rate plans and pre-paid credits after | ❌                                     |\n| [PlayWright Browser Toolkit](/oss/python/integrations/tools/playwright)                             | Free                                                        | ✅                                     |\n| [Requests Toolkit](/oss/python/integrations/tools/requests)                                         | Free                                                        | ❌                                     |\n\n## Database\n\nThe following table shows tools that can be used to automate tasks in databases:\n\n| Tool/Toolkit                                                                    | Allowed Operations              |\n| ------------------------------------------------------------------------------- | ------------------------------- |\n| [Cassandra Database Toolkit](/oss/python/integrations/tools/cassandra_database) | SELECT and schema introspection |\n| [MCP Toolbox](/oss/python/integrations/tools/mcp_toolbox)                       | Any SQL operation               |\n| [SQLDatabase Toolkit](/oss/python/integrations/tools/sql_database)              | Any SQL operation               |\n| [Spark SQL Toolkit](/oss/python/integrations/tools/spark_sql)                   | Any SQL operation               |\n\n## Finance\n\nThe following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:\n\n| Tool/Toolkit                                  | Pricing | Capabilities                                                                      |\n| --------------------------------------------- | ------- | --------------------------------------------------------------------------------- |\n| [GOAT](/oss/python/integrations/tools/goat)   | Free    | Create and receive payments, purchase physical goods, make investments, and more. |\n| [Privy](/oss/python/integrations/tools/privy) | Free    | Create wallets with configurable permissions and execute transactions with speed. |\n\n## Integration Platforms\n\nThe following platforms provide access to multiple tools and services through a unified interface:\n\n| Tool/Toolkit                                        | Number of Integrations | Pricing             | Key Features                                               |\n| --------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- |\n| [Composio](/oss/python/integrations/tools/composio) | 500+                   | Free tier available | OAuth handling, event-driven workflows, multi-user support |\n\n## All tools and toolkits\n\n<Columns cols={3}>\n  <Card title=\"ADS4GPTs\" icon=\"link\" href=\"/oss/python/integrations/tools/ads4gpts\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AgentQL\" icon=\"link\" href=\"/oss/python/integrations/tools/agentql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AINetwork Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/ainetwork\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Alpha Vantage\" icon=\"link\" href=\"/oss/python/integrations/tools/alpha_vantage\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Amadeus Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/amadeus\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Anchor Browser\" icon=\"link\" href=\"/oss/python/integrations/tools/anchor_browser\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Apify Actor\" icon=\"link\" href=\"/oss/python/integrations/tools/apify_actors\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ArXiv\" icon=\"link\" href=\"/oss/python/integrations/tools/arxiv\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AskNews\" icon=\"link\" href=\"/oss/python/integrations/tools/asknews\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AWS Lambda\" icon=\"link\" href=\"/oss/python/integrations/tools/awslambda\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure AI Services Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/azure_ai_services\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure Cognitive Services Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/azure_cognitive_services\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure Container Apps Dynamic Sessions\" icon=\"link\" href=\"/oss/python/integrations/tools/azure_dynamic_sessions\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Shell (bash)\" icon=\"link\" href=\"/oss/python/integrations/tools/bash\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bearly Code Interpreter\" icon=\"link\" href=\"/oss/python/integrations/tools/bearly\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bing Search\" icon=\"link\" href=\"/oss/python/integrations/tools/bing_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bodo DataFrames\" icon=\"link\" href=\"/oss/python/integrations/tools/bodo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Brave Search\" icon=\"link\" href=\"/oss/python/integrations/tools/brave_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BrightData Web Scraper API\" icon=\"link\" href=\"/oss/python/integrations/tools/brightdata-webscraperapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BrightData SERP\" icon=\"link\" href=\"/oss/python/integrations/tools/brightdata_serp\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BrightData Unlocker\" icon=\"link\" href=\"/oss/python/integrations/tools/brightdata_unlocker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cassandra Database Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/cassandra_database\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"CDP\" icon=\"link\" href=\"/oss/python/integrations/tools/cdp_agentkit\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatGPT Plugins\" icon=\"link\" href=\"/oss/python/integrations/tools/chatgpt_plugins\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ClickUp Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/clickup\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cogniswitch Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/cogniswitch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Compass DeFi Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/compass\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Composio\" icon=\"link\" href=\"/oss/python/integrations/tools/composio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Connery Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/connery\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dall-E Image Generator\" icon=\"link\" href=\"/oss/python/integrations/tools/dalle_image_generator\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dappier\" icon=\"link\" href=\"/oss/python/integrations/tools/dappier\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Databricks Unity Catalog\" icon=\"link\" href=\"/oss/python/integrations/tools/databricks\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DataForSEO\" icon=\"link\" href=\"/oss/python/integrations/tools/dataforseo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dataherald\" icon=\"link\" href=\"/oss/python/integrations/tools/dataherald\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Daytona Data Analysis\" icon=\"link\" href=\"/oss/python/integrations/tools/daytona_data_analysis\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DuckDuckGo Search\" icon=\"link\" href=\"/oss/python/integrations/tools/ddg\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Discord\" icon=\"link\" href=\"/oss/python/integrations/tools/discord\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"E2B Data Analysis\" icon=\"link\" href=\"/oss/python/integrations/tools/e2b_data_analysis\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Eden AI\" icon=\"link\" href=\"/oss/python/integrations/tools/edenai_tools\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ElevenLabs Text2Speech\" icon=\"link\" href=\"/oss/python/integrations/tools/eleven_labs_tts\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Exa Search\" icon=\"link\" href=\"/oss/python/integrations/tools/exa_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"File System\" icon=\"link\" href=\"/oss/python/integrations/tools/filesystem\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Financial Datasets Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/financial_datasets\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FMP Data\" icon=\"link\" href=\"/oss/python/integrations/tools/fmp-data\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Github Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/github\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gitlab Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/gitlab\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gmail Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/google_gmail\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GOAT\" icon=\"link\" href=\"/oss/python/integrations/tools/goat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Privy\" icon=\"link\" href=\"/oss/python/integrations/tools/privy\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Golden Query\" icon=\"link\" href=\"/oss/python/integrations/tools/golden_query\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Books\" icon=\"link\" href=\"/oss/python/integrations/tools/google_books\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Calendar Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/google_calendar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud Text-to-Speech\" icon=\"link\" href=\"/oss/python/integrations/tools/google_cloud_texttospeech\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Drive\" icon=\"link\" href=\"/oss/python/integrations/tools/google_drive\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Finance\" icon=\"link\" href=\"/oss/python/integrations/tools/google_finance\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Imagen\" icon=\"link\" href=\"/oss/python/integrations/tools/google_imagen\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Jobs\" icon=\"link\" href=\"/oss/python/integrations/tools/google_jobs\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Lens\" icon=\"link\" href=\"/oss/python/integrations/tools/google_lens\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Places\" icon=\"link\" href=\"/oss/python/integrations/tools/google_places\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Scholar\" icon=\"link\" href=\"/oss/python/integrations/tools/google_scholar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Search\" icon=\"link\" href=\"/oss/python/integrations/tools/google_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Serper\" icon=\"link\" href=\"/oss/python/integrations/tools/google_serper\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Trends\" icon=\"link\" href=\"/oss/python/integrations/tools/google_trends\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gradio\" icon=\"link\" href=\"/oss/python/integrations/tools/gradio_tools\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GraphQL\" icon=\"link\" href=\"/oss/python/integrations/tools/graphql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"HuggingFace Hub Tools\" icon=\"link\" href=\"/oss/python/integrations/tools/huggingface_tools\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Human as a Tool\" icon=\"link\" href=\"/oss/python/integrations/tools/human_tools\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Hyperbrowser Browser Agent Tools\" icon=\"link\" href=\"/oss/python/integrations/tools/hyperbrowser_browser_agent_tools\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Hyperbrowser Web Scraping Tools\" icon=\"link\" href=\"/oss/python/integrations/tools/hyperbrowser_web_scraping_tools\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IBM watsonx.ai\" icon=\"link\" href=\"/oss/python/integrations/tools/ibm_watsonx\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IBM watsonx.ai (SQL)\" icon=\"link\" href=\"/oss/python/integrations/tools/ibm_watsonx_sql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IFTTT WebHooks\" icon=\"link\" href=\"/oss/python/integrations/tools/ifttt\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Infobip\" icon=\"link\" href=\"/oss/python/integrations/tools/infobip\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Ionic Shopping Tool\" icon=\"link\" href=\"/oss/python/integrations/tools/ionic_shopping\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Jenkins\" icon=\"link\" href=\"/oss/python/integrations/tools/jenkins\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Jina Search\" icon=\"link\" href=\"/oss/python/integrations/tools/jina_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Jira Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/jira\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JSON Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/json\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Lemon Agent\" icon=\"link\" href=\"/oss/python/integrations/tools/lemonai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Linkup Search Tool\" icon=\"link\" href=\"/oss/python/integrations/tools/linkup_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Memgraph\" icon=\"link\" href=\"/oss/python/integrations/tools/memgraph\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Memorize\" icon=\"link\" href=\"/oss/python/integrations/tools/memorize\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Mojeek Search\" icon=\"link\" href=\"/oss/python/integrations/tools/mojeek_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MultiOn Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/multion\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NASA Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/nasa\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Naver Search\" icon=\"link\" href=\"/oss/python/integrations/tools/naver_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nuclia Understanding\" icon=\"link\" href=\"/oss/python/integrations/tools/nuclia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NVIDIA Riva\" icon=\"link\" href=\"/oss/python/integrations/tools/nvidia_riva\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Office365 Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/office365\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenAPI Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/openapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Natural Language API Toolkits\" icon=\"link\" href=\"/oss/python/integrations/tools/openapi_nla\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenGradient\" icon=\"link\" href=\"/oss/python/integrations/tools/opengradient_toolkit\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenWeatherMap\" icon=\"link\" href=\"/oss/python/integrations/tools/openweathermap\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oracle AI Vector Search\" icon=\"link\" href=\"/oss/python/integrations/tools/oracleai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oxylabs\" icon=\"link\" href=\"/oss/python/integrations/tools/oxylabs\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pandas Dataframe\" icon=\"link\" href=\"/oss/python/integrations/tools/pandas\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Passio NutritionAI\" icon=\"link\" href=\"/oss/python/integrations/tools/passio_nutrition_ai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Parallel Extract\" icon=\"link\" href=\"/oss/python/integrations/tools/parallel_extract\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Parallel Search\" icon=\"link\" href=\"/oss/python/integrations/tools/parallel_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Permit\" icon=\"link\" href=\"/oss/python/integrations/tools/permit\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PlayWright Browser Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/playwright\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Polygon IO Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/polygon\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PowerBI Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/powerbi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Prolog\" icon=\"link\" href=\"/oss/python/integrations/tools/prolog_tool\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PubMed\" icon=\"link\" href=\"/oss/python/integrations/tools/pubmed\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Python REPL\" icon=\"link\" href=\"/oss/python/integrations/tools/python\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Reddit Search\" icon=\"link\" href=\"/oss/python/integrations/tools/reddit_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Requests Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/requests\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Riza Code Interpreter\" icon=\"link\" href=\"/oss/python/integrations/tools/riza\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Robocorp Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/robocorp\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Salesforce\" icon=\"link\" href=\"/oss/python/integrations/tools/salesforce\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SceneXplain\" icon=\"link\" href=\"/oss/python/integrations/tools/sceneXplain\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ScrapeGraph\" icon=\"link\" href=\"/oss/python/integrations/tools/scrapegraph\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Scrapeless Crawl\" icon=\"link\" href=\"/oss/python/integrations/tools/scrapeless_crawl\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Scrapeless Scraping API\" icon=\"link\" href=\"/oss/python/integrations/tools/scrapeless_scraping_api\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Scrapeless Universal Scraping\" icon=\"link\" href=\"/oss/python/integrations/tools/scrapeless_universal_scraping\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SearchApi\" icon=\"link\" href=\"/oss/python/integrations/tools/searchapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SearxNG Search\" icon=\"link\" href=\"/oss/python/integrations/tools/searx_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Semantic Scholar API\" icon=\"link\" href=\"/oss/python/integrations/tools/semanticscholar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SerpAPI\" icon=\"link\" href=\"/oss/python/integrations/tools/serpapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Slack Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/slack\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Spark SQL Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/spark_sql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SQLDatabase Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/sql_database\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"StackExchange\" icon=\"link\" href=\"/oss/python/integrations/tools/stackexchange\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Steam Toolkit\" icon=\"link\" href=\"/oss/python/integrations/tools/steam\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Stripe\" icon=\"link\" href=\"/oss/python/integrations/tools/stripe\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tableau\" icon=\"link\" href=\"/oss/python/integrations/tools/tableau\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Taiga\" icon=\"link\" href=\"/oss/python/integrations/tools/taiga\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Extract\" icon=\"link\" href=\"/oss/python/integrations/tools/tavily_extract\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Search\" icon=\"link\" href=\"/oss/python/integrations/tools/tavily_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tilores\" icon=\"link\" href=\"/oss/python/integrations/tools/tilores\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MCP Toolbox\" icon=\"link\" href=\"/oss/python/integrations/tools/mcp_toolbox\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Twilio\" icon=\"link\" href=\"/oss/python/integrations/tools/twilio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Upstage\" icon=\"link\" href=\"/oss/python/integrations/tools/upstage_groundedness_check\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Valthera\" icon=\"link\" href=\"/oss/python/integrations/tools/valthera\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ValyuContext\" icon=\"link\" href=\"/oss/python/integrations/tools/valyu_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vectara\" icon=\"link\" href=\"/oss/python/integrations/tools/vectara\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikidata\" icon=\"link\" href=\"/oss/python/integrations/tools/wikidata\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia\" icon=\"link\" href=\"/oss/python/integrations/tools/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wolfram Alpha\" icon=\"link\" href=\"/oss/python/integrations/tools/wolfram_alpha\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"WRITER Tools\" icon=\"link\" href=\"/oss/python/integrations/tools/writer\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Yahoo Finance News\" icon=\"link\" href=\"/oss/python/integrations/tools/yahoo_finance_news\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"You.com Search\" icon=\"link\" href=\"/oss/python/integrations/tools/you\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"YouTube\" icon=\"link\" href=\"/oss/python/integrations/tools/youtube\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zapier Natural Language Actions\" icon=\"link\" href=\"/oss/python/integrations/tools/zapier\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ZenGuard AI\" icon=\"link\" href=\"/oss/python/integrations/tools/zenguard\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n<Info>\n  If you'd like to contribute an integration, see [Contributing integrations](/oss/python/contributing#add-a-new-integration).\n</Info>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/tools/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 29101,
    "word_count": 2137
  },
  {
    "title": "Vector stores",
    "source": "https://docs.langchain.com/oss/python/integrations/vectorstores/index",
    "content": "## Overview\n\nA vector store stores [embedded](/oss/python/integrations/text_embedding) data and performs similarity search.\n\n```mermaid  theme={null}\nflowchart LR\n\n    subgraph \"📥 Indexing phase (store)\"\n        A[📄 Documents] --> B[🔢 Embedding model]\n        B --> C[🔘 Embedding vectors]\n        C --> D[(Vector store)]\n    end\n\n    subgraph \"📤 Query phase (retrieval)\"\n        E[❓ Query text] --> F[🔢 Embedding model]\n        F --> G[🔘 Query vector]\n        G --> H[🔍 Similarity search]\n        H --> D\n        D --> I[📄 Top-k results]\n    end\n```\n\n### Interface\n\nLangChain provides a unified interface for vector stores, allowing you to:\n\n* `add_documents` - Add documents to the store.\n* `delete` - Remove stored documents by ID.\n* `similarity_search` - Query for semantically similar documents.\n\nThis abstraction lets you switch between different implementations without altering your application logic.\n\n### Initialization\n\nTo initialize a vector store, provide it with an embedding model:\n\n```python  theme={null}\nfrom langchain_core.vectorstores import InMemoryVectorStore\nvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\n```\n\n### Adding documents\n\nAdd [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects (holding `page_content` and optional metadata) like so:\n\n```python  theme={null}\nvector_store.add_documents(documents=[doc1, doc2], ids=[\"id1\", \"id2\"])\n```\n\n### Deleting documents\n\nDelete by specifying IDs:\n\n```python  theme={null}\nvector_store.delete(ids=[\"id1\"])\n```\n\n### Similarity search\n\nIssue a semantic query using `similarity_search`, which returns the closest embedded documents:\n\n```python  theme={null}\nsimilar_docs = vector_store.similarity_search(\"your query here\")\n```\n\nMany vector stores support parameters like:\n\n* `k` — number of results to return\n* `filter` — conditional filtering based on metadata\n\n### Similarity metrics & indexing\n\nEmbedding similarity may be computed using:\n\n* **Cosine similarity**\n* **Euclidean distance**\n* **Dot product**\n\nEfficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.\n\n### Metadata filtering\n\nFiltering by metadata (e.g., source, date) can refine search results:\n\n```python  theme={null}\nvector_store.similarity_search(\n  \"query\",\n  k=3,\n  filter={\"source\": \"tweets\"}\n)\n```\n\n<important>\n  Support for metadata-based filtering varies between implementations.\n  Check the documentation of your chosen vector store for details.\n</important>\n\n## Top integrations\n\n**Select embedding model:**\n\n<AccordionGroup>\n  <Accordion title=\"OpenAI\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-openai\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-openai\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"OPENAI_API_KEY\"):\n      os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\n    from langchain_openai import OpenAIEmbeddings\n\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Azure\">\n    ```bash  theme={null}\n    pip install -qU langchain-azure-ai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n\n    from langchain_openai import AzureOpenAIEmbeddings\n\n    embeddings = AzureOpenAIEmbeddings(\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n        openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"Google Gemini\">\n    ```bash  theme={null}\n    pip install -qU langchain-google-genai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"GOOGLE_API_KEY\"):\n      os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\n    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Google Vertex\">\n    ```bash  theme={null}\n    pip install -qU langchain-google-vertexai\n    ```\n\n    ```python  theme={null}\n    from langchain_google_vertexai import VertexAIEmbeddings\n\n    embeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"AWS\">\n    ```bash  theme={null}\n    pip install -qU langchain-aws\n    ```\n\n    ```python  theme={null}\n    from langchain_aws import BedrockEmbeddings\n\n    embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"HuggingFace\">\n    ```bash  theme={null}\n    pip install -qU langchain-huggingface\n    ```\n\n    ```python  theme={null}\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Ollama\">\n    ```bash  theme={null}\n    pip install -qU langchain-ollama\n    ```\n\n    ```python  theme={null}\n    from langchain_ollama import OllamaEmbeddings\n\n    embeddings = OllamaEmbeddings(model=\"llama3\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Cohere\">\n    ```bash  theme={null}\n    pip install -qU langchain-cohere\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"COHERE_API_KEY\"):\n      os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n\n    from langchain_cohere import CohereEmbeddings\n\n    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Mistral AI\">\n    ```bash  theme={null}\n    pip install -qU langchain-mistralai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"MISTRALAI_API_KEY\"):\n      os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\n\n    from langchain_mistralai import MistralAIEmbeddings\n\n    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Nomic\">\n    ```bash  theme={null}\n    pip install -qU langchain-nomic\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NOMIC_API_KEY\"):\n      os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\n\n    from langchain_nomic import NomicEmbeddings\n\n    embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"NVIDIA\">\n    ```bash  theme={null}\n    pip install -qU langchain-nvidia-ai-endpoints\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NVIDIA_API_KEY\"):\n      os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n\n    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\n    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Voyage AI\">\n    ```bash  theme={null}\n    pip install -qU langchain-voyageai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"VOYAGE_API_KEY\"):\n      os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n\n    from langchain-voyageai import VoyageAIEmbeddings\n\n    embeddings = VoyageAIEmbeddings(model=\"voyage-3\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"IBM watsonx\">\n    ```bash  theme={null}\n    pip install -qU langchain-ibm\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"WATSONX_APIKEY\"):\n      os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\n\n    from langchain_ibm import WatsonxEmbeddings\n\n    embeddings = WatsonxEmbeddings(\n        model_id=\"ibm/slate-125m-english-rtrvr\",\n        url=\"https://us-south.ml.cloud.ibm.com\",\n        project_id=\"<WATSONX PROJECT_ID>\",\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"Fake\">\n    ```bash  theme={null}\n    pip install -qU langchain-core\n    ```\n\n    ```python  theme={null}\n    from langchain_core.embeddings import DeterministicFakeEmbedding\n\n    embeddings = DeterministicFakeEmbedding(size=4096)\n    ```\n  </Accordion>\n\n  <Accordion title=\"xAI\">\n    ```bash  theme={null}\n    pip install -qU langchain-xai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"XAI_API_KEY\"):\n      os.environ[\"XAI_API_KEY\"] = getpass.getpass(\"Enter API key for xAI: \")\n\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\"grok-2\", model_provider=\"xai\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Perplexity\">\n    ```bash  theme={null}\n    pip install -qU langchain-perplexity\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"PPLX_API_KEY\"):\n      os.environ[\"PPLX_API_KEY\"] = getpass.getpass(\"Enter API key for Perplexity: \")\n\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\"llama-3.1-sonar-small-128k-online\", model_provider=\"perplexity\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"DeepSeek\">\n    ```bash  theme={null}\n    pip install -qU langchain-deepseek\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"DEEPSEEK_API_KEY\"):\n      os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter API key for DeepSeek: \")\n\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n    ```\n  </Accordion>\n</AccordionGroup>\n\n**Select vector store:**\n\n<AccordionGroup>\n  <Accordion title=\"In-memory\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-core\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-core\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_core.vectorstores import InMemoryVectorStore\n\n    vector_store = InMemoryVectorStore(embeddings)\n    ```\n  </Accordion>\n\n  <Accordion title=\"Astra DB\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-astradb\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-astradb\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_astradb import AstraDBVectorStore\n\n    vector_store = AstraDBVectorStore(\n        embedding=embeddings,\n        api_endpoint=ASTRA_DB_API_ENDPOINT,\n        collection_name=\"astra_vector_langchain\",\n        token=ASTRA_DB_APPLICATION_TOKEN,\n        namespace=ASTRA_DB_NAMESPACE,\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"Azure Cosmos DB NoSQL\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-azure-ai azure-cosmos\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-azure-ai\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_azure_ai.vectorstores.azure_cosmos_db_no_sql import (\n        AzureCosmosDBNoSqlVectorSearch,\n    )\n    vector_search = AzureCosmosDBNoSqlVectorSearch.from_documents(\n        documents=docs,\n        embedding=openai_embeddings,\n        cosmos_client=cosmos_client,\n        database_name=database_name,\n        container_name=container_name,\n        vector_embedding_policy=vector_embedding_policy,\n        full_text_policy=full_text_policy,\n        indexing_policy=indexing_policy,\n        cosmos_container_properties=cosmos_container_properties,\n        cosmos_database_properties={},\n        full_text_search_enabled=True,\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"Azure Cosmos DB Mongo vCore\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-azure-ai pymongo\n      ```\n\n      ```bash uv theme={null}\n      uv add pymongo\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_azure_ai.vectorstores.azure_cosmos_db_mongo_vcore import (\n        AzureCosmosDBMongoVCoreVectorSearch,\n    )\n\n    vectorstore = AzureCosmosDBMongoVCoreVectorSearch.from_documents(\n        docs,\n        openai_embeddings,\n        collection=collection,\n        index_name=INDEX_NAME,\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"Chroma\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-chroma\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-chroma\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_chroma import Chroma\n\n    vector_store = Chroma(\n        collection_name=\"example_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"FAISS\">\n    ```bash  theme={null}\n    pip install -qU langchain-community\n    ```\n\n    ```python  theme={null}\n    import faiss\n    from langchain_community.docstore.in_memory import InMemoryDocstore\n    from langchain_community.vectorstores import FAISS\n\n    embedding_dim = len(embeddings.embed_query(\"hello world\"))\n    index = faiss.IndexFlatL2(embedding_dim)\n\n    vector_store = FAISS(\n        embedding_function=embeddings,\n        index=index,\n        docstore=InMemoryDocstore(),\n        index_to_docstore_id={},\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"Milvus\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-milvus\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-milvus\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_milvus import Milvus\n\n    URI = \"./milvus_example.db\"\n\n    vector_store = Milvus(\n        embedding_function=embeddings,\n        connection_args={\"uri\": URI},\n        index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"MongoDB\">\n    ```bash  theme={null}\n    pip install -qU langchain-mongodb\n    ```\n\n    ```python  theme={null}\n    from langchain_mongodb import MongoDBAtlasVectorSearch\n\n    vector_store = MongoDBAtlasVectorSearch(\n        embedding=embeddings,\n        collection=MONGODB_COLLECTION,\n        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n        relevance_score_fn=\"cosine\",\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"PGVector\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-postgres\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-postgres\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_postgres import PGVector\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=\"my_docs\",\n        connection=\"postgresql+psycopg://...\"\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"PGVectorStore\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-postgres\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-postgres\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_postgres import PGEngine, PGVectorStore\n\n    $engine = PGEngine.from_connection_string(\n        url=\"postgresql+psycopg://...\"\n    )\n\n    vector_store = PGVectorStore.create_sync(\n        engine=pg_engine,\n        table_name='test_table',\n        embedding_service=embedding\n    )\n    ```\n  </Accordion>\n\n  <Accordion title=\"Pinecone\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-pinecone\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-pinecone\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from langchain_pinecone import PineconeVectorStore\n    from pinecone import Pinecone\n\n    pc = Pinecone(api_key=...)\n    index = pc.Index(index_name)\n\n    vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n    ```\n  </Accordion>\n\n  <Accordion title=\"Qdrant\">\n    <CodeGroup>\n      ```bash pip theme={null}\n      pip install -qU langchain-qdrant\n      ```\n\n      ```bash uv theme={null}\n      uv add langchain-qdrant\n      ```\n    </CodeGroup>\n\n    ```python  theme={null}\n    from qdrant_client.models import Distance, VectorParams\n    from langchain_qdrant import QdrantVectorStore\n    from qdrant_client import QdrantClient\n\n    client = QdrantClient(\":memory:\")\n\n    vector_size = len(embeddings.embed_query(\"sample text\"))\n\n    if not client.collection_exists(\"test\"):\n        client.create_collection(\n            collection_name=\"test\",\n            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n        )\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=\"test\",\n        embedding=embeddings,\n    )\n    ```\n  </Accordion>\n</AccordionGroup>\n\n| Vectorstore                                                                                                                                          | Delete by ID | Filtering | Search by Vector | Search with score | Async | Passes Standard Tests | Multi Tenancy | IDs in add Documents |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | --------- | ---------------- | ----------------- | ----- | --------------------- | ------------- | -------------------- |\n| [`AstraDBVectorStore`](/oss/python/integrations/vectorstores/astradb)                                                                                | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |\n| [`AzureCosmosDBNoSqlVectorStore`](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql)                                                      | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ✅             | ✅                    |\n| [`AzureCosmosDBMongoVCoreVectorStore`](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore)                                            | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ✅             | ✅                    |\n| [`Chroma`](/oss/python/integrations/vectorstores/chroma)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |\n| [`Clickhouse`](/oss/python/integrations/vectorstores/clickhouse)                                                                                     | ✅            | ✅         | ❌                | ✅                 | ❌     | ❌                     | ❌             | ✅                    |\n| [`CouchbaseSearchVectorStore`](/oss/python/integrations/vectorstores/couchbase)                                                                      | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |\n| [`DatabricksVectorSearch`](/oss/python/integrations/vectorstores/databricks_vector_search)                                                           | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |\n| [`ElasticsearchStore`](/oss/python/integrations/vectorstores/elasticsearch)                                                                          | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |\n| [`FAISS`](/oss/python/integrations/vectorstores/faiss)                                                                                               | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |\n| [`InMemoryVectorStore`](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | ✅            | ✅         | ❌                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |\n| [`Milvus`](/oss/python/integrations/vectorstores/milvus)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |\n| [`Moorcheh`](/oss/python/integrations/vectorstores/moorcheh)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |\n| [`MongoDBAtlasVectorSearch`](/oss/python/integrations/vectorstores/mongodb_atlas)                                                                    | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |\n| [`openGauss`](/oss/python/integrations/vectorstores/opengauss)                                                                                       | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ❌             | ✅                    |\n| [`PGVector`](/oss/python/integrations/vectorstores/pgvector)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |\n| [`PGVectorStore`](/oss/python/integrations/vectorstores/pgvectorstore)                                                                               | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |\n| [`PineconeVectorStore`](/oss/python/integrations/vectorstores/pinecone)                                                                              | ✅            | ✅         | ✅                | ❌                 | ✅     | ❌                     | ❌             | ✅                    |\n| [`QdrantVectorStore`](/oss/python/integrations/vectorstores/qdrant)                                                                                  | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |\n| [`Weaviate`](/oss/python/integrations/vectorstores/weaviate)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |\n| [`SQLServer`](/oss/python/integrations/vectorstores/sqlserver)                                                                                       | ✅            | ✅         | ✅                | ✅                 | ❌     | ❌                     | ❌             | ✅                    |\n| [`ZeusDB`](/oss/python/integrations/vectorstores/zeusdb)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |\n| [`Oracle AI Vector Search`](/oss/python/integrations/vectorstores/oracle)                                                                            | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |\n\n## All vector stores\n\n<Columns cols={3}>\n  <Card title=\"Activeloop Deep Lake\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/activeloop_deeplake\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Alibaba Cloud OpenSearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/alibabacloud_opensearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AnalyticDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/analyticdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Annoy\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/annoy\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Apache Doris\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/apache_doris\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ApertureDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/aperturedb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Astra DB Vector Store\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/astradb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Atlas\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/atlas\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AwaDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/awadb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure Cosmos DB Mongo vCore\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure Cosmos DB No SQL\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure Database for PostgreSQL - Flexible Server\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/azure_db_for_postgresql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure AI Search\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/azuresearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bagel\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/bagel\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BagelDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/bageldb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baidu Cloud ElasticSearch VectorSearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/baiducloud_vector_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Baidu VectorDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/baiduvectordb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Apache Cassandra\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/cassandra\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Chroma\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/chroma\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Clarifai\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/clarifai\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ClickHouse\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/clickhouse\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Couchbase\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/couchbase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DashVector\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/dashvector\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Databricks\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/databricks_vector_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IBM Db2\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/db2\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DingoDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/dingo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DocArray HnswSearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/docarray_hnsw\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DocArray InMemorySearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/docarray_in_memory\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Amazon Document DB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/documentdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DuckDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/duckdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"China Mobile ECloud ElasticSearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/ecloud_vector_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Elasticsearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/elasticsearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Epsilla\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/epsilla\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Faiss\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/faiss\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Faiss (Async)\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/faiss_async\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FalkorDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/falkordbvector\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gel\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/gel\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google AlloyDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_alloydb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google BigQuery Vector Search\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_bigquery_vector_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud SQL for MySQL\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_cloud_sql_mysql\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Cloud SQL for PostgreSQL\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_cloud_sql_pg\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Firestore\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_firestore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Memorystore for Redis\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_memorystore_redis\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Spanner\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_spanner\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Vertex AI Feature Store\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_vertex_ai_feature_store\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Vertex AI Vector Search\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_vertex_ai_vector_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Hippo\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/hippo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Hologres\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/hologres\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Jaguar Vector Database\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/jaguar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Kinetica\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/kinetica\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LanceDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/lancedb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Lantern\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/lantern\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Lindorm\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/lindorm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LLMRails\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/llm_rails\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ManticoreSearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/manticore_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MariaDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/mariadb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Marqo\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/marqo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Meilisearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/meilisearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Amazon MemoryDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/memorydb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Milvus\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/milvus\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Momento Vector Index\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/momento_vector_index\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Moorcheh\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/moorcheh\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MongoDB Atlas\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/mongodb_atlas\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"MyScale\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/myscale\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Neo4j Vector Index\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/neo4jvector\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NucliaDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/nucliadb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oceanbase\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/oceanbase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"openGauss\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/opengauss\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"OpenSearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/opensearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oracle AI Vector Search\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/oracle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pathway\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/pathway\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Postgres Embedding\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/pgembedding\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PGVecto.rs\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/pgvecto_rs\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PGVector\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/pgvector\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PGVectorStore\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/pgvectorstore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pinecone\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/pinecone\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pinecone (sparse)\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/pinecone_sparse\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Qdrant\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/qdrant\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Relyt\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/relyt\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Rockset\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/rockset\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SAP HANA Cloud Vector Engine\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/sap_hanavector\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ScaNN\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/google_scann\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SemaDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/semadb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SingleStore\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/singlestore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"scikit-learn\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/sklearn\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SQLiteVec\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/sqlitevec\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SQLite-VSS\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/sqlitevss\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SQLServer\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/sqlserver\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"StarRocks\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/starrocks\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Supabase\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/supabase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SurrealDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/surrealdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tablestore\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/tablestore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tair\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/tair\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tencent Cloud VectorDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/tencentvectordb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Teradata VectorStore\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/teradata\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ThirdAI NeuralDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/thirdai_neuraldb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TiDB Vector\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/tidb_vector\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tigris\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/tigris\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TileDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/tiledb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Timescale Vector\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/timescalevector\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Typesense\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/typesense\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Upstash Vector\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/upstash\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"USearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/usearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vald\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/vald\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"VDMS\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/vdms\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vearch\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/vearch\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vectara\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/vectara\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vespa\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/vespa\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"viking DB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/vikingdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"vlite\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/vlite\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Weaviate\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/weaviate\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Xata\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/xata\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"YDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/ydb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Yellowbrick\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/yellowbrick\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zep\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/zep\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zep Cloud\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/zep_cloud\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ZeusDB\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/zeusdb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zilliz\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/zilliz\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Oracle AI Vector Search\" icon=\"link\" href=\"/oss/python/integrations/vectorstores/oracle\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/vectorstores/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/integrations",
    "char_count": 39881,
    "word_count": 2892
  },
  {
    "title": "LangChain Academy",
    "source": "https://docs.langchain.com/oss/python/langchain/academy",
    "content": "***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/academy.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 352,
    "word_count": 31
  },
  {
    "title": "Agents",
    "source": "https://docs.langchain.com/oss/python/langchain/agents",
    "content": "Agents combine language models with [tools](/oss/python/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) builds a **graph**-based agent runtime using [LangGraph](/oss/python/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/python/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/python/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```python wrap theme={null}\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=tools\n)\n```\n\n<Tip>\n  Model identifier strings support automatic inference (e.g., `\"gpt-5\"` will be inferred as `\"openai:gpt-5\"`). Refer to the [reference](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) to see a full list of model identifier string mappings.\n</Tip>\n\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI). See [Chat models](/oss/python/integrations/chat) for other available chat model classes.\n\n```python wrap theme={null}\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-5\",\n    temperature=0.1,\n    max_tokens=1000,\n    timeout=30\n    # ... (other params)\n)\nagent = create_agent(model, tools=tools)\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific [parameters](/oss/python/langchain/models#parameters) like `temperature`, `max_tokens`, `timeouts`, `base_url`, and other provider-specific settings. Refer to the [reference](/oss/python/integrations/providers/all_providers) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware using the [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) decorator that modifies the model in the request:\n\n```python  theme={null}\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\n\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"Choose model based on conversation complexity.\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 10:\n        # Use an advanced model for longer conversations\n        model = advanced_model\n    else:\n        model = basic_model\n\n    return handler(request.override(model=model))\n\nagent = create_agent(\n    model=basic_model,  # Default model\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n```\n\n<Warning>\n  Pre-bound models (models with [`bind_tools`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.bind_tools) already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\n</Warning>\n\n<Tip>\n  For model configuration details, see [Models](/oss/python/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/python/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/python/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n<Tip>\n  Tools can be specified as plain Python functions or <Tooltip tip=\"A method that can suspend execution and resume at a later time\">coroutines</Tooltip>.\n\n  The [tool decorator](/oss/python/langchain/tools#create-tools) can be used to customize tool names, descriptions, argument schemas, and other properties.\n</Tip>\n\n```python wrap theme={null}\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) decorator to create middleware:\n\n```python wrap theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # Return a custom error message to the model\n        return ToolMessage(\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search, get_weather],\n    middleware=[handle_tool_errors]\n)\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) with the custom error message when a tool fails:\n\n```python  theme={null}\n[\n    ...\n    ToolMessage(\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\n        tool_call_id=\"...\"\n    ),\n    ...\n]\n```\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/python/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)) parameter can be provided as a string:\n\n```python wrap theme={null}\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n)\n```\n\nWhen no [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)) is provided, the agent will infer its task from the messages directly.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/python/langchain/middleware).\n\nThe [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) decorator creates middleware that generates system prompts based on the model request:\n\n```python wrap theme={null}\nfrom typing import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest) -> str:\n    \"\"\"Generate system prompt based on user role.\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} Provide detailed technical responses.\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# The system prompt will be set dynamically based on context\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n    context={\"user_role\": \"expert\"}\n)\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/python/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/python/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/python/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/python/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```python  theme={null}\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n)\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/python/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/python/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the `response_format` parameter.\n\n#### ToolStrategy\n\n`ToolStrategy` uses artificial tool calling to generate structured output. This works with any model that supports tool calling:\n\n```python wrap theme={null}\nfrom pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[search_tool],\n    response_format=ToolStrategy(ContactInfo)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n```\n\n#### ProviderStrategy\n\n`ProviderStrategy` uses the model provider's native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\n\n```python wrap theme={null}\nfrom langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n```\n\n<Note>\n  As of `langchain 1.0`, simply passing a schema (e.g., `response_format=ContactInfo`) is no longer supported. You must explicitly use `ToolStrategy` or `ProviderStrategy`.\n</Note>\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/python/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/python/langchain/short-term-memory) of the agent:\n\nCustom state schemas must extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) as a `TypedDict`.\n\nThere are two ways to define custom state:\n\n1. Via [middleware](/oss/python/langchain/middleware) (preferred)\n2. Via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)\n\n#### Defining state via middleware\n\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\n\n```python  theme={null}\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\nfrom typing import Any\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1, tool2]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        ...\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[CustomMiddleware()]\n)\n\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n#### Defining state via `state_schema`\n\nUse the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter as a shortcut to define custom state that is only used in tools.\n\n```python  theme={null}\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n<Note>\n  As of `langchain 1.0`, custom state schemas **must** be `TypedDict` types. Pydantic models and dataclasses are no longer supported. See the [v1 migration guide](/oss/python/migrate/langchain-v1#state-type-restrictions) for more details.\n</Note>\n\n<Note>\n  Defining custom state via middleware is preferred over defining it via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\n\n  [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) is still supported for backwards compatibility on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent).\n</Note>\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/python/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/python/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python  theme={null}\nfor chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n}, stream_mode=\"values\"):\n    # Each chunk contains the full state at that point\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        print(f\"Agent: {latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/python/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/python/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including decorators like [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model), [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model), and [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call), see [Middleware](/oss/python/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 21513,
    "word_count": 2193
  },
  {
    "title": "Component architecture",
    "source": "https://docs.langchain.com/oss/python/langchain/component-architecture",
    "content": "LangChain's power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.\n\n## Core component ecosystem\n\nThe diagram below shows how LangChain's major components connect to form complete AI applications:\n\n```mermaid  theme={null}\ngraph TD\n    %% Input processing\n    subgraph \"📥 Input processing\"\n        A[Text input] --> B[Document loaders]\n        B --> C[Text splitters]\n        C --> D[Documents]\n    end\n\n    %% Embedding & storage\n    subgraph \"🔢 Embedding & storage\"\n        D --> E[Embedding models]\n        E --> F[Vectors]\n        F --> G[(Vector stores)]\n    end\n\n    %% Retrieval\n    subgraph \"🔍 Retrieval\"\n        H[User Query] --> I[Embedding models]\n        I --> J[Query vector]\n        J --> K[Retrievers]\n        K --> G\n        G --> L[Relevant context]\n    end\n\n    %% Generation\n    subgraph \"🤖 Generation\"\n        M[Chat models] --> N[Tools]\n        N --> O[Tool results]\n        O --> M\n        L --> M\n        M --> P[AI response]\n    end\n\n    %% Orchestration\n    subgraph \"🎯 Orchestration\"\n        Q[Agents] --> M\n        Q --> N\n        Q --> K\n        Q --> R[Memory]\n    end\n```\n\n### How components connect\n\nEach component layer builds on the previous ones:\n\n1. **Input processing** – Transform raw data into structured documents\n2. **Embedding & storage** – Convert text into searchable vector representations\n3. **Retrieval** – Find relevant information based on user queries\n4. **Generation** – Use AI models to create responses, optionally with tools\n5. **Orchestration** – Coordinate everything through agents and memory systems\n\n## Component categories\n\nLangChain organizes components into these main categories:\n\n| Category                                                             | Purpose                     | Key Components                      | Use Cases                                          |\n| -------------------------------------------------------------------- | --------------------------- | ----------------------------------- | -------------------------------------------------- |\n| **[Models](/oss/python/langchain/models)**                           | AI reasoning and generation | Chat models, LLMs, Embedding models | Text generation, reasoning, semantic understanding |\n| **[Tools](/oss/python/langchain/tools)**                             | External capabilities       | APIs, databases, etc.               | Web search, data access, computations              |\n| **[Agents](/oss/python/langchain/agents)**                           | Orchestration and reasoning | ReAct agents, tool calling agents   | Nondeterministic workflows, decision making        |\n| **[Memory](/oss/python/langchain/short-term-memory)**                | Context preservation        | Message history, custom state       | Conversations, stateful interactions               |\n| **[Retrievers](/oss/python/integrations/retrievers)**                | Information access          | Vector retrievers, web retrievers   | RAG, knowledge base search                         |\n| **[Document processing](/oss/python/integrations/document_loaders)** | Data ingestion              | Loaders, splitters, transformers    | PDF processing, web scraping                       |\n| **[Vector Stores](/oss/python/integrations/vectorstores)**           | Semantic search             | Chroma, Pinecone, FAISS             | Similarity search, embeddings storage              |\n\n## Common patterns\n\n### RAG (Retrieval-Augmented Generation)\n\n```mermaid  theme={null}\ngraph LR\n    A[User question] --> B[Retriever]\n    B --> C[Relevant docs]\n    C --> D[Chat model]\n    A --> D\n    D --> E[Informed response]\n```\n\n### Agent with tools\n\n```mermaid  theme={null}\ngraph LR\n    A[User request] --> B[Agent]\n    B --> C{Need tool?}\n    C -->|Yes| D[Call tool]\n    D --> E[Tool result]\n    E --> B\n    C -->|No| F[Final answer]\n```\n\n### Multi-agent system\n\n```mermaid  theme={null}\ngraph LR\n    A[Complex Task] --> B[Supervisor agent]\n    B --> C[Specialist agent 1]\n    B --> D[Specialist agent 2]\n    C --> E[Results]\n    D --> E\n    E --> B\n    B --> F[Coordinated response]\n```\n\n## Learn more\n\nNow that you understand how components relate to each other, explore specific areas:\n\n* [Building your first RAG system](/oss/python/langchain/knowledge-base)\n* [Creating agents](/oss/python/langchain/agents)\n* [Working with tools](/oss/python/langchain/tools)\n* [Setting up memory](/oss/python/langchain/short-term-memory)\n* [Browse integrations](/oss/python/integrations/providers/overview)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/component-architecture.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 4984,
    "word_count": 533
  },
  {
    "title": "Context engineering in agents",
    "source": "https://docs.langchain.com/oss/python/langchain/context-engineering",
    "content": "## Overview\n\nThe hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n\n### Why do agents fail?\n\nWhen agents fail, it's usually because the LLM call inside the agent took the wrong action / didn't do what we expected. LLMs fail for one of two reasons:\n\n1. The underlying LLM is not capable enough\n2. The \"right\" context was not passed to the LLM\n\nMore often than not - it's actually the second reason that causes agents to not be reliable.\n\n**Context engineering** is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of \"right\" context is the number one blocker for more reliable agents, and LangChain's agent abstractions are uniquely designed to facilitate context engineering.\n\n<Tip>\n  New to context engineering? Start with the [conceptual overview](/oss/python/concepts/context) to understand the different types of context and when to use them.\n</Tip>\n\n### The agent loop\n\nA typical agent loop consists of two main steps:\n\n1. **Model call** - calls the LLM with a prompt and available tools, returns either a response or a request to execute tools\n2. **Tool execution** - executes the tools that the LLM requested, returns tool results\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063\" alt=\"Core agent loop diagram\" className=\"rounded-lg\" data-og-width=\"300\" width=\"300\" data-og-height=\"268\" height=\"268\" data-path=\"oss/images/core_agent_loop.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w\" />\n</div>\n\nThis loop continues until the LLM decides to finish.\n\n### What you can control\n\nTo build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.\n\n| Context Type                                  | What You Control                                                                     | Transient or Persistent |\n| --------------------------------------------- | ------------------------------------------------------------------------------------ | ----------------------- |\n| **[Model Context](#model-context)**           | What goes into model calls (instructions, message history, tools, response format)   | Transient               |\n| **[Tool Context](#tool-context)**             | What tools can access and produce (reads/writes to state, store, runtime context)    | Persistent              |\n| **[Life-cycle Context](#life-cycle-context)** | What happens between model and tool calls (summarization, guardrails, logging, etc.) | Persistent              |\n\n<CardGroup>\n  <Card title=\"Transient context\" icon=\"bolt\" iconType=\"duotone\">\n    What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what's saved in state.\n  </Card>\n\n  <Card title=\"Persistent context\" icon=\"database\" iconType=\"duotone\">\n    What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\n  </Card>\n</CardGroup>\n\n### Data sources\n\nThroughout this process, your agent accesses (reads / writes) different sources of data:\n\n| Data Source         | Also Known As        | Scope               | Examples                                                                   |\n| ------------------- | -------------------- | ------------------- | -------------------------------------------------------------------------- |\n| **Runtime Context** | Static configuration | Conversation-scoped | User ID, API keys, database connections, permissions, environment settings |\n| **State**           | Short-term memory    | Conversation-scoped | Current messages, uploaded files, authentication status, tool results      |\n| **Store**           | Long-term memory     | Cross-conversation  | User preferences, extracted insights, memories, historical data            |\n\n### How it works\n\nLangChain [middleware](/oss/python/langchain/middleware) is the mechanism under the hood that makes context engineering practical for developers using LangChain.\n\nMiddleware allows you to hook into any step in the agent lifecycle and:\n\n* Update context\n* Jump to a different step in the agent lifecycle\n\nThroughout this guide, you'll see frequent use of the middleware API as a means to the context engineering end.\n\n## Model Context\n\nControl what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.\n\n<CardGroup cols={2}>\n  <Card title=\"System Prompt\" icon=\"message-lines\" href=\"#system-prompt\">\n    Base instructions from the developer to the LLM.\n  </Card>\n\n  <Card title=\"Messages\" icon=\"comments\" href=\"#messages\">\n    The full list of messages (conversation history) sent to the LLM.\n  </Card>\n\n  <Card title=\"Tools\" icon=\"wrench\" href=\"#tools\">\n    Utilities the agent has access to to take actions.\n  </Card>\n\n  <Card title=\"Model\" icon=\"brain-circuit\" href=\"#model\">\n    The actual model (including configuration) to be called.\n  </Card>\n\n  <Card title=\"Response Format\" icon=\"brackets-curly\" href=\"#response-format\">\n    Schema specification for the model's final response.\n  </Card>\n</CardGroup>\n\nAll of these types of model context can draw from **state** (short-term memory), **store** (long-term memory), or **runtime context** (static configuration).\n\n### System Prompt\n\nThe system prompt sets the LLM's behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\n\n<Tabs>\n  <Tab title=\"State\">\n    Access message count or conversation context from state:\n\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n    @dynamic_prompt\n    def state_aware_prompt(request: ModelRequest) -> str:\n        # request.messages is a shortcut for request.state[\"messages\"]\n        message_count = len(request.messages)\n\n        base = \"You are a helpful assistant.\"\n\n        if message_count > 10:\n            base += \"\\nThis is a long conversation - be extra concise.\"\n\n        return base\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[state_aware_prompt]\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Access user preferences from long-term memory:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @dynamic_prompt\n    def store_aware_prompt(request: ModelRequest) -> str:\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user preferences\n        store = request.runtime.store\n        user_prefs = store.get((\"preferences\",), user_id)\n\n        base = \"You are a helpful assistant.\"\n\n        if user_prefs:\n            style = user_prefs.value.get(\"communication_style\", \"balanced\")\n            base += f\"\\nUser prefers {style} responses.\"\n\n        return base\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[store_aware_prompt],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Access user ID or configuration from Runtime Context:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n    @dataclass\n    class Context:\n        user_role: str\n        deployment_env: str\n\n    @dynamic_prompt\n    def context_aware_prompt(request: ModelRequest) -> str:\n        # Read from Runtime Context: user role and environment\n        user_role = request.runtime.context.user_role\n        env = request.runtime.context.deployment_env\n\n        base = \"You are a helpful assistant.\"\n\n        if user_role == \"admin\":\n            base += \"\\nYou have admin access. You can perform all operations.\"\n        elif user_role == \"viewer\":\n            base += \"\\nYou have read-only access. Guide users to read operations only.\"\n\n        if env == \"production\":\n            base += \"\\nBe extra careful with any data modifications.\"\n\n        return base\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[context_aware_prompt],\n        context_schema=Context\n    )\n    ```\n  </Tab>\n</Tabs>\n\n### Messages\n\nMessages make up the prompt that is sent to the LLM.\nIt's critical to manage the content of messages to ensure that the LLM has the right information to respond well.\n\n<Tabs>\n  <Tab title=\"State\">\n    Inject uploaded file context from State when relevant to current query:\n\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @wrap_model_call\n    def inject_file_context(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Inject context about files user has uploaded this session.\"\"\"\n        # Read from State: get uploaded files metadata\n        uploaded_files = request.state.get(\"uploaded_files\", [])  # [!code highlight]\n\n        if uploaded_files:\n            # Build context about available files\n            file_descriptions = []\n            for file in uploaded_files:\n                file_descriptions.append(\n                    f\"- {file['name']} ({file['type']}): {file['summary']}\"\n                )\n\n            file_context = f\"\"\"Files you have access to in this conversation:\n    {chr(10).join(file_descriptions)}\n\n    Reference these files when answering questions.\"\"\"\n\n            # Inject file context before recent messages\n            messages = [  # [!code highlight]\n                *request.messages,\n                {\"role\": \"user\", \"content\": file_context},\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[inject_file_context]\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Inject user's email writing style from Store to guide drafting:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @wrap_model_call\n    def inject_writing_style(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Inject user's email writing style from Store.\"\"\"\n        user_id = request.runtime.context.user_id  # [!code highlight]\n\n        # Read from Store: get user's writing style examples\n        store = request.runtime.store  # [!code highlight]\n        writing_style = store.get((\"writing_style\",), user_id)  # [!code highlight]\n\n        if writing_style:\n            style = writing_style.value\n            # Build style guide from stored examples\n            style_context = f\"\"\"Your writing style:\n    - Tone: {style.get('tone', 'professional')}\n    - Typical greeting: \"{style.get('greeting', 'Hi')}\"\n    - Typical sign-off: \"{style.get('sign_off', 'Best')}\"\n    - Example email you've written:\n    {style.get('example_email', '')}\"\"\"\n\n            # Append at end - models pay more attention to final messages\n            messages = [\n                *request.messages,\n                {\"role\": \"user\", \"content\": style_context}\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[inject_writing_style],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Inject compliance rules from Runtime Context based on user's jurisdiction:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_jurisdiction: str\n        industry: str\n        compliance_frameworks: list[str]\n\n    @wrap_model_call\n    def inject_compliance_rules(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Inject compliance constraints from Runtime Context.\"\"\"\n        # Read from Runtime Context: get compliance requirements\n        jurisdiction = request.runtime.context.user_jurisdiction  # [!code highlight]\n        industry = request.runtime.context.industry  # [!code highlight]\n        frameworks = request.runtime.context.compliance_frameworks  # [!code highlight]\n\n        # Build compliance constraints\n        rules = []\n        if \"GDPR\" in frameworks:\n            rules.append(\"- Must obtain explicit consent before processing personal data\")\n            rules.append(\"- Users have right to data deletion\")\n        if \"HIPAA\" in frameworks:\n            rules.append(\"- Cannot share patient health information without authorization\")\n            rules.append(\"- Must use secure, encrypted communication\")\n        if industry == \"finance\":\n            rules.append(\"- Cannot provide financial advice without proper disclaimers\")\n\n        if rules:\n            compliance_context = f\"\"\"Compliance requirements for {jurisdiction}:\n    {chr(10).join(rules)}\"\"\"\n\n            # Append at end - models pay more attention to final messages\n            messages = [\n                *request.messages,\n                {\"role\": \"user\", \"content\": compliance_context}\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[inject_compliance_rules],\n        context_schema=Context\n    )\n    ```\n  </Tab>\n</Tabs>\n\n<Note>\n  **Transient vs Persistent Message Updates:**\n\n  The examples above use `wrap_model_call` to make **transient** updates - modifying what messages are sent to the model for a single call without changing what's saved in state.\n\n  For **persistent** updates that modify state (like the summarization example in [Life-cycle Context](#summarization)), use life-cycle hooks like `before_model` or `after_model` to permanently update the conversation history. See the [middleware documentation](/oss/python/langchain/middleware) for more details.\n</Note>\n\n### Tools\n\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n\n#### Defining tools\n\nEach tool needs a clear name, description, argument names, and argument descriptions. These aren't just metadata—they guide the model's reasoning about when and how to use the tool.\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool(parse_docstring=True)\ndef search_orders(\n    user_id: str,\n    status: str,\n    limit: int = 10\n) -> str:\n    \"\"\"Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.\n\n    Args:\n        user_id: Unique identifier for the user\n        status: Order status: 'pending', 'shipped', or 'delivered'\n        limit: Maximum number of results to return\n    \"\"\"\n    # Implementation here\n    pass\n```\n\n#### Selecting tools\n\nNot every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\n\n<Tabs>\n  <Tab title=\"State\">\n    Enable advanced tools only after certain conversation milestones:\n\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @wrap_model_call\n    def state_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on conversation State.\"\"\"\n        # Read from State: check if user has authenticated\n        state = request.state  # [!code highlight]\n        is_authenticated = state.get(\"authenticated\", False)  # [!code highlight]\n        message_count = len(state[\"messages\"])\n\n        # Only enable sensitive tools after authentication\n        if not is_authenticated:\n            tools = [t for t in request.tools if t.name.startswith(\"public_\")]\n            request = request.override(tools=tools)  # [!code highlight]\n        elif message_count < 5:\n            # Limit tools early in conversation\n            tools = [t for t in request.tools if t.name != \"advanced_search\"]\n            request = request.override(tools=tools)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[public_search, private_search, advanced_search],\n        middleware=[state_based_tools]\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Filter tools based on user preferences or feature flags in Store:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @wrap_model_call\n    def store_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on Store preferences.\"\"\"\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user's enabled features\n        store = request.runtime.store\n        feature_flags = store.get((\"features\",), user_id)\n\n        if feature_flags:\n            enabled_features = feature_flags.value.get(\"enabled_tools\", [])\n            # Only include tools that are enabled for this user\n            tools = [t for t in request.tools if t.name in enabled_features]\n            request = request.override(tools=tools)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[search_tool, analysis_tool, export_tool],\n        middleware=[store_based_tools],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Filter tools based on user permissions from Runtime Context:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_role: str\n\n    @wrap_model_call\n    def context_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on Runtime Context permissions.\"\"\"\n        # Read from Runtime Context: get user role\n        user_role = request.runtime.context.user_role\n\n        if user_role == \"admin\":\n            # Admins get all tools\n            pass\n        elif user_role == \"editor\":\n            # Editors can't delete\n            tools = [t for t in request.tools if t.name != \"delete_data\"]\n            request = request.override(tools=tools)\n        else:\n            # Viewers get read-only tools\n            tools = [t for t in request.tools if t.name.startswith(\"read_\")]\n            request = request.override(tools=tools)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[read_data, write_data, delete_data],\n        middleware=[context_based_tools],\n        context_schema=Context\n    )\n    ```\n  </Tab>\n</Tabs>\n\nSee [Dynamically selecting tools](/oss/python/langchain/middleware#dynamically-selecting-tools) for more examples.\n\n### Model\n\nDifferent models have different strengths, costs, and context windows. Select the right model for the task at hand, which\nmight change during an agent run.\n\n<Tabs>\n  <Tab title=\"State\">\n    Use different models based on conversation length from State:\n\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n    # Initialize models once outside the middleware\n    large_model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n    standard_model = init_chat_model(\"gpt-4o\")\n    efficient_model = init_chat_model(\"gpt-4o-mini\")\n\n    @wrap_model_call\n    def state_based_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select model based on State conversation length.\"\"\"\n        # request.messages is a shortcut for request.state[\"messages\"]\n        message_count = len(request.messages)  # [!code highlight]\n\n        if message_count > 20:\n            # Long conversation - use model with larger context window\n            model = large_model\n        elif message_count > 10:\n            # Medium conversation\n            model = standard_model\n        else:\n            # Short conversation - use efficient model\n            model = efficient_model\n\n        request = request.override(model=model)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o-mini\",\n        tools=[...],\n        middleware=[state_based_model]\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Use user's preferred model from Store:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    # Initialize available models once\n    MODEL_MAP = {\n        \"gpt-4o\": init_chat_model(\"gpt-4o\"),\n        \"gpt-4o-mini\": init_chat_model(\"gpt-4o-mini\"),\n        \"claude-sonnet\": init_chat_model(\"claude-sonnet-4-5-20250929\"),\n    }\n\n    @wrap_model_call\n    def store_based_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select model based on Store preferences.\"\"\"\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user's preferred model\n        store = request.runtime.store\n        user_prefs = store.get((\"preferences\",), user_id)\n\n        if user_prefs:\n            preferred_model = user_prefs.value.get(\"preferred_model\")\n            if preferred_model and preferred_model in MODEL_MAP:\n                request = request.override(model=MODEL_MAP[preferred_model])\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[store_based_model],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Select model based on cost limits or environment from Runtime Context:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        cost_tier: str\n        environment: str\n\n    # Initialize models once outside the middleware\n    premium_model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n    standard_model = init_chat_model(\"gpt-4o\")\n    budget_model = init_chat_model(\"gpt-4o-mini\")\n\n    @wrap_model_call\n    def context_based_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select model based on Runtime Context.\"\"\"\n        # Read from Runtime Context: cost tier and environment\n        cost_tier = request.runtime.context.cost_tier\n        environment = request.runtime.context.environment\n\n        if environment == \"production\" and cost_tier == \"premium\":\n            # Production premium users get best model\n            model = premium_model\n        elif cost_tier == \"budget\":\n            # Budget tier gets efficient model\n            model = budget_model\n        else:\n            # Standard tier\n            model = standard_model\n\n        request = request.override(model=model)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[context_based_model],\n        context_schema=Context\n    )\n    ```\n  </Tab>\n</Tabs>\n\nSee [Dynamic model](/oss/python/langchain/agents#dynamic-model) for more examples.\n\n### Response Format\n\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn't sufficient.\n\n**How it works:** When you provide a schema as the response format, the model's final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\n\n#### Defining formats\n\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.\n\n```python  theme={null}\nfrom pydantic import BaseModel, Field\n\nclass CustomerSupportTicket(BaseModel):\n    \"\"\"Structured ticket information extracted from customer message.\"\"\"\n\n    category: str = Field(\n        description=\"Issue category: 'billing', 'technical', 'account', or 'product'\"\n    )\n    priority: str = Field(\n        description=\"Urgency level: 'low', 'medium', 'high', or 'critical'\"\n    )\n    summary: str = Field(\n        description=\"One-sentence summary of the customer's issue\"\n    )\n    customer_sentiment: str = Field(\n        description=\"Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'\"\n    )\n```\n\n#### Selecting formats\n\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role—returning simple formats early and detailed formats as complexity increases.\n\n<Tabs>\n  <Tab title=\"State\">\n    Configure structured output based on conversation state:\n\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n\n    class SimpleResponse(BaseModel):\n        \"\"\"Simple response for early conversation.\"\"\"\n        answer: str = Field(description=\"A brief answer\")\n\n    class DetailedResponse(BaseModel):\n        \"\"\"Detailed response for established conversation.\"\"\"\n        answer: str = Field(description=\"A detailed answer\")\n        reasoning: str = Field(description=\"Explanation of reasoning\")\n        confidence: float = Field(description=\"Confidence score 0-1\")\n\n    @wrap_model_call\n    def state_based_output(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select output format based on State.\"\"\"\n        # request.messages is a shortcut for request.state[\"messages\"]\n        message_count = len(request.messages)  # [!code highlight]\n\n        if message_count < 3:\n            # Early conversation - use simple format\n            request = request.override(response_format=SimpleResponse)  # [!code highlight]\n        else:\n            # Established conversation - use detailed format\n            request = request.override(response_format=DetailedResponse)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[state_based_output]\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Configure output format based on user preferences in Store:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    class VerboseResponse(BaseModel):\n        \"\"\"Verbose response with details.\"\"\"\n        answer: str = Field(description=\"Detailed answer\")\n        sources: list[str] = Field(description=\"Sources used\")\n\n    class ConciseResponse(BaseModel):\n        \"\"\"Concise response.\"\"\"\n        answer: str = Field(description=\"Brief answer\")\n\n    @wrap_model_call\n    def store_based_output(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select output format based on Store preferences.\"\"\"\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user's preferred response style\n        store = request.runtime.store\n        user_prefs = store.get((\"preferences\",), user_id)\n\n        if user_prefs:\n            style = user_prefs.value.get(\"response_style\", \"concise\")\n            if style == \"verbose\":\n                request = request.override(response_format=VerboseResponse)\n            else:\n                request = request.override(response_format=ConciseResponse)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[store_based_output],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Configure output format based on Runtime Context like user role or environment:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_role: str\n        environment: str\n\n    class AdminResponse(BaseModel):\n        \"\"\"Response with technical details for admins.\"\"\"\n        answer: str = Field(description=\"Answer\")\n        debug_info: dict = Field(description=\"Debug information\")\n        system_status: str = Field(description=\"System status\")\n\n    class UserResponse(BaseModel):\n        \"\"\"Simple response for regular users.\"\"\"\n        answer: str = Field(description=\"Answer\")\n\n    @wrap_model_call\n    def context_based_output(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select output format based on Runtime Context.\"\"\"\n        # Read from Runtime Context: user role and environment\n        user_role = request.runtime.context.user_role\n        environment = request.runtime.context.environment\n\n        if user_role == \"admin\" and environment == \"production\":\n            # Admins in production get detailed output\n            request = request.override(response_format=AdminResponse)\n        else:\n            # Regular users get simple output\n            request = request.override(response_format=UserResponse)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[...],\n        middleware=[context_based_output],\n        context_schema=Context\n    )\n    ```\n  </Tab>\n</Tabs>\n\n## Tool Context\n\nTools are special in that they both read and write context.\n\nIn the most basic case, when a tool executes, it receives the LLM's request parameters and returns a tool message back. The tool does its work and produces a result.\n\nTools can also fetch important information for the model that allows it to perform and complete tasks.\n\n### Reads\n\nMost real-world tools need more than just the LLM's parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\n\n<Tabs>\n  <Tab title=\"State\">\n    Read from State to check current session information:\n\n    ```python  theme={null}\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n\n    @tool\n    def check_authentication(\n        runtime: ToolRuntime\n    ) -> str:\n        \"\"\"Check if user is authenticated.\"\"\"\n        # Read from State: check current auth status\n        current_state = runtime.state\n        is_authenticated = current_state.get(\"authenticated\", False)\n\n        if is_authenticated:\n            return \"User is authenticated\"\n        else:\n            return \"User is not authenticated\"\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[check_authentication]\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Read from Store to access persisted user preferences:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @tool\n    def get_preference(\n        preference_key: str,\n        runtime: ToolRuntime[Context]\n    ) -> str:\n        \"\"\"Get user preference from Store.\"\"\"\n        user_id = runtime.context.user_id\n\n        # Read from Store: get existing preferences\n        store = runtime.store\n        existing_prefs = store.get((\"preferences\",), user_id)\n\n        if existing_prefs:\n            value = existing_prefs.value.get(preference_key)\n            return f\"{preference_key}: {value}\" if value else f\"No preference set for {preference_key}\"\n        else:\n            return \"No preferences found\"\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[get_preference],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Read from Runtime Context for configuration like API keys and user IDs:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n\n    @dataclass\n    class Context:\n        user_id: str\n        api_key: str\n        db_connection: str\n\n    @tool\n    def fetch_user_data(\n        query: str,\n        runtime: ToolRuntime[Context]\n    ) -> str:\n        \"\"\"Fetch data using Runtime Context configuration.\"\"\"\n        # Read from Runtime Context: get API key and DB connection\n        user_id = runtime.context.user_id\n        api_key = runtime.context.api_key\n        db_connection = runtime.context.db_connection\n\n        # Use configuration to fetch data\n        results = perform_database_query(db_connection, query, api_key)\n\n        return f\"Found {len(results)} results for user {user_id}\"\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[fetch_user_data],\n        context_schema=Context\n    )\n\n    # Invoke with runtime context\n    result = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Get my data\"}]},\n        context=Context(\n            user_id=\"user_123\",\n            api_key=\"sk-...\",\n            db_connection=\"postgresql://...\"\n        )\n    )\n    ```\n  </Tab>\n</Tabs>\n\n### Writes\n\nTool results can be used to help an agent complete a given task. Tools can both return results directly to the model\nand update the memory of the agent to make important context available to future steps.\n\n<Tabs>\n  <Tab title=\"State\">\n    Write to State to track session-specific information using Command:\n\n    ```python  theme={null}\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.types import Command\n\n    @tool\n    def authenticate_user(\n        password: str,\n        runtime: ToolRuntime\n    ) -> Command:\n        \"\"\"Authenticate user and update State.\"\"\"\n        # Perform authentication (simplified)\n        if password == \"correct\":\n            # Write to State: mark as authenticated using Command\n            return Command(\n                update={\"authenticated\": True},\n            )\n        else:\n            return Command(update={\"authenticated\": False})\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[authenticate_user]\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Write to Store to persist data across sessions:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @tool\n    def save_preference(\n        preference_key: str,\n        preference_value: str,\n        runtime: ToolRuntime[Context]\n    ) -> str:\n        \"\"\"Save user preference to Store.\"\"\"\n        user_id = runtime.context.user_id\n\n        # Read existing preferences\n        store = runtime.store\n        existing_prefs = store.get((\"preferences\",), user_id)\n\n        # Merge with new preference\n        prefs = existing_prefs.value if existing_prefs else {}\n        prefs[preference_key] = preference_value\n\n        # Write to Store: save updated preferences\n        store.put((\"preferences\",), user_id, prefs)\n\n        return f\"Saved preference: {preference_key} = {preference_value}\"\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[save_preference],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n</Tabs>\n\nSee [Tools](/oss/python/langchain/tools) for comprehensive examples of accessing state, store, and runtime context in tools.\n\n## Life-cycle Context\n\nControl what happens **between** the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\n\nAs you've seen in [Model Context](#model-context) and [Tool Context](#tool-context), [middleware](/oss/python/langchain/middleware) is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\n\n1. **Update context** - Modify state and store to persist changes, update conversation history, or save insights\n2. **Jump in the lifecycle** - Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware hooks in the agent loop\" className=\"rounded-lg\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n</div>\n\n### Example: Summarization\n\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in [Model Context](#messages), summarization **persistently updates state** - permanently replacing old messages with a summary that's saved for all future turns.\n\nLangChain offers built-in middleware for this:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",\n            trigger={\"tokens\": 4000},\n            keep={\"messages\": 20},\n        ),\n    ],\n)\n```\n\nWhen the conversation exceeds the token limit, `SummarizationMiddleware` automatically:\n\n1. Summarizes older messages using a separate LLM call\n2. Replaces them with a summary message in State (permanently)\n3. Keeps recent messages intact for context\n\nThe summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\n\n<Note>\n  For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the [Middleware documentation](/oss/python/langchain/middleware).\n</Note>\n\n## Best practices\n\n1. **Start simple** - Begin with static prompts and tools, add dynamics only when needed\n2. **Test incrementally** - Add one context engineering feature at a time\n3. **Monitor performance** - Track model calls, token usage, and latency\n4. **Use built-in middleware** - Leverage [`SummarizationMiddleware`](/oss/python/langchain/middleware#summarization), [`LLMToolSelectorMiddleware`](/oss/python/langchain/middleware#llm-tool-selector), etc.\n5. **Document your context strategy** - Make it clear what context is being passed and why\n6. **Understand transient vs persistent**: Model context changes are transient (per-call), while life-cycle context changes persist to state\n\n## Related resources\n\n* [Context conceptual overview](/oss/python/concepts/context) - Understand context types and when to use them\n* [Middleware](/oss/python/langchain/middleware) - Complete middleware guide\n* [Tools](/oss/python/langchain/tools) - Tool creation and context access\n* [Memory](/oss/python/concepts/memory) - Short-term and long-term memory patterns\n* [Agents](/oss/python/langchain/agents) - Core agent concepts\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/context-engineering.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 45182,
    "word_count": 4323
  },
  {
    "title": "LangSmith Deployment",
    "source": "https://docs.langchain.com/oss/python/langchain/deploy",
    "content": "When you're ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is **purpose-built for stateful, long-running agents** that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n* A [GitHub account](https://github.com/)\n* A [LangSmith account](https://smith.langchain.com/) (free to sign up)\n\n## Deploy your agent\n\n### 1. Create a repository on GitHub\n\nYour application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/python/langchain/studio#setup-local-agent-server). Then, push your code to the repository.\n\n### 2. Deploy to LangSmith\n\n<Steps>\n  <Step title=\"Navigate to LangSmith Deployments\">\n    Log in to [LangSmith](https://smith.langchain.com/). In the left sidebar, select **Deployments**.\n  </Step>\n\n  <Step title=\"Create new deployment\">\n    Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.\n  </Step>\n\n  <Step title=\"Link repository\">\n    If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.\n  </Step>\n\n  <Step title=\"Deploy repository\">\n    Select your application's repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.\n  </Step>\n</Steps>\n\n### 3. Test your application in Studio\n\nOnce your application is deployed:\n\n1. Select the deployment you just created to view more details.\n2. Click the **Studio** button in the top right corner. Studio will open to display your graph.\n\n### 4. Get the API URL for your deployment\n\n1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.\n2. Click the `URL` to copy it to the clipboard.\n\n### 5. Test the API\n\nYou can now test the API:\n\n<Tabs>\n  <Tab title=\"Python\">\n    1. Install LangGraph Python:\n\n    ```shell  theme={null}\n    pip install langgraph-sdk\n    ```\n\n    2. Send a message to the agent:\n\n    ```python  theme={null}\n    from langgraph_sdk import get_sync_client # or get_client for async\n\n    client = get_sync_client(url=\"your-deployment-url\", api_key=\"your-langsmith-api-key\")\n\n    for chunk in client.runs.stream(\n        None,    # Threadless run\n        \"agent\", # Name of agent. Defined in langgraph.json.\n        input={\n            \"messages\": [{\n                \"role\": \"human\",\n                \"content\": \"What is LangGraph?\",\n            }],\n        },\n        stream_mode=\"updates\",\n    ):\n        print(f\"Receiving new event of type: {chunk.event}...\")\n        print(chunk.data)\n        print(\"\\n\\n\")\n    ```\n  </Tab>\n\n  <Tab title=\"Rest API\">\n    ```bash  theme={null}\n    curl -s --request POST \\\n        --url <DEPLOYMENT_URL>/runs/stream \\\n        --header 'Content-Type: application/json' \\\n        --header \"X-Api-Key: <LANGSMITH API KEY> \\\n        --data \"{\n            \\\"assistant_id\\\": \\\"agent\\\", `# Name of agent. Defined in langgraph.json.`\n            \\\"input\\\": {\n                \\\"messages\\\": [\n                    {\n                        \\\"role\\\": \\\"human\\\",\n                        \\\"content\\\": \\\"What is LangGraph?\\\"\n                    }\n                ]\n            },\n            \\\"stream_mode\\\": \\\"updates\\\"\n        }\"\n    ```\n  </Tab>\n</Tabs>\n\n<Tip>\n  LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the [Platform setup overview](/langsmith/platform-setup).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/deploy.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 4376,
    "word_count": 519
  },
  {
    "title": "Get help",
    "source": "https://docs.langchain.com/oss/python/langchain/get-help",
    "content": "Connect with the LangChain community, access learning resources, and get the support you need to build with confidence.\n\n## Learning resources\n\nStart your journey or deepen your knowledge with our comprehensive learning materials.\n\n* **[Chat LangChain](https://chat.langchain.com/)**: Ask the docs anything about LangChain, powered by real-time docs\n* **[API Reference](https://reference.langchain.com/python/)**: Complete documentation for all LangChain packages\n\n## Community support\n\nGet help from fellow developers and the LangChain team through our active community channels.\n\n* **[Community Forum](https://forum.langchain.com/)**: Ask questions, share solutions, and discuss best practices\n* **[Community Slack](https://www.langchain.com/join-community)**: Connect with other builders and get quick help\n\n## Professional support\n\nFor enterprise needs and critical applications, access dedicated support channels.\n\n* **[Support portal](https://support.langchain.com/)**: Submit tickets and track support requests\n* **[LangSmith status](https://status.smith.langchain.com/)**: Real-time status of LangSmith services and APIs\n\n## Contribute\n\nHelp us improve LangChain for everyone. Whether you're fixing bugs, adding features, or improving documentation, we welcome your contributions.\n\n* **[Contributing Guide](/oss/python/contributing/overview)**: Everything you need to know about contributing to LangChain\n\n## Stay connected\n\nFollow us for the latest updates, announcements, and community highlights.\n\n* **[X (Twitter)](https://twitter.com/langchainai)**: Daily updates and community spotlights\n* **[LinkedIn](https://www.linkedin.com/company/langchain/)**: Professional network and company updates\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/get-help.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 2060,
    "word_count": 218
  },
  {
    "title": "Guardrails",
    "source": "https://docs.langchain.com/oss/python/langchain/guardrails",
    "content": "Implement safety checks and content filtering for your agents\n\nGuardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent's execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\n\nCommon use cases include:\n\n* Preventing PII leakage\n* Detecting and blocking prompt injection attacks\n* Blocking inappropriate or harmful content\n* Enforcing business rules and compliance requirements\n* Validating output quality and accuracy\n\nYou can implement guardrails using [middleware](/oss/python/langchain/middleware) to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware flow diagram\" className=\"rounded-lg\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n</div>\n\nGuardrails can be implemented using two complementary approaches:\n\n<CardGroup cols={2}>\n  <Card title=\"Deterministic guardrails\" icon=\"list-check\">\n    Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.\n  </Card>\n\n  <Card title=\"Model-based guardrails\" icon=\"brain\">\n    Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\n  </Card>\n</CardGroup>\n\nLangChain provides both built-in guardrails (e.g., [PII detection](#pii-detection), [human-in-the-loop](#human-in-the-loop)) and a flexible middleware system for building custom guardrails using either approach.\n\n## Built-in guardrails\n\n### PII detection\n\nLangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.\n\nPII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.\n\nThe PII middleware supports multiple strategies for handling detected PII:\n\n| Strategy | Description                             | Example               |\n| -------- | --------------------------------------- | --------------------- |\n| `redact` | Replace with `[REDACTED_TYPE]`          | `[REDACTED_EMAIL]`    |\n| `mask`   | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |\n| `hash`   | Replace with deterministic hash         | `a8f5f167...`         |\n| `block`  | Raise exception when detected           | Error thrown          |\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[customer_service_tool, email_tool],\n    middleware=[\n        # Redact emails in user input before sending to model\n        PIIMiddleware(\n            \"email\",\n            strategy=\"redact\",\n            apply_to_input=True,\n        ),\n        # Mask credit cards in user input\n        PIIMiddleware(\n            \"credit_card\",\n            strategy=\"mask\",\n            apply_to_input=True,\n        ),\n        # Block API keys - raise error if detected\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n            apply_to_input=True,\n        ),\n    ],\n)\n\n# When user provides PII, it will be handled according to the strategy\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"My email is john.doe@example.com and card is 5105-1051-0510-5100\"}]\n})\n```\n\n<Accordion title=\"Built-in PII types and configuration\">\n  **Built-in PII types:**\n\n  * `email` - Email addresses\n  * `credit_card` - Credit card numbers (Luhn validated)\n  * `ip` - IP addresses\n  * `mac_address` - MAC addresses\n  * `url` - URLs\n\n  **Configuration options:**\n\n  | Parameter               | Description                                                            | Default                |\n  | ----------------------- | ---------------------------------------------------------------------- | ---------------------- |\n  | `pii_type`              | Type of PII to detect (built-in or custom)                             | Required               |\n  | `strategy`              | How to handle detected PII (`\"block\"`, `\"redact\"`, `\"mask\"`, `\"hash\"`) | `\"redact\"`             |\n  | `detector`              | Custom detector function or regex pattern                              | `None` (uses built-in) |\n  | `apply_to_input`        | Check user messages before model call                                  | `True`                 |\n  | `apply_to_output`       | Check AI messages after model call                                     | `False`                |\n  | `apply_to_tool_results` | Check tool result messages after execution                             | `False`                |\n</Accordion>\n\nSee the [middleware documentation](/oss/python/langchain/middleware#pii-detection) for complete details on PII detection capabilities.\n\n### Human-in-the-loop\n\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.\n\nHuman-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, send_email_tool, delete_database_tool],\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                # Require approval for sensitive operations\n                \"send_email\": True,\n                \"delete_database\": True,\n                # Auto-approve safe operations\n                \"search\": False,\n            }\n        ),\n    ],\n    # Persist the state across interrupts\n    checkpointer=InMemorySaver(),\n)\n\n# Human-in-the-loop requires a thread ID for persistence\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Agent will pause and wait for approval before executing sensitive tools\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n    config=config\n)\n\nresult = agent.invoke(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config  # Same thread ID to resume the paused conversation\n)\n```\n\n<Tip>\n  See the [human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop) for complete details on implementing approval workflows.\n</Tip>\n\n## Custom guardrails\n\nFor more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\n\n### Before agent guardrails\n\nUse \"before agent\" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.\n\n<CodeGroup>\n  ```python title=\"Class syntax\" theme={null}\n  from typing import Any\n\n  from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\n  from langgraph.runtime import Runtime\n\n  class ContentFilterMiddleware(AgentMiddleware):\n      \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n\n      def __init__(self, banned_keywords: list[str]):\n          super().__init__()\n          self.banned_keywords = [kw.lower() for kw in banned_keywords]\n\n      @hook_config(can_jump_to=[\"end\"])\n      def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n          # Get the first user message\n          if not state[\"messages\"]:\n              return None\n\n          first_message = state[\"messages\"][0]\n          if first_message.type != \"human\":\n              return None\n\n          content = first_message.content.lower()\n\n          # Check for banned keywords\n          for keyword in self.banned_keywords:\n              if keyword in content:\n                  # Block execution before any processing\n                  return {\n                      \"messages\": [{\n                          \"role\": \"assistant\",\n                          \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                      }],\n                      \"jump_to\": \"end\"\n                  }\n\n          return None\n\n  # Use the custom guardrail\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, calculator_tool],\n      middleware=[\n          ContentFilterMiddleware(\n              banned_keywords=[\"hack\", \"exploit\", \"malware\"]\n          ),\n      ],\n  )\n\n  # This request will be blocked before any processing\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n  })\n  ```\n\n  ```python title=\"Decorator syntax\" theme={null}\n  from typing import Any\n\n  from langchain.agents.middleware import before_agent, AgentState, hook_config\n  from langgraph.runtime import Runtime\n\n  banned_keywords = [\"hack\", \"exploit\", \"malware\"]\n\n  @before_agent(can_jump_to=[\"end\"])\n  def content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n      \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n      # Get the first user message\n      if not state[\"messages\"]:\n          return None\n\n      first_message = state[\"messages\"][0]\n      if first_message.type != \"human\":\n          return None\n\n      content = first_message.content.lower()\n\n      # Check for banned keywords\n      for keyword in banned_keywords:\n          if keyword in content:\n              # Block execution before any processing\n              return {\n                  \"messages\": [{\n                      \"role\": \"assistant\",\n                      \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                  }],\n                  \"jump_to\": \"end\"\n              }\n\n      return None\n\n  # Use the custom guardrail\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, calculator_tool],\n      middleware=[content_filter],\n  )\n\n  # This request will be blocked before any processing\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n  })\n  ```\n</CodeGroup>\n\n### After agent guardrails\n\nUse \"after agent\" hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.\n\n<CodeGroup>\n  ```python title=\"Class syntax\" theme={null}\n  from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\n  from langgraph.runtime import Runtime\n  from langchain.messages import AIMessage\n  from langchain.chat_models import init_chat_model\n  from typing import Any\n\n  class SafetyGuardrailMiddleware(AgentMiddleware):\n      \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n\n      def __init__(self):\n          super().__init__()\n          self.safety_model = init_chat_model(\"gpt-4o-mini\")\n\n      @hook_config(can_jump_to=[\"end\"])\n      def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n          # Get the final AI response\n          if not state[\"messages\"]:\n              return None\n\n          last_message = state[\"messages\"][-1]\n          if not isinstance(last_message, AIMessage):\n              return None\n\n          # Use a model to evaluate safety\n          safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n          Respond with only 'SAFE' or 'UNSAFE'.\n\n          Response: {last_message.content}\"\"\"\n\n          result = self.safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n\n          if \"UNSAFE\" in result.content:\n              last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n          return None\n\n  # Use the safety guardrail\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, calculator_tool],\n      middleware=[SafetyGuardrailMiddleware()],\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n  })\n  ```\n\n  ```python title=\"Decorator syntax\" theme={null}\n  from langchain.agents.middleware import after_agent, AgentState, hook_config\n  from langgraph.runtime import Runtime\n  from langchain.messages import AIMessage\n  from langchain.chat_models import init_chat_model\n  from typing import Any\n\n  safety_model = init_chat_model(\"gpt-4o-mini\")\n\n  @after_agent(can_jump_to=[\"end\"])\n  def safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n      \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n      # Get the final AI response\n      if not state[\"messages\"]:\n          return None\n\n      last_message = state[\"messages\"][-1]\n      if not isinstance(last_message, AIMessage):\n          return None\n\n      # Use a model to evaluate safety\n      safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n      Respond with only 'SAFE' or 'UNSAFE'.\n\n      Response: {last_message.content}\"\"\"\n\n      result = safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n\n      if \"UNSAFE\" in result.content:\n          last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n      return None\n\n  # Use the safety guardrail\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, calculator_tool],\n      middleware=[safety_guardrail],\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n  })\n  ```\n</CodeGroup>\n\n### Combine multiple guardrails\n\nYou can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, send_email_tool],\n    middleware=[\n        # Layer 1: Deterministic input filter (before agent)\n        ContentFilterMiddleware(banned_keywords=[\"hack\", \"exploit\"]),\n\n        # Layer 2: PII protection (before and after model)\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n\n        # Layer 3: Human approval for sensitive tools\n        HumanInTheLoopMiddleware(interrupt_on={\"send_email\": True}),\n\n        # Layer 4: Model-based safety check (after agent)\n        SafetyGuardrailMiddleware(),\n    ],\n)\n```\n\n## Additional resources\n\n* [Middleware documentation](/oss/python/langchain/middleware) - Complete guide to custom middleware\n* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware\n* [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) - Add human review for sensitive operations\n* [Testing agents](/oss/python/langchain/test) - Strategies for testing safety mechanisms\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/guardrails.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 17604,
    "word_count": 1660
  },
  {
    "title": "Human-in-the-loop",
    "source": "https://docs.langchain.com/oss/python/langchain/human-in-the-loop",
    "content": "The Human-in-the-Loop (HITL) [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) lets you add human oversight to agent tool calls.\nWhen a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.\n\nIt does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) that halts execution. The graph state is saved using LangGraph's [persistence layer](/oss/python/langgraph/persistence), so execution can pause safely and resume later.\n\nA human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).\n\n## Interrupt decision types\n\nThe [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) defines three built-in ways a human can respond to an interrupt:\n\n| Decision Type | Description                                                               | Example Use Case                                    |\n| ------------- | ------------------------------------------------------------------------- | --------------------------------------------------- |\n| ✅ `approve`   | The action is approved as-is and executed without changes.                | Send an email draft exactly as written              |\n| ✏️ `edit`     | The tool call is executed with modifications.                             | Change the recipient before sending an email        |\n| ❌ `reject`    | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |\n\nThe available decision types for each tool depend on the policy you configure in `interrupt_on`.\nWhen multiple tool calls are paused at the same time, each action requires a separate decision.\nDecisions must be provided in the same order as the actions appear in the interrupt request.\n\n<Tip>\n  When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n</Tip>\n\n## Configuring interrupts\n\nTo use HITL, add the [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) to the agent's `middleware` list when creating the agent.\n\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n    middleware=[\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\n                \"write_file\": True,  # All decisions (approve, edit, reject) allowed\n                \"execute_sql\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing allowed\n                # Safe operation, no approval needed\n                \"read_data\": False,\n            },\n            # Prefix for interrupt messages - combined with tool name and args to form the full message\n            # e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n            # Individual tools can override this by specifying a \"description\" in their interrupt config\n            description_prefix=\"Tool execution pending approval\",\n        ),\n    ],\n    # Human-in-the-loop requires checkpointing to handle interrupts.\n    # In production, use a persistent checkpointer like AsyncPostgresSaver.\n    checkpointer=InMemorySaver(),  # [!code highlight]\n)\n```\n\n<Info>\n  You must configure a checkpointer to persist the graph state across interrupts.\n  In production, use a persistent checkpointer like [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver). For testing or prototyping, use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver).\n\n  When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.\n  See the [LangGraph interrupts documentation](/oss/python/langgraph/interrupts) for details.\n</Info>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"interrupt_on\" type=\"dict\" required>\n    Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.\n  </ParamField>\n\n  <ParamField body=\"description_prefix\" type=\"string\" default=\"Tool execution requires approval\">\n    Prefix for action request descriptions\n  </ParamField>\n\n  **`InterruptOnConfig` options:**\n\n  <ParamField body=\"allowed_decisions\" type=\"list[string]\">\n    List of allowed decisions: `'approve'`, `'edit'`, or `'reject'`\n  </ParamField>\n\n  <ParamField body=\"description\" type=\"string | callable\">\n    Static string or callable function for custom description\n  </ParamField>\n</Accordion>\n\n## Responding to interrupts\n\nWhen you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\n\n```python  theme={null}\nfrom langgraph.types import Command\n\n# Human-in-the-loop leverages LangGraph's persistence layer.\n# You must provide a thread ID to associate the execution with a conversation thread,\n# so the conversation can be paused and resumed (as is needed for human review).\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}} # [!code highlight]\n# Run the graph until the interrupt is hit.\nresult = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Delete old records from the database\",\n            }\n        ]\n    },\n    config=config # [!code highlight]\n)\n\n# The interrupt contains the full HITL request with action_requests and review_configs\nprint(result['__interrupt__'])\n# > [\n# >    Interrupt(\n# >       value={\n# >          'action_requests': [\n# >             {\n# >                'name': 'execute_sql',\n# >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';'},\n# >                'description': 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n# >             }\n# >          ],\n# >          'review_configs': [\n# >             {\n# >                'action_name': 'execute_sql',\n# >                'allowed_decisions': ['approve', 'reject']\n# >             }\n# >          ]\n# >       }\n# >    )\n# > ]\n\n\n# Resume with approval decision\nagent.invoke(\n    Command( # [!code highlight]\n        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"edit\", \"reject\" [!code highlight]\n    ), # [!code highlight]\n    config=config # Same thread ID to resume the paused conversation\n)\n```\n\n### Decision types\n\n<Tabs>\n  <Tab title=\"✅ approve\">\n    Use `approve` to approve the tool call as-is and execute it without changes.\n\n    ```python  theme={null}\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"approve\",\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"✏️ edit\">\n    Use `edit` to modify the tool call before execution.\n    Provide the edited action with the new tool name and arguments.\n\n    ```python  theme={null}\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"edit\",\n                        # Edited action with tool name and args\n                        \"edited_action\": {\n                            # Tool name to call.\n                            # Will usually be the same as the original action.\n                            \"name\": \"new_tool_name\",\n                            # Arguments to pass to the tool.\n                            \"args\": {\"key1\": \"new_value\", \"key2\": \"original_value\"},\n                        }\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n\n    <Tip>\n      When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n    </Tip>\n  </Tab>\n\n  <Tab title=\"❌ reject\">\n    Use `reject` to reject the tool call and provide feedback instead of execution.\n\n    ```python  theme={null}\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"reject\",\n                        # An explanation about why the action was rejected\n                        \"message\": \"No, this is wrong because ..., instead do this ...\",\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n\n    The `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.\n\n    ***\n\n    ### Multiple decisions\n\n    When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:\n\n    ```python  theme={null}\n    {\n        \"decisions\": [\n            {\"type\": \"approve\"},\n            {\n                \"type\": \"edit\",\n                \"edited_action\": {\n                    \"name\": \"tool_name\",\n                    \"args\": {\"param\": \"new_value\"}\n                }\n            },\n            {\n                \"type\": \"reject\",\n                \"message\": \"This action is not allowed\"\n            }\n        ]\n    }\n    ```\n  </Tab>\n</Tabs>\n\n## Execution lifecycle\n\nThe middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:\n\n1. The agent invokes the model to generate a response.\n2. The middleware inspects the response for tool calls.\n3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).\n4. The agent waits for human decisions.\n5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes [ToolMessage](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage)'s for rejected calls, and resumes execution.\n\n## Custom HITL logic\n\nFor more specialized workflows, you can build custom HITL logic directly using the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) primitive and [middleware](/oss/python/langchain/middleware) abstraction.\n\nReview the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/human-in-the-loop.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 12756,
    "word_count": 1442
  },
  {
    "title": "Install LangChain",
    "source": "https://docs.langchain.com/oss/python/langchain/install",
    "content": "To install the LangChain package:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langchain\n  # Requires Python 3.10+\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain\n  # Requires Python 3.10+\n  ```\n</CodeGroup>\n\nLangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  # Installing the OpenAI integration\n  pip install -U langchain-openai\n\n  # Installing the Anthropic integration\n  pip install -U langchain-anthropic\n  ```\n\n  ```bash uv theme={null}\n  # Installing the OpenAI integration\n  uv add langchain-openai\n\n  # Installing the Anthropic integration\n  uv add langchain-anthropic\n  ```\n</CodeGroup>\n\n<Tip>\n  See the [Integrations tab](/oss/python/integrations/providers/overview) for a full list of available integrations.\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/install.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 1234,
    "word_count": 138
  },
  {
    "title": "Build a semantic search engine with LangChain",
    "source": "https://docs.langchain.com/oss/python/langchain/knowledge-base",
    "content": "## Overview\n\nThis tutorial will familiarize you with LangChain's [document loader](/oss/python/langchain/retrieval#document-loaders), [embedding](/oss/python/langchain/retrieval#embedding-models), and [vector store](/oss/python/langchain/retrieval#vector-store) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources -- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/oss/python/langchain/retrieval).\n\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\n\n### Concepts\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\n* [Documents and document loaders](/oss/python/integrations/document_loaders);\n* [Text splitters](/oss/python/integrations/splitters);\n* [Embeddings](/oss/python/integrations/text_embedding);\n* [Vector stores](/oss/python/integrations/vectorstores) and [retrievers](/oss/python/integrations/retrievers).\n\n## Setup\n\n### Installation\n\nThis tutorial requires the `langchain-community` and `pypdf` packages:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-community pypdf\n  ```\n\n  ```bash conda theme={null}\n  conda install langchain-community pypdf -c conda-forge\n  ```\n</CodeGroup>\n\nFor more details, see our [Installation guide](/oss/python/langchain/install).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with [LangSmith](https://smith.langchain.com).\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```shell  theme={null}\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\nOr, if in a notebook, you can set them with:\n\n```python  theme={null}\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\n## 1. Documents and Document Loaders\n\nLangChain implements a [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n\n* `page_content`: a string representing the content;\n* `metadata`: a dict containing arbitrary metadata;\n* `id`: (optional) a string identifier for the document.\n\nThe `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object often represents a chunk of a larger document.\n\nWe can generate sample documents when desired:\n\n```python  theme={null}\nfrom langchain_core.documents import Document\n\ndocuments = [\n    Document(\n        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n    Document(\n        page_content=\"Cats are independent pets that often enjoy their own space.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n]\n```\n\nHowever, the LangChain ecosystem implements [document loaders](/oss/python/langchain/retrieval#document-loaders) that [integrate with hundreds of common sources](/oss/python/integrations/document_loaders/). This makes it easy to incorporate data from these sources into your AI application.\n\n### Loading documents\n\nLet's load a PDF into a sequence of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects. [Here is a sample PDF](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/example_data/nke-10k-2023.pdf) -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for [available PDF document loaders](/oss/python/integrations/document_loaders/#pdfs).\n\n```python  theme={null}\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfile_path = \"../example_data/nke-10k-2023.pdf\"\nloader = PyPDFLoader(file_path)\n\ndocs = loader.load()\n\nprint(len(docs))\n```\n\n```output  theme={null}\n107\n```\n\n`PyPDFLoader` loads one [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object per PDF page. For each, we can easily access:\n\n* The string content of the page;\n* Metadata containing the file name and page number.\n\n```python  theme={null}\nprint(f\"{docs[0].page_content[:200]}\\n\")\nprint(docs[0].metadata)\n```\n\n```output  theme={null}\nTable of Contents\nUNITED STATES\nSECURITIES AND EXCHANGE COMMISSION\nWashington, D.C. 20549\nFORM 10-K\n(Mark One)\n☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\nFO\n\n{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}\n```\n\n### Splitting\n\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n\nWe can use [text splitters](/oss/python/langchain/retrieval#text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the\n`RecursiveCharacterTextSplitter`,\nwhich will recursively split the document using common separators like\nnew lines until each chunk is the appropriate size. This is the\nrecommended text splitter for generic text use cases.\n\nWe set `add_start_index=True` so that the character index where each\nsplit Document starts within the initial Document is preserved as\nmetadata attribute “start\\_index”.\n\n```python  theme={null}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(len(all_splits))\n```\n\n```output  theme={null}\n514\n```\n\n## 2. Embeddings\n\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/oss/python/langchain/retrieval#embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n\nLangChain supports embeddings from [dozens of providers](/oss/python/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"OPENAI_API_KEY\"):\n        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\n    from langchain_openai import OpenAIEmbeddings\n\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n    ```\n  </Tab>\n\n  <Tab title=\"Azure\">\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n        os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n\n    from langchain_openai import AzureOpenAIEmbeddings\n\n    embeddings = AzureOpenAIEmbeddings(\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n        openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    ```shell  theme={null}\n    pip install -qU langchain-google-genai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"GOOGLE_API_KEY\"):\n        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\n    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n    ```\n  </Tab>\n\n  <Tab title=\"Google Vertex\">\n    ```shell  theme={null}\n    pip install -qU langchain-google-vertexai\n    ```\n\n    ```python  theme={null}\n    from langchain_google_vertexai import VertexAIEmbeddings\n\n    embeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n    ```\n  </Tab>\n\n  <Tab title=\"AWS\">\n    ```shell  theme={null}\n    pip install -qU langchain-aws\n    ```\n\n    ```python  theme={null}\n    from langchain_aws import BedrockEmbeddings\n\n    embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n    ```\n  </Tab>\n\n  <Tab title=\"HuggingFace\">\n    ```shell  theme={null}\n    pip install -qU langchain-huggingface\n    ```\n\n    ```python  theme={null}\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    ```\n  </Tab>\n\n  <Tab title=\"Ollama\">\n    ```shell  theme={null}\n    pip install -qU langchain-ollama\n    ```\n\n    ```python  theme={null}\n    from langchain_ollama import OllamaEmbeddings\n\n    embeddings = OllamaEmbeddings(model=\"llama3\")\n    ```\n  </Tab>\n\n  <Tab title=\"Cohere\">\n    ```shell  theme={null}\n    pip install -qU langchain-cohere\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"COHERE_API_KEY\"):\n        os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n\n    from langchain_cohere import CohereEmbeddings\n\n    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n    ```\n  </Tab>\n\n  <Tab title=\"MistralAI\">\n    ```shell  theme={null}\n    pip install -qU langchain-mistralai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"MISTRALAI_API_KEY\"):\n        os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\n\n    from langchain_mistralai import MistralAIEmbeddings\n\n    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n    ```\n  </Tab>\n\n  <Tab title=\"Nomic\">\n    ```shell  theme={null}\n    pip install -qU langchain-nomic\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NOMIC_API_KEY\"):\n        os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\n\n    from langchain_nomic import NomicEmbeddings\n\n    embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n    ```\n  </Tab>\n\n  <Tab title=\"NVIDIA\">\n    ```shell  theme={null}\n    pip install -qU langchain-nvidia-ai-endpoints\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NVIDIA_API_KEY\"):\n        os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n\n    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\n    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n    ```\n  </Tab>\n\n  <Tab title=\"Voyage AI\">\n    ```shell  theme={null}\n    pip install -qU langchain-voyageai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"VOYAGE_API_KEY\"):\n        os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n\n    from langchain-voyageai import VoyageAIEmbeddings\n\n    embeddings = VoyageAIEmbeddings(model=\"voyage-3\")\n    ```\n  </Tab>\n\n  <Tab title=\"IBM watsonx\">\n    ```shell  theme={null}\n    pip install -qU langchain-ibm\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"WATSONX_APIKEY\"):\n        os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\n\n    from langchain_ibm import WatsonxEmbeddings\n\n    embeddings = WatsonxEmbeddings(\n        model_id=\"ibm/slate-125m-english-rtrvr\",\n        url=\"https://us-south.ml.cloud.ibm.com\",\n        project_id=\"<WATSONX PROJECT_ID>\",\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Fake\">\n    ```shell  theme={null}\n    pip install -qU langchain-core\n    ```\n\n    ```python  theme={null}\n    from langchain_core.embeddings import DeterministicFakeEmbedding\n\n    embeddings = DeterministicFakeEmbedding(size=4096)\n    ```\n  </Tab>\n\n  <Tab title=\"Isaacus\">\n    ```shell  theme={null}\n    pip install -qU langchain-isaacus\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"ISAACUS_API_KEY\"):\n    os.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\n\n    from langchain_isaacus import IsaacusEmbeddings\n\n    embeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\n    ```\n  </Tab>\n</Tabs>\n\n```python  theme={null}\nvector_1 = embeddings.embed_query(all_splits[0].page_content)\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\n\nassert len(vector_1) == len(vector_2)\nprint(f\"Generated vectors of length {len(vector_1)}\\n\")\nprint(vector_1[:10])\n```\n\n```output  theme={null}\nGenerated vectors of length 1536\n\n[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\n```\n\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n\n## 3. Vector stores\n\nLangChain [VectorStore](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) objects contain methods for adding text and [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/oss/python/langchain/retrieval#embedding_models) models, which determine how text data is translated to numeric vectors.\n\nLangChain includes a suite of [integrations](/oss/python/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/oss/python/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:\n\n<Tabs>\n  <Tab title=\"In-memory\">\n    ```shell  theme={null}\n    pip install -U \"langchain-core\"\n    ```\n\n    ```python  theme={null}\n    from langchain_core.vectorstores import InMemoryVectorStore\n\n    vector_store = InMemoryVectorStore(embeddings)\n    ```\n  </Tab>\n\n  <Tab title=\"AstraDB\">\n    ```shell  theme={null}\n    pip install -U \"langchain-astradb\"\n    ```\n\n    ```python  theme={null}\n    from langchain_astradb import AstraDBVectorStore\n\n    vector_store = AstraDBVectorStore(\n        embedding=embeddings,\n        api_endpoint=ASTRA_DB_API_ENDPOINT,\n        collection_name=\"astra_vector_langchain\",\n        token=ASTRA_DB_APPLICATION_TOKEN,\n        namespace=ASTRA_DB_NAMESPACE,\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Chroma\">\n    ```shell  theme={null}\n    pip install -qU langchain-chroma\n    ```\n\n    ```python  theme={null}\n    from langchain_chroma import Chroma\n\n    vector_store = Chroma(\n        collection_name=\"example_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"FAISS\">\n    ```shell  theme={null}\n    pip install -qU langchain-community faiss-cpu\n    ```\n\n    ```python  theme={null}\n    import faiss\n    from langchain_community.docstore.in_memory import InMemoryDocstore\n    from langchain_community.vectorstores import FAISS\n\n    embedding_dim = len(embeddings.embed_query(\"hello world\"))\n    index = faiss.IndexFlatL2(embedding_dim)\n\n    vector_store = FAISS(\n        embedding_function=embeddings,\n        index=index,\n        docstore=InMemoryDocstore(),\n        index_to_docstore_id={},\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Milvus\">\n    ```shell  theme={null}\n    pip install -qU langchain-milvus\n    ```\n\n    ```python  theme={null}\n    from langchain_milvus import Milvus\n\n    URI = \"./milvus_example.db\"\n\n    vector_store = Milvus(\n        embedding_function=embeddings,\n        connection_args={\"uri\": URI},\n        index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"MongoDB\">\n    ```shell  theme={null}\n    pip install -qU langchain-mongodb\n    ```\n\n    ```python  theme={null}\n    from langchain_mongodb import MongoDBAtlasVectorSearch\n\n    vector_store = MongoDBAtlasVectorSearch(\n        embedding=embeddings,\n        collection=MONGODB_COLLECTION,\n        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n        relevance_score_fn=\"cosine\",\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"PGVector\">\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGVector\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=\"my_docs\",\n        connection=\"postgresql+psycopg://...\",\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"PGVectorStore\">\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGEngine, PGVectorStore\n\n    pg_engine = PGEngine.from_connection_string(\n        url=\"postgresql+psycopg://...\"\n    )\n\n    vector_store = PGVectorStore.create_sync(\n        engine=pg_engine,\n        table_name='test_table',\n        embedding_service=embedding\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Pinecone\">\n    ```shell  theme={null}\n    pip install -qU langchain-pinecone\n    ```\n\n    ```python  theme={null}\n    from langchain_pinecone import PineconeVectorStore\n    from pinecone import Pinecone\n\n    pc = Pinecone(api_key=...)\n    index = pc.Index(index_name)\n\n    vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n    ```\n  </Tab>\n\n  <Tab title=\"Qdrant\">\n    ```shell  theme={null}\n    pip install -qU langchain-qdrant\n    ```\n\n    ```python  theme={null}\n    from qdrant_client.models import Distance, VectorParams\n    from langchain_qdrant import QdrantVectorStore\n    from qdrant_client import QdrantClient\n\n    client = QdrantClient(\":memory:\")\n\n    vector_size = len(embeddings.embed_query(\"sample text\"))\n\n    if not client.collection_exists(\"test\"):\n        client.create_collection(\n            collection_name=\"test\",\n            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n        )\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=\"test\",\n        embedding=embeddings,\n    )\n    ```\n  </Tab>\n</Tabs>\n\nHaving instantiated our vector store, we can now index the documents.\n\n```python  theme={null}\nids = vector_store.add_documents(documents=all_splits)\n```\n\nNote that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/oss/python/integrations/vectorstores) for more detail.\n\nOnce we've instantiated a [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) that contains documents, we can query it. [VectorStore](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) includes methods for querying:\n\n* Synchronously and asynchronously;\n* By string query and by vector;\n* With and without returning similarity scores;\n* By similarity and [maximum marginal relevance](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore.max_marginal_relevance_search) (to balance similarity with query to diversity in retrieved results).\n\nThe methods will generally include a list of [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects in their outputs.\n\n**Usage**\n\nEmbeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\n\nReturn documents based on similarity to a string query:\n\n```python  theme={null}\nresults = vector_store.similarity_search(\n    \"How many distribution centers does Nike have in the US?\"\n)\n\nprint(results[0])\n```\n\n```output  theme={null}\npage_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213\nNIKE Brand in-line stores (including employee-only stores) 74\nConverse stores (including factory stores) 82\nTOTAL 369\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\n```\n\nAsync query:\n\n```python  theme={null}\nresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n\nprint(results[0])\n```\n\n```output  theme={null}\npage_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n\nReturn scores:\n\n```python  theme={null}\n# Note that providers implement different scores; the score here\n# is a distance metric that varies inversely with similarity.\n\nresults = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")\ndoc, score = results[0]\nprint(f\"Score: {score}\\n\")\nprint(doc)\n```\n\n```output  theme={null}\nScore: 0.23699893057346344\n\npage_content='Table of Contents\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\nFISCAL 2023 COMPARED TO FISCAL 2022\n•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\n•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\nincrease was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\nequivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n\nReturn documents based on similarity to an embedded query:\n\n```python  theme={null}\nembedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n\nresults = vector_store.similarity_search_by_vector(embedding)\nprint(results[0])\n```\n\n```output  theme={null}\npage_content='Table of Contents\nGROSS MARGIN\nFISCAL 2023 COMPARED TO FISCAL 2022\nFor fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\n43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\n*Wholesale equivalent\nThe decrease in gross margin for fiscal 2023 was primarily due to:\n•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\nproduct mix;\n•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\nthe prior period resulting from lower available inventory supply;\n•Unfavorable changes in net foreign currency exchange rates, including hedges; and\n•Lower off-price margin, on a wholesale equivalent basis.\nThis was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n\nLearn more:\n\n* [API Reference](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore)\n* [Integration-specific docs](/oss/python/integrations/vectorstores)\n\n## 4. Retrievers\n\nLangChain [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) objects do not subclass [Runnable](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.Runnable). LangChain [Retrievers](https://reference.langchain.com/python/langchain_core/retrievers/#langchain_core.retrievers.BaseRetriever) are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\n\nWe can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:\n\n```python  theme={null}\nfrom typing import List\n\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import chain\n\n\n@chain\ndef retriever(query: str) -> List[Document]:\n    return vector_store.similarity_search(query, k=1)\n\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n```\n\n```output  theme={null}\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n```\n\nVectorstores implement an `as_retriever` method that will generate a Retriever, specifically a [VectorStoreRetriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html). These retrievers include specific `search_type` and `search_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\n\n```python  theme={null}\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},\n)\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n```\n\n```output  theme={null}\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n```\n\n`VectorStoreRetriever` supports search types of `\"similarity\"` (default), `\"mmr\"` (maximum marginal relevance, described above), and `\"similarity_score_threshold\"`. We can use the latter to threshold documents output by the retriever by similarity score.\n\nRetrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/oss/python/langchain/retrieval) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/oss/python/langchain/rag) tutorial.\n\n## Next steps\n\nYou've now seen how to build a semantic search engine over a PDF document.\n\nFor more on document loaders:\n\n* [Overview](/oss/python/langchain/retrieval#document_loaders)\n* [Available integrations](/oss/python/integrations/document_loaders/)\n\nFor more on embeddings:\n\n* [Overview](/oss/python/langchain/retrieval#embedding_models/)\n* [Available integrations](/oss/python/integrations/text_embedding/)\n\nFor more on vector stores:\n\n* [Overview](/oss/python/langchain/retrieval#vectorstores/)\n* [Available integrations](/oss/python/integrations/vectorstores/)\n\nFor more on RAG, see:\n\n* [Build a Retrieval Augmented Generation (RAG) App](/oss/python/langchain/rag/)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/knowledge-base.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 33077,
    "word_count": 3365
  },
  {
    "title": "Long-term memory",
    "source": "https://docs.langchain.com/oss/python/langchain/long-term-memory",
    "content": "## Overview\n\nLangChain agents use [LangGraph persistence](/oss/python/langgraph/persistence#memory-store) to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.\n\n## Memory storage\n\nLangGraph stores long-term memories as JSON documents in a [store](/oss/python/langgraph/persistence#memory-store).\n\nEach memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.\n\nThis structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\n\n```python  theme={null}\nfrom langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) # [!code highlight]\nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context) # [!code highlight]\nstore.put( # [!code highlight]\n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\") # [!code highlight]\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search( # [!code highlight]\n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n```\n\nFor more information about the memory store, see the [Persistence](/oss/python/langgraph/persistence#memory-store) guide.\n\n## Read long-term memory in tools\n\n```python A tool the agent can use to look up user information theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n@dataclass\nclass Context:\n    user_id: str\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() # [!code highlight]\n\n# Write sample data to the store using the put method\nstore.put( # [!code highlight]\n    (\"users\",),  # Namespace to group related data together (users namespace for user data)\n    \"user_123\",  # Key within the namespace (user ID as key)\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    }  # Data to store for the given user\n)\n\n@tool\ndef get_user_info(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Look up user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store # [!code highlight]\n    user_id = runtime.context.user_id\n    # Retrieve data from store - returns StoreValue object with value and metadata\n    user_info = store.get((\"users\",), user_id) # [!code highlight]\n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_user_info],\n    # Pass store to agent - enables agent to access store when running tools\n    store=store, # [!code highlight]\n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    context=Context(user_id=\"user_123\") # [!code highlight]\n)\n```\n\n<a id=\"write-long-term\" />\n\n## Write long-term memory from tools\n\n```python Example of a tool that updates user information theme={null}\nfrom dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() # [!code highlight]\n\n@dataclass\nclass Context:\n    user_id: str\n\n# TypedDict defines the structure of user information for the LLM\nclass UserInfo(TypedDict):\n    name: str\n\n# Tool that allows agent to update user information (useful for chat applications)\n@tool\ndef save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Save user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store # [!code highlight]\n    user_id = runtime.context.user_id # [!code highlight]\n    # Store data in the store (namespace, key, data)\n    store.put((\"users\",), user_id, user_info) # [!code highlight]\n    return \"Successfully saved user info.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[save_user_info],\n    store=store, # [!code highlight]\n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    # user_id passed in context to identify whose information is being updated\n    context=Context(user_id=\"user_123\") # [!code highlight]\n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/long-term-memory.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 5664,
    "word_count": 684
  },
  {
    "title": "Model Context Protocol (MCP)",
    "source": "https://docs.langchain.com/oss/python/langchain/mcp",
    "content": "[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library.\n\n## Install\n\nInstall the `langchain-mcp-adapters` library to use MCP tools in LangGraph:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-mcp-adapters\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-mcp-adapters\n  ```\n</CodeGroup>\n\n## Transport types\n\nMCP supports different transport mechanisms for client-server communication:\n\n* **stdio** – Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n* **Streamable HTTP** – Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.\n* **Server-Sent Events (SSE)** – a variant of streamable HTTP optimized for real-time streaming communication.\n\n## Use MCP tools\n\n`langchain-mcp-adapters` enables agents to use tools defined across one or more MCP server.\n\n```python Accessing multiple MCP servers icon=\"server\" theme={null}\nfrom langchain_mcp_adapters.client import MultiServerMCPClient  # [!code highlight]\nfrom langchain.agents import create_agent\n\n\nclient = MultiServerMCPClient(  # [!code highlight]\n    {\n        \"math\": {\n            \"transport\": \"stdio\",  # Local subprocess communication\n            \"command\": \"python\",\n            # Absolute path to your math_server.py file\n            \"args\": [\"/path/to/math_server.py\"],\n        },\n        \"weather\": {\n            \"transport\": \"streamable_http\",  # HTTP-based remote server\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp\",\n        }\n    }\n)\n\ntools = await client.get_tools()  # [!code highlight]\nagent = create_agent(\n    \"claude-sonnet-4-5-20250929\",\n    tools  # [!code highlight]\n)\nmath_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n```\n\n<Note>\n  `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up.\n</Note>\n\n## Custom MCP servers\n\nTo create your own MCP servers, you can use the `mcp` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install mcp\n  ```\n\n  ```bash uv theme={null}\n  uv add mcp\n  ```\n</CodeGroup>\n\nUse the following reference implementations to test your agent with MCP tool servers.\n\n```python title=\"Math server (stdio transport)\" icon=\"floppy-disk\" theme={null}\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n```\n\n```python title=\"Weather server (streamable HTTP transport)\" icon=\"wifi\" theme={null}\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Weather\")\n\n@mcp.tool()\nasync def get_weather(location: str) -> str:\n    \"\"\"Get weather for location.\"\"\"\n    return \"It's always sunny in New York\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```\n\n## Stateful tool usage\n\nFor stateful servers that maintain context between tool calls, use `client.session()` to create a persistent `ClientSession`.\n\n```python Using MCP ClientSession for stateful tool usage theme={null}\nfrom langchain_mcp_adapters.tools import load_mcp_tools\n\nclient = MultiServerMCPClient({...})\nasync with client.session(\"math\") as session:\n    tools = await load_mcp_tools(session)\n```\n\n## Additional resources\n\n* [MCP documentation](https://modelcontextprotocol.io/introduction)\n* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)\n* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/mcp.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 4660,
    "word_count": 516
  },
  {
    "title": "Messages",
    "source": "https://docs.langchain.com/oss/python/langchain/messages",
    "content": "Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n\nMessages are objects that contain:\n\n* <Icon icon=\"user\" size={16} /> [**Role**](#message-types) - Identifies the message type (e.g. `system`, `user`)\n* <Icon icon=\"folder-closed\" size={16} /> [**Content**](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.)\n* <Icon icon=\"tag\" size={16} /> [**Metadata**](#message-metadata) - Optional fields such as response information, message IDs, and token usage\n\nLangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\n\n## Basic usage\n\nThe simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/python/langchain/models#invocation).\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nsystem_msg = SystemMessage(\"You are a helpful assistant.\")\nhuman_msg = HumanMessage(\"Hello, how are you?\")\n\n# Use with chat models\nmessages = [system_msg, human_msg]\nresponse = model.invoke(messages)  # Returns AIMessage\n```\n\n### Text prompts\n\nText prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.\n\n```python  theme={null}\nresponse = model.invoke(\"Write a haiku about spring\")\n```\n\n**Use text prompts when:**\n\n* You have a single, standalone request\n* You don't need conversation history\n* You want minimal code complexity\n\n### Message prompts\n\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.\n\n```python  theme={null}\nfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(\"You are a poetry expert\"),\n    HumanMessage(\"Write a haiku about spring\"),\n    AIMessage(\"Cherry blossoms bloom...\")\n]\nresponse = model.invoke(messages)\n```\n\n**Use message prompts when:**\n\n* Managing multi-turn conversations\n* Working with multimodal content (images, audio, files)\n* Including system instructions\n\n### Dictionary format\n\nYou can also specify messages directly in OpenAI chat completions format.\n\n```python  theme={null}\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n]\nresponse = model.invoke(messages)\n```\n\n## Message types\n\n* <Icon icon=\"gear\" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions\n* <Icon icon=\"user\" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model\n* <Icon icon=\"robot\" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata\n* <Icon icon=\"wrench\" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/python/langchain/models#tool-calling)\n\n### System Message\n\nA [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.\n\n```python Basic instructions theme={null}\nsystem_msg = SystemMessage(\"You are a helpful coding assistant.\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n```\n\n```python Detailed persona theme={null}\nfrom langchain.messages import SystemMessage, HumanMessage\n\nsystem_msg = SystemMessage(\"\"\"\nYou are a senior Python developer with expertise in web frameworks.\nAlways provide code examples and explain your reasoning.\nBe concise but thorough in your explanations.\n\"\"\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n```\n\n***\n\n### Human Message\n\nA [`HumanMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.HumanMessage) represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal [content](#message-content).\n\n#### Text content\n\n<CodeGroup>\n  ```python Message object theme={null}\n  response = model.invoke([\n    HumanMessage(\"What is machine learning?\")\n  ])\n  ```\n\n  ```python String shortcut theme={null}\n  # Using a string is a shortcut for a single HumanMessage\n  response = model.invoke(\"What is machine learning?\")\n  ```\n</CodeGroup>\n\n#### Message metadata\n\n```python Add metadata theme={null}\nhuman_msg = HumanMessage(\n    content=\"Hello!\",\n    name=\"alice\",  # Optional: identify different users\n    id=\"msg_123\",  # Optional: unique identifier for tracing\n)\n```\n\n<Note>\n  The `name` field behavior varies by provider – some use it for user identification, others ignore it. To check, refer to the model provider's [reference](https://reference.langchain.com/python/integrations/).\n</Note>\n\n***\n\n### AI Message\n\nAn [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\n\n```python  theme={null}\nresponse = model.invoke(\"Explain AI\")\nprint(type(response))  # <class 'langchain.messages.AIMessage'>\n```\n\n[`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects are returned by the model when calling it, which contains all of the associated metadata in the response.\n\nProviders weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) object and insert it into the message history as if it came from the model.\n\n```python  theme={null}\nfrom langchain.messages import AIMessage, SystemMessage, HumanMessage\n\n# Create an AI message manually (e.g., for conversation history)\nai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n\n# Add to conversation history\nmessages = [\n    SystemMessage(\"You are a helpful assistant\"),\n    HumanMessage(\"Can you help me?\"),\n    ai_msg,  # Insert as if it came from the model\n    HumanMessage(\"Great! What's 2+2?\")\n]\n\nresponse = model.invoke(messages)\n```\n\n<Accordion title=\"Attributes\">\n  <ParamField path=\"text\" type=\"string\">\n    The text content of the message.\n  </ParamField>\n\n  <ParamField path=\"content\" type=\"string | dict[]\">\n    The raw content of the message.\n  </ParamField>\n\n  <ParamField path=\"content_blocks\" type=\"ContentBlock[]\">\n    The standardized [content blocks](#message-content) of the message.\n  </ParamField>\n\n  <ParamField path=\"tool_calls\" type=\"dict[] | None\">\n    The tool calls made by the model.\n\n    Empty if no tools are called.\n  </ParamField>\n\n  <ParamField path=\"id\" type=\"string\">\n    A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n  </ParamField>\n\n  <ParamField path=\"usage_metadata\" type=\"dict | None\">\n    The usage metadata of the message, which can contain token counts when available.\n  </ParamField>\n\n  <ParamField path=\"response_metadata\" type=\"ResponseMetadata | None\">\n    The response metadata of the message.\n  </ParamField>\n</Accordion>\n\n#### Tool calls\n\nWhen models make [tool calls](/oss/python/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage):\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    ...\n\nmodel_with_tools = model.bind_tools([get_weather])\nresponse = model_with_tools.invoke(\"What's the weather in Paris?\")\n\nfor tool_call in response.tool_calls:\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n    print(f\"ID: {tool_call['id']}\")\n```\n\nOther structured data, such as reasoning or citations, can also appear in message [content](/oss/python/langchain/messages#message-content).\n\n#### Token usage\n\nAn [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) field:\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nresponse = model.invoke(\"Hello!\")\nresponse.usage_metadata\n```\n\n```\n{'input_tokens': 8,\n 'output_tokens': 304,\n 'total_tokens': 312,\n 'input_token_details': {'audio': 0, 'cache_read': 0},\n 'output_token_details': {'audio': 0, 'reasoning': 256}}\n```\n\nSee [`UsageMetadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) for details.\n\n#### Streaming and chunks\n\nDuring streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects that can be combined into a full message object:\n\n```python  theme={null}\nchunks = []\nfull_message = None\nfor chunk in model.stream(\"Hi\"):\n    chunks.append(chunk)\n    print(chunk.text)\n    full_message = chunk if full_message is None else full_message + chunk\n```\n\n<Note>\n  Learn more:\n\n  * [Streaming tokens from chat models](/oss/python/langchain/models#stream)\n  * [Streaming tokens and/or steps from agents](/oss/python/langchain/streaming)\n</Note>\n\n***\n\n### Tool Message\n\nFor models that support [tool calling](/oss/python/langchain/models#tool-calling), AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\n\n[Tools](/oss/python/langchain/tools) can generate [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) objects directly. Below, we show a simple example. Read more in the [tools guide](/oss/python/langchain/tools).\n\n```python  theme={null}\nfrom langchain.messages import AIMessage\nfrom langchain.messages import ToolMessage\n\n# After a model makes a tool call\n# (Here, we demonstrate manually creating the messages for brevity)\nai_message = AIMessage(\n    content=[],\n    tool_calls=[{\n        \"name\": \"get_weather\",\n        \"args\": {\"location\": \"San Francisco\"},\n        \"id\": \"call_123\"\n    }]\n)\n\n# Execute tool and create result message\nweather_result = \"Sunny, 72°F\"\ntool_message = ToolMessage(\n    content=weather_result,\n    tool_call_id=\"call_123\"  # Must match the call ID\n)\n\n# Continue conversation\nmessages = [\n    HumanMessage(\"What's the weather in San Francisco?\"),\n    ai_message,  # Model's tool call\n    tool_message,  # Tool execution result\n]\nresponse = model.invoke(messages)  # Model processes the result\n```\n\n<Accordion title=\"Attributes\">\n  <ParamField path=\"content\" type=\"string\" required>\n    The stringified output of the tool call.\n  </ParamField>\n\n  <ParamField path=\"tool_call_id\" type=\"string\" required>\n    The ID of the tool call that this message is responding to. Must match the ID of the tool call in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage).\n  </ParamField>\n\n  <ParamField path=\"name\" type=\"string\" required>\n    The name of the tool that was called.\n  </ParamField>\n\n  <ParamField path=\"artifact\" type=\"dict\">\n    Additional data not sent to the model but can be accessed programmatically.\n  </ParamField>\n</Accordion>\n\n<Note>\n  The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.\n\n  <Accordion title=\"Example: Using artifact for retrieval metadata\">\n    For example, a [retrieval](/oss/python/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:\n\n    ```python  theme={null}\n    from langchain.messages import ToolMessage\n\n    # Sent to model\n    message_content = \"It was the best of times, it was the worst of times.\"\n\n    # Artifact available downstream\n    artifact = {\"document_id\": \"doc_123\", \"page\": 0}\n\n    tool_message = ToolMessage(\n        content=message_content,\n        tool_call_id=\"call_123\",\n        name=\"search_books\",\n        artifact=artifact,\n    )\n    ```\n\n    See the [RAG tutorial](/oss/python/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/python/langchain/agents) with LangChain.\n  </Accordion>\n</Note>\n\n***\n\n## Message content\n\nYou can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data.\n\nSeparately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below.\n\nLangChain chat models accept message content in the `content` attribute.\n\nThis may contain either:\n\n1. A string\n2. A list of content blocks in a provider-native format\n3. A list of [LangChain's standard content blocks](#standard-content-blocks)\n\nSee below for an example using [multimodal](#multimodal) inputs:\n\n```python  theme={null}\nfrom langchain.messages import HumanMessage\n\n# String content\nhuman_message = HumanMessage(\"Hello, how are you?\")\n\n# Provider-native format (e.g., OpenAI)\nhuman_message = HumanMessage(content=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n])\n\n# List of standard content blocks\nhuman_message = HumanMessage(content_blocks=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n])\n```\n\n<Tip>\n  Specifying `content_blocks` when initializing a message will still populate message\n  `content`, but provides a type-safe interface for doing so.\n</Tip>\n\n### Standard content blocks\n\nLangChain provides a standard representation for message content that works across providers.\n\nMessage objects implement a `content_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/python/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/python/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:\n\n<Tabs>\n  <Tab title=\"Anthropic\">\n    ```python  theme={null}\n    from langchain.messages import AIMessage\n\n    message = AIMessage(\n        content=[\n            {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n            {\"type\": \"text\", \"text\": \"...\"},\n        ],\n        response_metadata={\"model_provider\": \"anthropic\"}\n    )\n    message.content_blocks\n    ```\n\n    ```\n    [{'type': 'reasoning',\n      'reasoning': '...',\n      'extras': {'signature': 'WaUjzkyp...'}},\n     {'type': 'text', 'text': '...'}]\n    ```\n  </Tab>\n\n  <Tab title=\"OpenAI\">\n    ```python  theme={null}\n    from langchain.messages import AIMessage\n\n    message = AIMessage(\n        content=[\n            {\n                \"type\": \"reasoning\",\n                \"id\": \"rs_abc123\",\n                \"summary\": [\n                    {\"type\": \"summary_text\", \"text\": \"summary 1\"},\n                    {\"type\": \"summary_text\", \"text\": \"summary 2\"},\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"...\", \"id\": \"msg_abc123\"},\n        ],\n        response_metadata={\"model_provider\": \"openai\"}\n    )\n    message.content_blocks\n    ```\n\n    ```\n    [{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},\n     {'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},\n     {'type': 'text', 'text': '...', 'id': 'msg_abc123'}]\n    ```\n  </Tab>\n</Tabs>\n\nSee the [integrations guides](/oss/python/integrations/providers/overview) to get started with the\ninference provider of your choice.\n\n<Note>\n  **Serializing standard content**\n\n  If an application outside of LangChain needs access to the standard content block\n  representation, you can opt-in to storing content blocks in message content.\n\n  To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,\n  initialize any chat model with `output_version=\"v1\"`:\n\n  ```python  theme={null}\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n  ```\n</Note>\n\n### Multimodal\n\n**Multimodality** refers to the ability to work with data that comes in different\nforms, such as text, audio, images, and video. LangChain includes standard types\nfor these data that can be used across providers.\n\n[Chat models](/oss/python/langchain/models) can accept multimodal data as input and generate\nit as output. Below we show short examples of input messages featuring multimodal data.\n\n<Note>\n  Extra keys can be included top-level in the content block or nested in `\"extras\": {\"key\": value}`.\n\n  [OpenAI](/oss/python/integrations/chat/openai#pdfs) and [AWS Bedrock Converse](/oss/python/integrations/chat/bedrock),\n  for example, require a filename for PDFs. See the [provider page](/oss/python/integrations/providers/overview)\n  for your chosen model for specifics.\n</Note>\n\n<CodeGroup>\n  ```python Image input theme={null}\n  # From URL\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n          {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n      ]\n  }\n\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n          {\n              \"type\": \"image\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"image/jpeg\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n          {\"type\": \"image\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n\n  ```python PDF document input theme={null}\n  # From URL\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n          {\"type\": \"file\", \"url\": \"https://example.com/path/to/document.pdf\"},\n      ]\n  }\n\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n          {\n              \"type\": \"file\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"application/pdf\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n          {\"type\": \"file\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n\n  ```python Audio input theme={null}\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"},\n          {\n              \"type\": \"audio\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"audio/wav\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"},\n          {\"type\": \"audio\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n\n  ```python Video input theme={null}\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n          {\n              \"type\": \"video\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"video/mp4\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n          {\"type\": \"video\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n</CodeGroup>\n\n<Warning>\n  Not all models support all file types. Check the model provider's [reference](https://reference.langchain.com/python/integrations/) for supported formats and size limits.\n</Warning>\n\n### Content block reference\n\nContent blocks are represented (either when creating a message or accessing the `content_blocks` property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\n\n<AccordionGroup>\n  <Accordion title=\"Core\" icon=\"cube\">\n    <AccordionGroup>\n      <Accordion title=\"TextContentBlock\" icon=\"text\">\n        **Purpose:** Standard text output\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"text\"`\n        </ParamField>\n\n        <ParamField body=\"text\" type=\"string\" required>\n          The text content\n        </ParamField>\n\n        <ParamField body=\"annotations\" type=\"object[]\">\n          List of annotations for the text\n        </ParamField>\n\n        <ParamField body=\"extras\" type=\"object\">\n          Additional provider-specific data\n        </ParamField>\n\n        **Example:**\n\n        ```python  theme={null}\n        {\n            \"type\": \"text\",\n            \"text\": \"Hello world\",\n            \"annotations\": []\n        }\n        ```\n      </Accordion>\n\n      <Accordion title=\"ReasoningContentBlock\" icon=\"brain\">\n        **Purpose:** Model reasoning steps\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"reasoning\"`\n        </ParamField>\n\n        <ParamField body=\"reasoning\" type=\"string\">\n          The reasoning content\n        </ParamField>\n\n        <ParamField body=\"extras\" type=\"object\">\n          Additional provider-specific data\n        </ParamField>\n\n        **Example:**\n\n        ```python  theme={null}\n        {\n            \"type\": \"reasoning\",\n            \"reasoning\": \"The user is asking about...\",\n            \"extras\": {\"signature\": \"abc123\"},\n        }\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Multimodal\" icon=\"images\">\n    <AccordionGroup>\n      <Accordion title=\"ImageContentBlock\" icon=\"image\">\n        **Purpose:** Image data\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"image\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the image location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded image data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored image (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"AudioContentBlock\" icon=\"volume-high\">\n        **Purpose:** Audio data\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"audio\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the audio location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded audio data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored audio file (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"VideoContentBlock\" icon=\"video\">\n        **Purpose:** Video data\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"video\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the video location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded video data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored video file (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"FileContentBlock\" icon=\"file\">\n        **Purpose:** Generic files (PDF, etc)\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"file\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the file location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded file data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored file (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"PlainTextContentBlock\" icon=\"align-left\">\n        **Purpose:** Document text (`.txt`, `.md`)\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"text-plain\"`\n        </ParamField>\n\n        <ParamField body=\"text\" type=\"string\">\n          The text content\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) of the text (e.g., `text/plain`, `text/markdown`)\n        </ParamField>\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Tool Calling\" icon=\"wrench\">\n    <AccordionGroup>\n      <Accordion title=\"ToolCall\" icon=\"function\">\n        **Purpose:** Function calls\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"tool_call\"`\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\" required>\n          Name of the tool to call\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"object\" required>\n          Arguments to pass to the tool\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\" required>\n          Unique identifier for this tool call\n        </ParamField>\n\n        **Example:**\n\n        ```python  theme={null}\n        {\n            \"type\": \"tool_call\",\n            \"name\": \"search\",\n            \"args\": {\"query\": \"weather\"},\n            \"id\": \"call_123\"\n        }\n        ```\n      </Accordion>\n\n      <Accordion title=\"ToolCallChunk\" icon=\"puzzle-piece\">\n        **Purpose:** Streaming tool call fragments\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"tool_call_chunk\"`\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\">\n          Name of the tool being called\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"string\">\n          Partial tool arguments (may be incomplete JSON)\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Tool call identifier\n        </ParamField>\n\n        <ParamField body=\"index\" type=\"number | string\">\n          Position of this chunk in the stream\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"InvalidToolCall\" icon=\"triangle-exclamation\">\n        **Purpose:** Malformed calls, intended to catch JSON parsing errors.\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"invalid_tool_call\"`\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\">\n          Name of the tool that failed to be called\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"object\">\n          Arguments to pass to the tool\n        </ParamField>\n\n        <ParamField body=\"error\" type=\"string\">\n          Description of what went wrong\n        </ParamField>\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Server-Side Tool Execution\" icon=\"server\">\n    <AccordionGroup>\n      <Accordion title=\"ServerToolCall\" icon=\"wrench\">\n        **Purpose:** Tool call that is executed server-side.\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"server_tool_call\"`\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\" required>\n          An identifier associated with the tool call.\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\" required>\n          The name of the tool to be called.\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"string\" required>\n          Partial tool arguments (may be incomplete JSON)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"ServerToolCallChunk\" icon=\"puzzle-piece\">\n        **Purpose:** Streaming server-side tool call fragments\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"server_tool_call_chunk\"`\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          An identifier associated with the tool call.\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\">\n          Name of the tool being called\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"string\">\n          Partial tool arguments (may be incomplete JSON)\n        </ParamField>\n\n        <ParamField body=\"index\" type=\"number | string\">\n          Position of this chunk in the stream\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"ServerToolResult\" icon=\"box-open\">\n        **Purpose:** Search results\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"server_tool_result\"`\n        </ParamField>\n\n        <ParamField body=\"tool_call_id\" type=\"string\" required>\n          Identifier of the corresponding server tool call.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Identifier associated with the server tool result.\n        </ParamField>\n\n        <ParamField body=\"status\" type=\"string\" required>\n          Execution status of the server-side tool. `\"success\"` or `\"error\"`.\n        </ParamField>\n\n        <ParamField body=\"output\">\n          Output of the executed tool.\n        </ParamField>\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Provider-Specific Blocks\" icon=\"plug\">\n    <Accordion title=\"NonStandardContentBlock\" icon=\"asterisk\">\n      **Purpose:** Provider-specific escape hatch\n\n      <ParamField body=\"type\" type=\"string\" required>\n        Always `\"non_standard\"`\n      </ParamField>\n\n      <ParamField body=\"value\" type=\"object\" required>\n        Provider-specific data structure\n      </ParamField>\n\n      **Usage:** For experimental or provider-unique features\n    </Accordion>\n\n    Additional provider-specific content types may be found within the [reference documentation](/oss/python/integrations/providers/overview) of each model provider.\n  </Accordion>\n</AccordionGroup>\n\n<Tip>\n  View the canonical type definitions in the [API reference](https://reference.langchain.com/python/langchain/messages).\n</Tip>\n\n<Info>\n  Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.\n\n  Content blocks are not a replacement for the [`content`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content) property, but rather a new property that can be used to access the content of a message in a standardized format.\n</Info>\n\n## Use with chat models\n\n[Chat models](/oss/python/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\n\nRefer to the below guides to learn more:\n\n* Built-in features for [persisting and managing conversation histories](/oss/python/langchain/short-term-memory)\n* Strategies for managing context windows, including [trimming and summarizing messages](/oss/python/langchain/short-term-memory#common-patterns)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 34145,
    "word_count": 3370
  },
  {
    "title": "Built-in middleware",
    "source": "https://docs.langchain.com/oss/python/langchain/middleware/built-in",
    "content": "Prebuilt middleware for common agent use cases\n\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\n\n## Provider-agnostic middleware\n\nThe following middleware work with any LLM provider:\n\n| Middleware                              | Description                                                                 |\n| --------------------------------------- | --------------------------------------------------------------------------- |\n| [Summarization](#summarization)         | Automatically summarize conversation history when approaching token limits. |\n| [Human-in-the-loop](#human-in-the-loop) | Pause execution for human approval of tool calls.                           |\n| [Model call limit](#model-call-limit)   | Limit the number of model calls to prevent excessive costs.                 |\n| [Tool call limit](#tool-call-limit)     | Control tool execution by limiting call counts.                             |\n| [Model fallback](#model-fallback)       | Automatically fallback to alternative models when primary fails.            |\n| [PII detection](#pii-detection)         | Detect and handle Personally Identifiable Information (PII).                |\n| [To-do list](#to-do-list)               | Equip agents with task planning and tracking capabilities.                  |\n| [LLM tool selector](#llm-tool-selector) | Use an LLM to select relevant tools before calling main model.              |\n| [Tool retry](#tool-retry)               | Automatically retry failed tool calls with exponential backoff.             |\n| [Model retry](#model-retry)             | Automatically retry failed model calls with exponential backoff.            |\n| [LLM tool emulator](#llm-tool-emulator) | Emulate tool execution using an LLM for testing purposes.                   |\n| [Context editing](#context-editing)     | Manage conversation context by trimming or clearing tool uses.              |\n| [Shell tool](#shell-tool)               | Expose a persistent shell session to agents for command execution.          |\n| [File search](#file-search)             | Provide Glob and Grep search tools over filesystem files.                   |\n\n### Summarization\n\nAutomatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:\n\n* Long-running conversations that exceed context windows.\n* Multi-turn dialogues with extensive history.\n* Applications where preserving full conversation context matters.\n\n**API reference:** [`SummarizationMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.SummarizationMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[your_weather_tool, your_calculator_tool],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 20),\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <Tip>\n    The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model's [profile data](/oss/python/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:\n\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n\n    custom_profile = {\n        \"max_input_tokens\": 100_000,\n        # ...\n    }\n    model = init_chat_model(\"gpt-4o\", profile=custom_profile)\n    ```\n  </Tip>\n\n  <ParamField body=\"model\" type=\"string | BaseChatModel\" required>\n    Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) for more information.\n  </ParamField>\n\n  <ParamField body=\"trigger\" type=\"ContextSize | list[ContextSize] | None\">\n    Conditions for triggering summarization. Can be:\n\n    * A single [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dict (all properties must be met - AND logic)\n    * A list of [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dicts (any condition must be met - OR logic)\n\n    Each condition can include:\n\n    * `fraction` (float): Fraction of model's context size (0-1)\n    * `tokens` (int): Absolute token count\n    * `messages` (int): Message count\n\n    At least one property must be specified per condition. If not provided, summarization will not trigger automatically.\n\n    See the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.\n  </ParamField>\n\n  <ParamField body=\"keep\" type=\"ContextSize\" default=\"{messages: 20}\">\n    How much context to preserve after summarization. Specify exactly one of:\n\n    * `fraction` (float): Fraction of model's context size to keep (0-1)\n    * `tokens` (int): Absolute token count to keep\n    * `messages` (int): Number of recent messages to keep\n\n    See the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.\n  </ParamField>\n\n  <ParamField body=\"token_counter\" type=\"function\">\n    Custom token counting function. Defaults to character-based counting.\n  </ParamField>\n\n  <ParamField body=\"summary_prompt\" type=\"string\">\n    Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.\n  </ParamField>\n\n  <ParamField body=\"trim_tokens_to_summarize\" type=\"number\" default=\"4000\">\n    Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.\n  </ParamField>\n\n  <ParamField body=\"summary_prefix\" type=\"string\">\n    Prefix to add to the summary message. If not provided, a default prefix is used.\n  </ParamField>\n\n  <ParamField body=\"max_tokens_before_summary\" type=\"number\" deprecated>\n    **Deprecated:** Use `trigger: {\"tokens\": value}` instead. Token threshold for triggering summarization.\n  </ParamField>\n\n  <ParamField body=\"messages_to_keep\" type=\"number\" deprecated>\n    **Deprecated:** Use `keep: {\"messages\": value}` instead. Recent messages to preserve.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.\n\n  **Trigger conditions** control when summarization runs:\n\n  * Single condition object (all properties must be met - AND logic)\n  * Array of conditions (any condition must be met - OR logic)\n  * Each condition can use `fraction` (of model's context size), `tokens` (absolute count), or `messages` (message count)\n\n  **Keep conditions** control how much context to preserve (specify exactly one):\n\n  * `fraction` - Fraction of model's context size to keep\n  * `tokens` - Absolute token count to keep\n  * `messages` - Number of recent messages to keep\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import SummarizationMiddleware\n\n\n  # Single condition: trigger if tokens >= 4000 AND messages >= 10\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[your_weather_tool, your_calculator_tool],\n      middleware=[\n          SummarizationMiddleware(\n              model=\"gpt-4o-mini\",\n              trigger=[(\"tokens\", 4000), (\"messages\", 10)],\n              keep=(\"messages\", 20),\n          ),\n      ],\n  )\n\n  # Multiple conditions\n  agent2 = create_agent(\n      model=\"gpt-4o\",\n      tools=[your_weather_tool, your_calculator_tool],\n      middleware=[\n          SummarizationMiddleware(\n              model=\"gpt-4o-mini\",\n              trigger=[\n                  (\"tokens\", 3000),\n                  (\"messages\", 6),\n              ],\n              keep=(\"messages\", 20),\n          ),\n      ],\n  )\n\n  # Using fractional limits\n  agent3 = create_agent(\n      model=\"gpt-4o\",\n      tools=[your_weather_tool, your_calculator_tool],\n      middleware=[\n          SummarizationMiddleware(\n              model=\"gpt-4o-mini\",\n              trigger=(\"fraction\", 0.8),\n              keep=(\"fraction\", 0.3),\n          ),\n      ],\n  )\n  ```\n</Accordion>\n\n### Human-in-the-loop\n\nPause agent execution for human approval, editing, or rejection of tool calls before they execute. [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) is useful for the following:\n\n* High-stakes operations requiring human approval (e.g. database writes, financial transactions).\n* Compliance workflows where human oversight is mandatory.\n* Long-running conversations where human feedback guides the agent.\n\n**API reference:** [`HumanInTheLoopMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.HumanInTheLoopMiddleware)\n\n<Warning>\n  Human-in-the-loop middleware requires a [checkpointer](/oss/python/langgraph/persistence#checkpoints) to maintain state across interruptions.\n</Warning>\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\ndef read_email_tool(email_id: str) -> str:\n    \"\"\"Mock function to read an email by its ID.\"\"\"\n    return f\"Email content for ID: {email_id}\"\n\ndef send_email_tool(recipient: str, subject: str, body: str) -> str:\n    \"\"\"Mock function to send an email.\"\"\"\n    return f\"Email sent to {recipient} with subject '{subject}'\"\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[your_read_email_tool, your_send_email_tool],\n    checkpointer=InMemorySaver(),\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                \"your_send_email_tool\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n                },\n                \"your_read_email_tool\": False,\n            }\n        ),\n    ],\n)\n```\n\n<Tip>\n  For complete examples, configuration options, and integration patterns, see the [Human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop).\n</Tip>\n\n### Model call limit\n\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:\n\n* Preventing runaway agents from making too many API calls.\n* Enforcing cost controls on production deployments.\n* Testing agent behavior within specific call budgets.\n\n**API reference:** [`ModelCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelCallLimitMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelCallLimitMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        ModelCallLimitMiddleware(\n            thread_limit=10,\n            run_limit=5,\n            exit_behavior=\"end\",\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"thread_limit\" type=\"number\">\n    Maximum model calls across all runs in a thread. Defaults to no limit.\n  </ParamField>\n\n  <ParamField body=\"run_limit\" type=\"number\">\n    Maximum model calls per single invocation. Defaults to no limit.\n  </ParamField>\n\n  <ParamField body=\"exit_behavior\" type=\"string\" default=\"end\">\n    Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (raise exception)\n  </ParamField>\n</Accordion>\n\n### Tool call limit\n\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:\n\n* Preventing excessive calls to expensive external APIs.\n* Limiting web searches or database queries.\n* Enforcing rate limits on specific tool usage.\n* Protecting against runaway agent loops.\n\n**API reference:** [`ToolCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolCallLimitMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolCallLimitMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        # Global limit\n        ToolCallLimitMiddleware(thread_limit=20, run_limit=10),\n        # Tool-specific limit\n        ToolCallLimitMiddleware(\n            tool_name=\"search\",\n            thread_limit=5,\n            run_limit=3,\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"tool_name\" type=\"string\">\n    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.\n  </ParamField>\n\n  <ParamField body=\"thread_limit\" type=\"number\">\n    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.\n  </ParamField>\n\n  <ParamField body=\"run_limit\" type=\"number\">\n    Maximum tool calls per single invocation (one user message → response cycle). Resets with each new user message. `None` means no run limit.\n\n    **Note:** At least one of `thread_limit` or `run_limit` must be specified.\n  </ParamField>\n\n  <ParamField body=\"exit_behavior\" type=\"string\" default=\"continue\">\n    Behavior when limit is reached:\n\n    * `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.\n    * `'error'` - Raise a `ToolCallLimitExceededError` exception, stopping execution immediately\n    * `'end'` - Stop execution immediately with a `ToolMessage` and AI message for the exceeded tool call. Only works when limiting a single tool; raises `NotImplementedError` if other tools have pending calls.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  Specify limits with:\n\n  * **Thread limit** - Max calls across all runs in a conversation (requires checkpointer)\n  * **Run limit** - Max calls per single invocation (resets each turn)\n\n  Exit behaviors:\n\n  * `'continue'` (default) - Block exceeded calls with error messages, agent continues\n  * `'error'` - Raise exception immediately\n  * `'end'` - Stop with ToolMessage + AI message (single-tool scenarios only)\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import ToolCallLimitMiddleware\n\n\n  global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\n  search_limiter = ToolCallLimitMiddleware(tool_name=\"search\", thread_limit=5, run_limit=3)\n  database_limiter = ToolCallLimitMiddleware(tool_name=\"query_database\", thread_limit=10)\n  strict_limiter = ToolCallLimitMiddleware(tool_name=\"scrape_webpage\", run_limit=2, exit_behavior=\"error\")\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, database_tool, scraper_tool],\n      middleware=[global_limiter, search_limiter, database_limiter, strict_limiter],\n  )\n  ```\n</Accordion>\n\n### Model fallback\n\nAutomatically fallback to alternative models when the primary model fails. Model fallback is useful for the following:\n\n* Building resilient agents that handle model outages.\n* Cost optimization by falling back to cheaper models.\n* Provider redundancy across OpenAI, Anthropic, etc.\n\n**API reference:** [`ModelFallbackMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelFallbackMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelFallbackMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        ModelFallbackMiddleware(\n            \"gpt-4o-mini\",\n            \"claude-3-5-sonnet-20241022\",\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"first_model\" type=\"string | BaseChatModel\" required>\n    First fallback model to try when the primary model fails. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance.\n  </ParamField>\n\n  <ParamField body=\"*additional_models\" type=\"string | BaseChatModel\">\n    Additional fallback models to try in order if previous models fail\n  </ParamField>\n</Accordion>\n\n### PII detection\n\nDetect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:\n\n* Healthcare and financial applications with compliance requirements.\n* Customer service agents that need to sanitize logs.\n* Any application handling sensitive user data.\n\n**API reference:** [`PIIMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.PIIMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n    ],\n)\n```\n\n#### Custom PII types\n\nYou can create custom PII types by providing a `detector` parameter. This allows you to detect patterns specific to your use case beyond the built-in types.\n\n**Three ways to create custom detectors:**\n\n1. **Regex pattern string** - Simple pattern matching\n\n2. **Custom function** - Complex detection logic with validation\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\nimport re\n\n\n# Method 1: Regex pattern string\nagent1 = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n        ),\n    ],\n)\n\n# Method 2: Compiled regex pattern\nagent2 = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"phone_number\",\n            detector=re.compile(r\"\\+?\\d{1,3}[\\s.-]?\\d{3,4}[\\s.-]?\\d{4}\"),\n            strategy=\"mask\",\n        ),\n    ],\n)\n\n# Method 3: Custom detector function\ndef detect_ssn(content: str) -> list[dict[str, str | int]]:\n    \"\"\"Detect SSN with validation.\n\n    Returns a list of dictionaries with 'text', 'start', and 'end' keys.\n    \"\"\"\n    import re\n    matches = []\n    pattern = r\"\\d{3}-\\d{2}-\\d{4}\"\n    for match in re.finditer(pattern, content):\n        ssn = match.group(0)\n        # Validate: first 3 digits shouldn't be 000, 666, or 900-999\n        first_three = int(ssn[:3])\n        if first_three not in [0, 666] and not (900 <= first_three <= 999):\n            matches.append({\n                \"text\": ssn,\n                \"start\": match.start(),\n                \"end\": match.end(),\n            })\n    return matches\n\nagent3 = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"ssn\",\n            detector=detect_ssn,\n            strategy=\"hash\",\n        ),\n    ],\n)\n```\n\n**Custom detector function signature:**\n\nThe detector function must accept a string (content) and return matches:\n\nReturns a list of dictionaries with `text`, `start`, and `end` keys:\n\n```python  theme={null}\ndef detector(content: str) -> list[dict[str, str | int]]:\n    return [\n        {\"text\": \"matched_text\", \"start\": 0, \"end\": 12},\n        # ... more matches\n    ]\n```\n\n<Tip>\n  For custom detectors:\n\n  * Use regex strings for simple patterns\n  * Use RegExp objects when you need flags (e.g., case-insensitive matching)\n  * Use custom functions when you need validation logic beyond pattern matching\n  * Custom functions give you full control over detection logic and can implement complex validation rules\n</Tip>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"pii_type\" type=\"string\" required>\n    Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.\n  </ParamField>\n\n  <ParamField body=\"strategy\" type=\"string\" default=\"redact\">\n    How to handle detected PII. Options:\n\n    * `'block'` - Raise exception when detected\n    * `'redact'` - Replace with `[REDACTED_TYPE]`\n    * `'mask'` - Partially mask (e.g., `****-****-****-1234`)\n    * `'hash'` - Replace with deterministic hash\n  </ParamField>\n\n  <ParamField body=\"detector\" type=\"function | regex\">\n    Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.\n  </ParamField>\n\n  <ParamField body=\"apply_to_input\" type=\"boolean\" default=\"True\">\n    Check user messages before model call\n  </ParamField>\n\n  <ParamField body=\"apply_to_output\" type=\"boolean\" default=\"False\">\n    Check AI messages after model call\n  </ParamField>\n\n  <ParamField body=\"apply_to_tool_results\" type=\"boolean\" default=\"False\">\n    Check tool result messages after execution\n  </ParamField>\n</Accordion>\n\n### To-do list\n\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\n\n* Complex multi-step tasks requiring coordination across multiple tools.\n* Long-running operations where progress visibility is important.\n\n<Note>\n  This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.\n</Note>\n\n**API reference:** [`TodoListMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.TodoListMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import TodoListMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[read_file, write_file, run_tests],\n    middleware=[TodoListMiddleware()],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"system_prompt\" type=\"string\">\n    Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.\n  </ParamField>\n\n  <ParamField body=\"tool_description\" type=\"string\">\n    Custom description for the `write_todos` tool. Uses built-in description if not specified.\n  </ParamField>\n</Accordion>\n\n### LLM tool selector\n\nUse an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:\n\n* Agents with many tools (10+) where most aren't relevant per query.\n* Reducing token usage by filtering irrelevant tools.\n* Improving model focus and accuracy.\n\nThis middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.\n\n**API reference:** [`LLMToolSelectorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.LLMToolSelectorMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolSelectorMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[tool1, tool2, tool3, tool4, tool5, ...],\n    middleware=[\n        LLMToolSelectorMiddleware(\n            model=\"gpt-4o-mini\",\n            max_tools=3,\n            always_include=[\"search\"],\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) for more information.\n\n    Defaults to the agent's main model.\n  </ParamField>\n\n  <ParamField body=\"system_prompt\" type=\"string\">\n    Instructions for the selection model. Uses built-in prompt if not specified.\n  </ParamField>\n\n  <ParamField body=\"max_tools\" type=\"number\">\n    Maximum number of tools to select. If the model selects more, only the first max\\_tools will be used. No limit if not specified.\n  </ParamField>\n\n  <ParamField body=\"always_include\" type=\"list[string]\">\n    Tool names to always include regardless of selection. These do not count against the max\\_tools limit.\n  </ParamField>\n</Accordion>\n\n### Tool retry\n\nAutomatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:\n\n* Handling transient failures in external API calls.\n* Improving reliability of network-dependent tools.\n* Building resilient agents that gracefully handle temporary errors.\n\n**API reference:** [`ToolRetryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolRetryMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"max_retries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default)\n  </ParamField>\n\n  <ParamField body=\"tools\" type=\"list[BaseTool | str]\">\n    Optional list of tools or tool names to apply retry logic to. If `None`, applies to all tools.\n  </ParamField>\n\n  <ParamField body=\"retry_on\" type=\"tuple[type[Exception], ...] | callable\" default=\"(Exception,)\">\n    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.\n  </ParamField>\n\n  <ParamField body=\"on_failure\" type=\"string | callable\" default=\"return_message\">\n    Behavior when all retries are exhausted. Options:\n\n    * `'return_message'` - Return a `ToolMessage` with error details (allows LLM to handle failure)\n    * `'raise'` - Re-raise the exception (stops agent execution)\n    * Custom callable - Function that takes the exception and returns a string for the `ToolMessage` content\n  </ParamField>\n\n  <ParamField body=\"backoff_factor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.\n  </ParamField>\n\n  <ParamField body=\"initial_delay\" type=\"number\" default=\"1.0\">\n    Initial delay in seconds before first retry\n  </ParamField>\n\n  <ParamField body=\"max_delay\" type=\"number\" default=\"60.0\">\n    Maximum delay in seconds between retries (caps exponential backoff growth)\n  </ParamField>\n\n  <ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`±25%`) to delay to avoid thundering herd\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware automatically retries failed tool calls with exponential backoff.\n\n  **Key configuration:**\n\n  * `max_retries` - Number of retry attempts (default: 2)\n  * `backoff_factor` - Multiplier for exponential backoff (default: 2.0)\n  * `initial_delay` - Starting delay in seconds (default: 1.0)\n  * `max_delay` - Cap on delay growth (default: 60.0)\n  * `jitter` - Add random variation (default: True)\n\n  **Failure handling:**\n\n  * `on_failure='return_message'` - Return error message\n  * `on_failure='raise'` - Re-raise exception\n  * Custom function - Function returning error message\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import ToolRetryMiddleware\n\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, database_tool, api_tool],\n      middleware=[\n          ToolRetryMiddleware(\n              max_retries=3,\n              backoff_factor=2.0,\n              initial_delay=1.0,\n              max_delay=60.0,\n              jitter=True,\n              tools=[\"api_tool\"],\n              retry_on=(ConnectionError, TimeoutError),\n              on_failure=\"continue\",\n          ),\n      ],\n  )\n  ```\n</Accordion>\n\n### Model retry\n\nAutomatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following:\n\n* Handling transient failures in model API calls.\n* Improving reliability of network-dependent model requests.\n* Building resilient agents that gracefully handle temporary model errors.\n\n**API reference:** [`ModelRetryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRetryMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ModelRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"max_retries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default)\n  </ParamField>\n\n  <ParamField body=\"retry_on\" type=\"tuple[type[Exception], ...] | callable\" default=\"(Exception,)\">\n    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.\n  </ParamField>\n\n  <ParamField body=\"on_failure\" type=\"string | callable\" default=\"continue\">\n    Behavior when all retries are exhausted. Options:\n\n    * `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully\n    * `'error'` - Re-raise the exception (stops agent execution)\n    * Custom callable - Function that takes the exception and returns a string for the `AIMessage` content\n  </ParamField>\n\n  <ParamField body=\"backoff_factor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.\n  </ParamField>\n\n  <ParamField body=\"initial_delay\" type=\"number\" default=\"1.0\">\n    Initial delay in seconds before first retry\n  </ParamField>\n\n  <ParamField body=\"max_delay\" type=\"number\" default=\"60.0\">\n    Maximum delay in seconds between retries (caps exponential backoff growth)\n  </ParamField>\n\n  <ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`±25%`) to delay to avoid thundering herd\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware automatically retries failed model calls with exponential backoff.\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import ModelRetryMiddleware\n\n\n  # Basic usage with default settings (2 retries, exponential backoff)\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool],\n      middleware=[ModelRetryMiddleware()],\n  )\n\n  # Custom exception filtering\n  class TimeoutError(Exception):\n      \"\"\"Custom exception for timeout errors.\"\"\"\n      pass\n\n  class ConnectionError(Exception):\n      \"\"\"Custom exception for connection errors.\"\"\"\n      pass\n\n  # Retry specific exceptions only\n  retry = ModelRetryMiddleware(\n      max_retries=4,\n      retry_on=(TimeoutError, ConnectionError),\n      backoff_factor=1.5,\n  )\n\n\n  def should_retry(error: Exception) -> bool:\n      # Only retry on rate limit errors\n      if isinstance(error, TimeoutError):\n          return True\n      # Or check for specific HTTP status codes\n      if hasattr(error, \"status_code\"):\n          return error.status_code in (429, 503)\n      return False\n\n  retry_with_filter = ModelRetryMiddleware(\n      max_retries=3,\n      retry_on=should_retry,\n  )\n\n  # Return error message instead of raising\n  retry_continue = ModelRetryMiddleware(\n      max_retries=4,\n      on_failure=\"continue\",  # Return AIMessage with error instead of raising\n  )\n\n  # Custom error message formatting\n  def format_error(error: Exception) -> str:\n      return f\"Model call failed: {error}. Please try again later.\"\n\n  retry_with_formatter = ModelRetryMiddleware(\n      max_retries=4,\n      on_failure=format_error,\n  )\n\n  # Constant backoff (no exponential growth)\n  constant_backoff = ModelRetryMiddleware(\n      max_retries=5,\n      backoff_factor=0.0,  # No exponential growth\n      initial_delay=2.0,  # Always wait 2 seconds\n  )\n\n  # Raise exception on failure\n  strict_retry = ModelRetryMiddleware(\n      max_retries=2,\n      on_failure=\"error\",  # Re-raise exception instead of returning message\n  )\n  ```\n</Accordion>\n\n### LLM tool emulator\n\nEmulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following:\n\n* Testing agent behavior without executing real tools.\n* Developing agents when external tools are unavailable or expensive.\n* Prototyping agent workflows before implementing actual tools.\n\n**API reference:** [`LLMToolEmulator`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.LLMToolEmulator)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolEmulator\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[get_weather, search_database, send_email],\n    middleware=[\n        LLMToolEmulator(),  # Emulate all tools\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"tools\" type=\"list[str | BaseTool]\">\n    List of tool names (str) or BaseTool instances to emulate. If `None` (default), ALL tools will be emulated. If empty list `[]`, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.\n  </ParamField>\n\n  <ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model to use for generating emulated tool responses. Can be a model identifier string (e.g., `'anthropic:claude-sonnet-4-5-20250929'`) or a `BaseChatModel` instance. Defaults to the agent's model if not specified. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) for more information.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools.\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import LLMToolEmulator\n  from langchain.tools import tool\n\n\n  @tool\n  def get_weather(location: str) -> str:\n      \"\"\"Get the current weather for a location.\"\"\"\n      return f\"Weather in {location}\"\n\n  @tool\n  def send_email(to: str, subject: str, body: str) -> str:\n      \"\"\"Send an email.\"\"\"\n      return \"Email sent\"\n\n\n  # Emulate all tools (default behavior)\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[get_weather, send_email],\n      middleware=[LLMToolEmulator()],\n  )\n\n  # Emulate specific tools only\n  agent2 = create_agent(\n      model=\"gpt-4o\",\n      tools=[get_weather, send_email],\n      middleware=[LLMToolEmulator(tools=[\"get_weather\"])],\n  )\n\n  # Use custom model for emulation\n  agent4 = create_agent(\n      model=\"gpt-4o\",\n      tools=[get_weather, send_email],\n      middleware=[LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\")],\n  )\n  ```\n</Accordion>\n\n### Context editing\n\nManage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following:\n\n* Long conversations with many tool calls that exceed token limits\n* Reducing token costs by removing older tool outputs that are no longer relevant\n* Maintaining only the most recent N tool results in context\n\n**API reference:** [`ContextEditingMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ContextEditingMiddleware), [`ClearToolUsesEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClearToolUsesEdit)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        ContextEditingMiddleware(\n            edits=[\n                ClearToolUsesEdit(\n                    trigger=100000,\n                    keep=3,\n                ),\n            ],\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"edits\" type=\"list[ContextEdit]\" default=\"[ClearToolUsesEdit()]\">\n    List of [`ContextEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ContextEdit) strategies to apply\n  </ParamField>\n\n  <ParamField body=\"token_count_method\" type=\"string\" default=\"approximate\">\n    Token counting method. Options: `'approximate'` or `'model'`\n  </ParamField>\n\n  **[`ClearToolUsesEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClearToolUsesEdit) options:**\n\n  <ParamField body=\"trigger\" type=\"number\" default=\"100000\">\n    Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.\n  </ParamField>\n\n  <ParamField body=\"clear_at_least\" type=\"number\" default=\"0\">\n    Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.\n  </ParamField>\n\n  <ParamField body=\"keep\" type=\"number\" default=\"3\">\n    Number of most recent tool results that must be preserved. These will never be cleared.\n  </ParamField>\n\n  <ParamField body=\"clear_tool_inputs\" type=\"boolean\" default=\"False\">\n    Whether to clear the originating tool call parameters on the AI message. When `True`, tool call arguments are replaced with empty objects.\n  </ParamField>\n\n  <ParamField body=\"exclude_tools\" type=\"list[string]\" default=\"()\">\n    List of tool names to exclude from clearing. These tools will never have their outputs cleared.\n  </ParamField>\n\n  <ParamField body=\"placeholder\" type=\"string\" default=\"[cleared]\">\n    Placeholder text inserted for cleared tool outputs. This replaces the original tool message content.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware applies context editing strategies when token limits are reached. The most common strategy is `ClearToolUsesEdit`, which clears older tool results while preserving recent ones.\n\n  **How it works:**\n\n  1. Monitor token count in conversation\n  2. When threshold is reached, clear older tool outputs\n  3. Keep most recent N tool results\n  4. Optionally preserve tool call arguments for context\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, your_calculator_tool, database_tool],\n      middleware=[\n          ContextEditingMiddleware(\n              edits=[\n                  ClearToolUsesEdit(\n                      trigger=2000,\n                      keep=3,\n                      clear_tool_inputs=False,\n                      exclude_tools=[],\n                      placeholder=\"[cleared]\",\n                  ),\n              ],\n          ),\n      ],\n  )\n  ```\n</Accordion>\n\n### Shell tool\n\nExpose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following:\n\n* Agents that need to execute system commands\n* Development and deployment automation tasks\n* Testing and validation workflows\n* File system operations and script execution\n\n<Warning>\n  **Security consideration**: Use appropriate execution policies (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`) to match your deployment's security requirements.\n</Warning>\n\n<Note>\n  **Limitation**: Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future.\n</Note>\n\n**API reference:** [`ShellToolMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ShellToolMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    ShellToolMiddleware,\n    HostExecutionPolicy,\n)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            execution_policy=HostExecutionPolicy(),\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"workspace_root\" type=\"str | Path | None\">\n    Base directory for the shell session. If omitted, a temporary directory is created when the agent starts and removed when it ends.\n  </ParamField>\n\n  <ParamField body=\"startup_commands\" type=\"tuple[str, ...] | list[str] | str | None\">\n    Optional commands executed sequentially after the session starts\n  </ParamField>\n\n  <ParamField body=\"shutdown_commands\" type=\"tuple[str, ...] | list[str] | str | None\">\n    Optional commands executed before the session shuts down\n  </ParamField>\n\n  <ParamField body=\"execution_policy\" type=\"BaseExecutionPolicy | None\">\n    Execution policy controlling timeouts, output limits, and resource configuration. Options:\n\n    * `HostExecutionPolicy` - Full host access (default); best for trusted environments where the agent already runs inside a container or VM\n    * `DockerExecutionPolicy` - Launches a separate Docker container for each agent run, providing harder isolation\n    * `CodexSandboxExecutionPolicy` - Reuses the Codex CLI sandbox for additional syscall/filesystem restrictions\n  </ParamField>\n\n  <ParamField body=\"redaction_rules\" type=\"tuple[RedactionRule, ...] | list[RedactionRule] | None\">\n    Optional redaction rules to sanitize command output before returning it to the model\n  </ParamField>\n\n  <ParamField body=\"tool_description\" type=\"str | None\">\n    Optional override for the registered shell tool description\n  </ParamField>\n\n  <ParamField body=\"shell_command\" type=\"Sequence[str] | str | None\">\n    Optional shell executable (string) or argument sequence used to launch the persistent session. Defaults to `/bin/bash`.\n  </ParamField>\n\n  <ParamField body=\"env\" type=\"Mapping[str, Any] | None\">\n    Optional environment variables to supply to the shell session. Values are coerced to strings before command execution.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware provides a single persistent shell session that agents can use to execute commands sequentially.\n\n  **Execution policies:**\n\n  * `HostExecutionPolicy` (default) - Native execution with full host access\n  * `DockerExecutionPolicy` - Isolated Docker container execution\n  * `CodexSandboxExecutionPolicy` - Sandboxed execution via Codex CLI\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import (\n      ShellToolMiddleware,\n      HostExecutionPolicy,\n      DockerExecutionPolicy,\n      RedactionRule,\n  )\n\n\n  # Basic shell tool with host execution\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool],\n      middleware=[\n          ShellToolMiddleware(\n              workspace_root=\"/workspace\",\n              execution_policy=HostExecutionPolicy(),\n          ),\n      ],\n  )\n\n  # Docker isolation with startup commands\n  agent_docker = create_agent(\n      model=\"gpt-4o\",\n      tools=[],\n      middleware=[\n          ShellToolMiddleware(\n              workspace_root=\"/workspace\",\n              startup_commands=[\"pip install requests\", \"export PYTHONPATH=/workspace\"],\n              execution_policy=DockerExecutionPolicy(\n                  image=\"python:3.11-slim\",\n                  command_timeout=60.0,\n              ),\n          ),\n      ],\n  )\n\n  # With output redaction\n  agent_redacted = create_agent(\n      model=\"gpt-4o\",\n      tools=[],\n      middleware=[\n          ShellToolMiddleware(\n              workspace_root=\"/workspace\",\n              redaction_rules=[\n                  RedactionRule(pii_type=\"api_key\", detector=r\"sk-[a-zA-Z0-9]{32}\"),\n              ],\n          ),\n      ],\n  )\n  ```\n</Accordion>\n\n### File search\n\nProvide Glob and Grep search tools over filesystem files. File search middleware is useful for the following:\n\n* Code exploration and analysis\n* Finding files by name patterns\n* Searching code content with regex\n* Large codebases where file discovery is needed\n\n**API reference:** [`FilesystemFileSearchMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemFileSearchMiddleware)\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import FilesystemFileSearchMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        FilesystemFileSearchMiddleware(\n            root_path=\"/workspace\",\n            use_ripgrep=True,\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"root_path\" type=\"str\" required>\n    Root directory to search. All file operations are relative to this path.\n  </ParamField>\n\n  <ParamField body=\"use_ripgrep\" type=\"bool\" default=\"True\">\n    Whether to use ripgrep for search. Falls back to Python regex if ripgrep is unavailable.\n  </ParamField>\n\n  <ParamField body=\"max_file_size_mb\" type=\"int\" default=\"10\">\n    Maximum file size to search in MB. Files larger than this are skipped.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware adds two search tools to agents:\n\n  **Glob tool** - Fast file pattern matching:\n\n  * Supports patterns like `**/*.py`, `src/**/*.ts`\n  * Returns matching file paths sorted by modification time\n\n  **Grep tool** - Content search with regex:\n\n  * Full regex syntax support\n  * Filter by file patterns with `include` parameter\n  * Three output modes: `files_with_matches`, `content`, `count`\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import FilesystemFileSearchMiddleware\n  from langchain.messages import HumanMessage\n\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[],\n      middleware=[\n          FilesystemFileSearchMiddleware(\n              root_path=\"/workspace\",\n              use_ripgrep=True,\n              max_file_size_mb=10,\n          ),\n      ],\n  )\n\n  # Agent can now use glob_search and grep_search tools\n  result = agent.invoke({\n      \"messages\": [HumanMessage(\"Find all Python files containing 'async def'\")]\n  })\n\n  # The agent will use:\n  # 1. glob_search(pattern=\"**/*.py\") to find Python files\n  # 2. grep_search(pattern=\"async def\", include=\"*.py\") to find async functions\n  ```\n</Accordion>\n\n## Provider-specific middleware\n\nThese middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples.\n\n<Columns cols={2}>\n  <Card title=\"Anthropic\" href=\"/oss/python/integrations/providers/anthropic#middleware\" icon=\"anthropic\" arrow>\n    Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\n  </Card>\n\n  <Card title=\"OpenAI\" href=\"/oss/python/integrations/providers/openai#middleware\" icon=\"openai\" arrow>\n    Content moderation middleware for OpenAI models.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/built-in.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 48445,
    "word_count": 4773
  },
  {
    "title": "Custom middleware",
    "source": "https://docs.langchain.com/oss/python/langchain/middleware/custom",
    "content": "Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\n## Hooks\n\nMiddleware provides two styles of hooks to intercept agent execution:\n\n<CardGroup cols={2}>\n  <Card title=\"Node-style hooks\" icon=\"share-nodes\" href=\"#node-style-hooks\">\n    Run sequentially at specific execution points.\n  </Card>\n\n  <Card title=\"Wrap-style hooks\" icon=\"container-storage\" href=\"#wrap-style-hooks\">\n    Run around each model or tool call.\n  </Card>\n</CardGroup>\n\n### Node-style hooks\n\nRun sequentially at specific execution points. Use for logging, validation, and state updates.\n\n**Available hooks:**\n\n* `before_agent` - Before agent starts (once per invocation)\n* `before_model` - Before each model call\n* `after_model` - After each model response\n* `after_agent` - After agent completes (once per invocation)\n\n**Example:**\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import before_model, after_model, AgentState\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n\n    @before_model(can_jump_to=[\"end\"])\n    def check_message_limit(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        if len(state[\"messages\"]) >= 50:\n            return {\n                \"messages\": [AIMessage(\"Conversation limit reached.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n\n    @after_model\n    def log_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n    class MessageLimitMiddleware(AgentMiddleware):\n        def __init__(self, max_messages: int = 50):\n            super().__init__()\n            self.max_messages = max_messages\n\n        @hook_config(can_jump_to=[\"end\"])\n        def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n            if len(state[\"messages\"]) == self.max_messages:\n                return {\n                    \"messages\": [AIMessage(\"Conversation limit reached.\")],\n                    \"jump_to\": \"end\"\n                }\n            return None\n\n        def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n            print(f\"Model returned: {state['messages'][-1].content}\")\n            return None\n    ```\n  </Tab>\n</Tabs>\n\n### Wrap-style hooks\n\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\n\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\n\n**Available hooks:**\n\n* `wrap_model_call` - Around each model call\n* `wrap_tool_call` - Around each tool call\n\n**Example:**\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n\n    @wrap_model_call\n    def retry_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        for attempt in range(3):\n            try:\n                return handler(request)\n            except Exception as e:\n                if attempt == 2:\n                    raise\n                print(f\"Retry {attempt + 1}/3 after error: {e}\")\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from typing import Callable\n\n    class RetryMiddleware(AgentMiddleware):\n        def __init__(self, max_retries: int = 3):\n            super().__init__()\n            self.max_retries = max_retries\n\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            for attempt in range(self.max_retries):\n                try:\n                    return handler(request)\n                except Exception as e:\n                    if attempt == self.max_retries - 1:\n                        raise\n                    print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n    ```\n  </Tab>\n</Tabs>\n\n## Create middleware\n\nYou can create middleware in two ways:\n\n<CardGroup cols={2}>\n  <Card title=\"Decorator-based middleware\" icon=\"at\" href=\"#decorator-based-middleware\">\n    Quick and simple for single-hook middleware. Use decorators to wrap individual functions.\n  </Card>\n\n  <Card title=\"Class-based middleware\" icon=\"brackets-curly\" href=\"#class-based-middleware\">\n    More powerful for complex middleware with multiple hooks or configuration.\n  </Card>\n</CardGroup>\n\n### Decorator-based middleware\n\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\n\n**Available decorators:**\n\n**Node-style:**\n\n* [`@before_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_agent) - Runs before agent starts (once per invocation)\n* [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) - Runs before each model call\n* [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model) - Runs after each model response\n* [`@after_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_agent) - Runs after agent completes (once per invocation)\n\n**Wrap-style:**\n\n* [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) - Wraps each model call with custom logic\n* [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) - Wraps each tool call with custom logic\n\n**Convenience:**\n\n* [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) - Generates dynamic system prompts\n\n**Example:**\n\n```python  theme={null}\nfrom langchain.agents.middleware import (\n    before_model,\n    wrap_model_call,\n    AgentState,\n    ModelRequest,\n    ModelResponse,\n)\nfrom langchain.agents import create_agent\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\n\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    print(f\"About to call model with {len(state['messages'])} messages\")\n    return None\n\n@wrap_model_call\ndef retry_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    for attempt in range(3):\n        try:\n            return handler(request)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[log_before_model, retry_model],\n    tools=[...],\n)\n```\n\n**When to use decorators:**\n\n* Single hook needed\n* No complex configuration\n* Quick prototyping\n\n### Class-based middleware\n\nMore powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\n\n**Example:**\n\n```python  theme={null}\nfrom langchain.agents.middleware import (\n    AgentMiddleware,\n    AgentState,\n    ModelRequest,\n    ModelResponse,\n)\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\nclass LoggingMiddleware(AgentMiddleware):\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"About to call model with {len(state['messages'])} messages\")\n        return None\n\n    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[LoggingMiddleware()],\n    tools=[...],\n)\n```\n\n**When to use classes:**\n\n* Defining both sync and async implementations for the same hook\n* Multiple hooks needed in a single middleware\n* Complex configuration required (e.g., configurable thresholds, custom models)\n* Reuse across projects with init-time configuration\n\n## Custom state schema\n\nMiddleware can extend the agent's state with custom properties.\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.messages import HumanMessage\n    from langchain.agents.middleware import AgentState, before_model, after_model\n    from typing_extensions import NotRequired\n    from typing import Any\n    from langgraph.runtime import Runtime\n\n\n    class CustomState(AgentState):\n        model_call_count: NotRequired[int]\n        user_id: NotRequired[str]\n\n\n    @before_model(state_schema=CustomState, can_jump_to=[\"end\"])\n    def check_call_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n\n    @after_model(state_schema=CustomState)\n    def increment_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        middleware=[check_call_limit, increment_counter],\n        tools=[],\n    )\n\n    # Invoke with custom state\n    result = agent.invoke({\n        \"messages\": [HumanMessage(\"Hello\")],\n        \"model_call_count\": 0,\n        \"user_id\": \"user-123\",\n    })\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.messages import HumanMessage\n    from langchain.agents.middleware import AgentState, AgentMiddleware\n    from typing_extensions import NotRequired\n    from typing import Any\n\n\n    class CustomState(AgentState):\n        model_call_count: NotRequired[int]\n        user_id: NotRequired[str]\n\n\n    class CallCounterMiddleware(AgentMiddleware[CustomState]):\n        state_schema = CustomState\n\n        def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n            count = state.get(\"model_call_count\", 0)\n            if count > 10:\n                return {\"jump_to\": \"end\"}\n            return None\n\n        def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n            return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        middleware=[CallCounterMiddleware()],\n        tools=[],\n    )\n\n    # Invoke with custom state\n    result = agent.invoke({\n        \"messages\": [HumanMessage(\"Hello\")],\n        \"model_call_count\": 0,\n        \"user_id\": \"user-123\",\n    })\n    ```\n  </Tab>\n</Tabs>\n\n## Execution order\n\nWhen using multiple middleware, understand how they execute:\n\n```python  theme={null}\nagent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[middleware1, middleware2, middleware3],\n    tools=[...],\n)\n```\n\n<Accordion title=\"Execution flow\">\n  **Before hooks run in order:**\n\n  1. `middleware1.before_agent()`\n  2. `middleware2.before_agent()`\n  3. `middleware3.before_agent()`\n\n  **Agent loop starts**\n\n  4. `middleware1.before_model()`\n  5. `middleware2.before_model()`\n  6. `middleware3.before_model()`\n\n  **Wrap hooks nest like function calls:**\n\n  7. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model\n\n  **After hooks run in reverse order:**\n\n  8. `middleware3.after_model()`\n  9. `middleware2.after_model()`\n  10. `middleware1.after_model()`\n\n  **Agent loop ends**\n\n  11. `middleware3.after_agent()`\n  12. `middleware2.after_agent()`\n  13. `middleware1.after_agent()`\n</Accordion>\n\n**Key rules:**\n\n* `before_*` hooks: First to last\n* `after_*` hooks: Last to first (reverse)\n* `wrap_*` hooks: Nested (first middleware wraps all others)\n\n## Agent jumps\n\nTo exit early from middleware, return a dictionary with `jump_to`:\n\n**Available jump targets:**\n\n* `'end'`: Jump to the end of the agent execution (or the first `after_agent` hook)\n* `'tools'`: Jump to the tools node\n* `'model'`: Jump to the model node (or the first `before_model` hook)\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import after_model, hook_config, AgentState\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n\n    @after_model\n    @hook_config(can_jump_to=[\"end\"])\n    def check_for_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        last_message = state[\"messages\"][-1]\n        if \"BLOCKED\" in last_message.content:\n            return {\n                \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, hook_config, AgentState\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n    class BlockedContentMiddleware(AgentMiddleware):\n        @hook_config(can_jump_to=[\"end\"])\n        def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n            last_message = state[\"messages\"][-1]\n            if \"BLOCKED\" in last_message.content:\n                return {\n                    \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n                    \"jump_to\": \"end\"\n                }\n            return None\n    ```\n  </Tab>\n</Tabs>\n\n## Best practices\n\n1. Keep middleware focused - each should do one thing well\n2. Handle errors gracefully - don't let middleware errors crash the agent\n3. **Use appropriate hook types**:\n   * Node-style for sequential logic (logging, validation)\n   * Wrap-style for control flow (retry, fallback, caching)\n4. Clearly document any custom state properties\n5. Unit test middleware independently before integrating\n6. Consider execution order - place critical middleware first in the list\n7. Use built-in middleware when possible\n\n## Examples\n\n### Dynamic model selection\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n\n    complex_model = init_chat_model(\"gpt-4o\")\n    simple_model = init_chat_model(\"gpt-4o-mini\")\n\n    @wrap_model_call\n    def dynamic_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Use different model based on conversation length\n        if len(request.messages) > 10:\n            model = complex_model\n        else:\n            model = simple_model\n        return handler(request.override(model=model))\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n    complex_model = init_chat_model(\"gpt-4o\")\n    simple_model = init_chat_model(\"gpt-4o-mini\")\n\n    class DynamicModelMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            # Use different model based on conversation length\n            if len(request.messages) > 10:\n                model = complex_model\n            else:\n                model = simple_model\n            return handler(request.override(model=model))\n    ```\n  </Tab>\n</Tabs>\n\n### Tool call monitoring\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_tool_call\n    from langchain.tools.tool_node import ToolCallRequest\n    from langchain.messages import ToolMessage\n    from langgraph.types import Command\n    from typing import Callable\n\n\n    @wrap_tool_call\n    def monitor_tool(\n        request: ToolCallRequest,\n        handler: Callable[[ToolCallRequest], ToolMessage | Command],\n    ) -> ToolMessage | Command:\n        print(f\"Executing tool: {request.tool_call['name']}\")\n        print(f\"Arguments: {request.tool_call['args']}\")\n        try:\n            result = handler(request)\n            print(f\"Tool completed successfully\")\n            return result\n        except Exception as e:\n            print(f\"Tool failed: {e}\")\n            raise\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.tools.tool_node import ToolCallRequest\n    from langchain.agents.middleware import AgentMiddleware\n    from langchain.messages import ToolMessage\n    from langgraph.types import Command\n    from typing import Callable\n\n    class ToolMonitoringMiddleware(AgentMiddleware):\n        def wrap_tool_call(\n            self,\n            request: ToolCallRequest,\n            handler: Callable[[ToolCallRequest], ToolMessage | Command],\n        ) -> ToolMessage | Command:\n            print(f\"Executing tool: {request.tool_call['name']}\")\n            print(f\"Arguments: {request.tool_call['args']}\")\n            try:\n                result = handler(request)\n                print(f\"Tool completed successfully\")\n                return result\n            except Exception as e:\n                print(f\"Tool failed: {e}\")\n                raise\n    ```\n  </Tab>\n</Tabs>\n\n### Dynamically selecting tools\n\nSelect relevant tools at runtime to improve performance and accuracy.\n\n**Benefits:**\n\n* **Shorter prompts** - Reduce complexity by exposing only relevant tools\n* **Better accuracy** - Models choose correctly from fewer options\n* **Permission control** - Dynamically filter tools based on user access\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n\n    @wrap_model_call\n    def select_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n        # Select a small, relevant subset of tools based on state/context\n        relevant_tools = select_relevant_tools(request.state, request.runtime)\n        return handler(request.override(tools=relevant_tools))\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=all_tools,  # All available tools need to be registered upfront\n        middleware=[select_tools],\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from typing import Callable\n\n\n    class ToolSelectorMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n            # Select a small, relevant subset of tools based on state/context\n            relevant_tools = select_relevant_tools(request.state, request.runtime)\n            return handler(request.override(tools=relevant_tools))\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=all_tools,  # All available tools need to be registered upfront\n        middleware=[ToolSelectorMiddleware()],\n    )\n    ```\n  </Tab>\n</Tabs>\n\n### Working with system messages\n\nModify system messages in middleware using the `system_message` field on `ModelRequest`. The `system_message` field contains a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object (even if the agent was created with a string `system_prompt`).\n\n**Example: Adding context to system message**\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    @wrap_model_call\n    def add_context(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": \"Additional context.\"}\n        ]\n        new_system_message = SystemMessage(content=new_content)\n        return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    class ContextMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            # Always work with content blocks\n            new_content = list(request.system_message.content_blocks) + [\n                {\"type\": \"text\", \"text\": \"Additional context.\"}\n            ]\n            new_system_message = SystemMessage(content=new_content)\n            return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n</Tabs>\n\n**Example: Working with cache control (Anthropic)**\n\nWhen working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    @wrap_model_call\n    def add_cached_context(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\n                \"type\": \"text\",\n                \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n                # content up until this point is cached\n                \"cache_control\": {\"type\": \"ephemeral\"}\n            }\n        ]\n\n        new_system_message = SystemMessage(content=new_content)\n        return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    class CachedContextMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            # Always work with content blocks\n            new_content = list(request.system_message.content_blocks) + [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}  # This content will be cached\n                }\n            ]\n\n            new_system_message = SystemMessage(content=new_content)\n            return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n</Tabs>\n\n**Notes:**\n\n* `ModelRequest.system_message` is always a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object, even if the agent was created with `system_prompt=\"string\"`\n* Use `SystemMessage.content_blocks` to access content as a list of blocks, regardless of whether the original content was a string or list\n* When modifying system messages, use `content_blocks` and append new blocks to preserve existing structure\n* You can pass [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects directly to `create_agent`'s `system_prompt` parameter for advanced use cases like cache control\n\n## Additional resources\n\n* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/)\n* [Built-in middleware](/oss/python/langchain/middleware/built-in)\n* [Testing agents](/oss/python/langchain/test)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/custom.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 25576,
    "word_count": 2262
  },
  {
    "title": "Overview",
    "source": "https://docs.langchain.com/oss/python/langchain/middleware/overview",
    "content": "Control and customize agent execution at every step\n\nMiddleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\n\n* Tracking agent behavior with logging, analytics, and debugging.\n* Transforming prompts, [tool selection](/oss/python/langchain/middleware/built-in#llm-tool-selector), and output formatting.\n* Adding [retries](/oss/python/langchain/middleware/built-in#tool-retry), [fallbacks](/oss/python/langchain/middleware/built-in#model-fallback), and early termination logic.\n* Applying [rate limits](/oss/python/langchain/middleware/built-in#model-call-limit), guardrails, and [PII detection](/oss/python/langchain/middleware/built-in#pii-detection).\n\nAdd middleware by passing them to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(...),\n        HumanInTheLoopMiddleware(...)\n    ],\n)\n```\n\n## The agent loop\n\nThe core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063\" alt=\"Core agent loop diagram\" style={{height: \"200px\", width: \"auto\", justifyContent: \"center\"}} className=\"rounded-lg block mx-auto\" data-og-width=\"300\" width=\"300\" data-og-height=\"268\" height=\"268\" data-path=\"oss/images/core_agent_loop.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w\" />\n\nMiddleware exposes hooks before and after each of those steps:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware flow diagram\" style={{height: \"300px\", width: \"auto\", justifyContent: \"center\"}} className=\"rounded-lg mx-auto\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n\n## Additional resources\n\n<CardGroup cols={2}>\n  <Card title=\"Built-in middleware\" icon=\"box\" href=\"/oss/python/langchain/middleware/built-in\">\n    Explore built-in middleware for common use cases.\n  </Card>\n\n  <Card title=\"Custom middleware\" icon=\"code\" href=\"/oss/python/langchain/middleware/custom\">\n    Build your own middleware with hooks and decorators.\n  </Card>\n\n  <Card title=\"Middleware API reference\" icon=\"book\" href=\"https://reference.langchain.com/python/langchain/middleware/\">\n    Complete API reference for middleware.\n  </Card>\n\n  <Card title=\"Testing agents\" icon=\"scale-unbalanced\" href=\"/oss/python/langchain/test\">\n    Test your agents with LangSmith.\n  </Card>\n</CardGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 5539,
    "word_count": 281
  },
  {
    "title": "Models",
    "source": "https://docs.langchain.com/oss/python/langchain/models",
    "content": "[LLMs](https://en.wikipedia.org/wiki/Large_language_model) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\n\nIn addition to text generation, many models support:\n\n* <Icon icon=\"hammer\" size={16} /> [Tool calling](#tool-calling) - calling external tools (like databases queries or API calls) and use results in their responses.\n* <Icon icon=\"shapes\" size={16} /> [Structured output](#structured-output) - where the model's response is constrained to follow a defined format.\n* <Icon icon=\"image\" size={16} /> [Multimodality](#multimodal) - process and return data other than text, such as images, audio, and video.\n* <Icon icon=\"brain\" size={16} /> [Reasoning](#reasoning) - models perform multi-step reasoning to arrive at a conclusion.\n\nModels are the reasoning engine of [agents](/oss/python/langchain/agents). They drive the agent's decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\n\nThe quality and capabilities of the model you choose directly impact your agent's baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\n\nLangChain's standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\n\n<Info>\n  For provider-specific integration information and capabilities, see the provider's [chat model page](/oss/python/integrations/chat).\n</Info>\n\n## Basic usage\n\nModels can be utilized in two ways:\n\n1. **With agents** - Models can be dynamically specified when creating an [agent](/oss/python/langchain/agents#model).\n2. **Standalone** - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\n\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\n\n### Initialize a model\n\nThe easiest way to get started with a standalone model in LangChain is to use [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) to initialize one from a [chat model provider](/oss/python/integrations/chat) of your choice (examples below):\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n\n    <Tab title=\"HuggingFace\">\n      👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n      ```shell  theme={null}\n      pip install -U \"langchain[huggingface]\"\n      ```\n\n      <CodeGroup>\n        ```python init_chat_model theme={null}\n        import os\n        from langchain.chat_models import init_chat_model\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        model = init_chat_model(\n            \"microsoft/Phi-3-mini-4k-instruct\",\n            model_provider=\"huggingface\",\n            temperature=0.7,\n            max_tokens=1024,\n        )\n        ```\n\n        ```python Model Class theme={null}\n        import os\n        from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        llm = HuggingFaceEndpoint(\n            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n            temperature=0.7,\n            max_length=1024,\n        )\n        model = ChatHuggingFace(llm=llm)\n        ```\n      </CodeGroup>\n    </Tab>\n  </Tab>\n</Tabs>\n\n```python  theme={null}\nresponse = model.invoke(\"Why do parrots talk?\")\n```\n\nSee [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) for more detail, including information on how to pass model [parameters](#parameters).\n\n### Key methods\n\n<Card title=\"Invoke\" href=\"#invoke\" icon=\"paper-plane\" arrow=\"true\" horizontal>\n  The model takes messages as input and outputs messages after generating a complete response.\n</Card>\n\n<Card title=\"Stream\" href=\"#stream\" icon=\"tower-broadcast\" arrow=\"true\" horizontal>\n  Invoke the model, but stream the output as it is generated in real-time.\n</Card>\n\n<Card title=\"Batch\" href=\"#batch\" icon=\"grip\" arrow=\"true\" horizontal>\n  Send multiple requests to a model in a batch for more efficient processing.\n</Card>\n\n<Info>\n  In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the [integrations page](/oss/python/integrations/providers/overview) for details.\n</Info>\n\n## Parameters\n\nA chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\n\n<ParamField body=\"model\" type=\"string\" required>\n  The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the '{model_provider}:{model}' format, for example, 'openai:o1'.\n</ParamField>\n\n<ParamField body=\"api_key\" type=\"string\">\n  The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip=\"A variable whose value is set outside the program, typically through functionality built into the operating system or microservice.\">environment variable</Tooltip>.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  Controls the randomness of the model's output. A higher number makes responses more creative; lower ones make them more deterministic.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"number\">\n  Limits the total number of <Tooltip tip=\"The basic unit that a model reads and generates. Providers may define them differently, but in general, they can represent a whole or part of word.\">tokens</Tooltip> in the response, effectively controlling how long the output can be.\n</ParamField>\n\n<ParamField body=\"timeout\" type=\"number\">\n  The maximum time (in seconds) to wait for a response from the model before canceling the request.\n</ParamField>\n\n<ParamField body=\"max_retries\" type=\"number\">\n  The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\n</ParamField>\n\nUsing [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), pass these parameters as inline <Tooltip tip=\"Arbitrary keyword arguments\" cta=\"Learn more\" href=\"https://www.w3schools.com/python/python_args_kwargs.asp\">`**kwargs`</Tooltip>:\n\n```python Initialize using model parameters theme={null}\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    # Kwargs passed to the model:\n    temperature=0.7,\n    timeout=30,\n    max_tokens=1000,\n)\n```\n\n<Info>\n  Each chat model integration may have additional params used to control provider-specific functionality.\n\n  For example, [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI) has `use_responses_api` to dictate whether to use the OpenAI Responses or Completions API.\n\n  To find all the parameters supported by a given chat model, head to the [chat model integrations](/oss/python/integrations/chat) page.\n</Info>\n\n***\n\n## Invocation\n\nA chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.\n\n### Invoke\n\nThe most straightforward way to call a model is to use [`invoke()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.invoke) with a single message or a list of messages.\n\n```python Single message theme={null}\nresponse = model.invoke(\"Why do parrots have colorful feathers?\")\nprint(response)\n```\n\nA list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\n\nSee the [messages](/oss/python/langchain/messages) guide for more detail on roles, types, and content.\n\n```python Dictionary format theme={null}\nconversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore créer des applications.\")\n```\n\n```python Message objects theme={null}\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nconversation = [\n    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n    HumanMessage(\"Translate: I love programming.\"),\n    AIMessage(\"J'adore la programmation.\"),\n    HumanMessage(\"Translate: I love building applications.\")\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore créer des applications.\")\n```\n\n<Info>\n  If the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with \"Chat\", e.g., [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI)(/oss/integrations/chat/openai).\n</Info>\n\n### Stream\n\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\n\nCalling [`stream()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.stream) returns an <Tooltip tip=\"An object that progressively provides access to each item of a collection, in order.\">iterator</Tooltip> that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\n\n<CodeGroup>\n  ```python Basic text streaming theme={null}\n  for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n      print(chunk.text, end=\"|\", flush=True)\n  ```\n\n  ```python Stream tool calls, reasoning, and other content theme={null}\n  for chunk in model.stream(\"What color is the sky?\"):\n      for block in chunk.content_blocks:\n          if block[\"type\"] == \"reasoning\" and (reasoning := block.get(\"reasoning\")):\n              print(f\"Reasoning: {reasoning}\")\n          elif block[\"type\"] == \"tool_call_chunk\":\n              print(f\"Tool call chunk: {block}\")\n          elif block[\"type\"] == \"text\":\n              print(block[\"text\"])\n          else:\n              ...\n  ```\n</CodeGroup>\n\nAs opposed to [`invoke()`](#invoke), which returns a single [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) after the model has finished generating its full response, `stream()` returns multiple [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:\n\n```python Construct an AIMessage theme={null}\nfull = None  # None | AIMessageChunk\nfor chunk in model.stream(\"What color is the sky?\"):\n    full = chunk if full is None else full + chunk\n    print(full.text)\n\n# The\n# The sky\n# The sky is\n# The sky is typically\n# The sky is typically blue\n# ...\n\nprint(full.content_blocks)\n# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\n```\n\nThe resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) – for example, it can be aggregated into a message history and passed back to the model as conversational context.\n\n<Warning>\n  Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.\n</Warning>\n\n<Accordion title=\"Advanced streaming topics\">\n  <Accordion title=\"Streaming events\">\n    LangChain chat models can also stream semantic events using `astream_events()`.\n\n    This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.\n\n    ```python  theme={null}\n    async for event in model.astream_events(\"Hello\"):\n\n        if event[\"event\"] == \"on_chat_model_start\":\n            print(f\"Input: {event['data']['input']}\")\n\n        elif event[\"event\"] == \"on_chat_model_stream\":\n            print(f\"Token: {event['data']['chunk'].text}\")\n\n        elif event[\"event\"] == \"on_chat_model_end\":\n            print(f\"Full message: {event['data']['output'].text}\")\n\n        else:\n            pass\n    ```\n\n    ```txt  theme={null}\n    Input: Hello\n    Token: Hi\n    Token:  there\n    Token: !\n    Token:  How\n    Token:  can\n    Token:  I\n    ...\n    Full message: Hi there! How can I help today?\n    ```\n\n    <Tip>\n      See the [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) reference for event types and other details.\n    </Tip>\n  </Accordion>\n\n  <Accordion title=\"&#x22;Auto-streaming&#x22; chat models\">\n    LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.\n\n    In [LangGraph agents](/oss/python/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\n\n    #### How it works\n\n    When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking [`on_llm_new_token`](https://reference.langchain.com/python/langchain_core/callbacks/#langchain_core.callbacks.base.AsyncCallbackHandler.on_llm_new_token) events in LangChain's callback system.\n\n    Callback events allow LangGraph `stream()` and `astream_events()` to surface the chat model's output in real-time.\n  </Accordion>\n</Accordion>\n\n### Batch\n\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\n\n```python Batch theme={null}\nresponses = model.batch([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n])\nfor response in responses:\n    print(response)\n```\n\n<Note>\n  This section describes a chat model method [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch), which parallelizes model calls client-side.\n\n  It is **distinct** from batch APIs supported by inference providers, such as [OpenAI](https://platform.openai.com/docs/guides/batch) or [Anthropic](https://platform.claude.com/docs/en/build-with-claude/batch-processing#message-batches-api).\n</Note>\n\nBy default, [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed):\n\n```python Yield batch responses upon completion theme={null}\nfor response in model.batch_as_completed([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n]):\n    print(response)\n```\n\n<Note>\n  When using [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed), results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\n</Note>\n\n<Tip>\n  When processing a large number of inputs using [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) or [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed), you may want to control the maximum number of parallel calls. This can be done by setting the [`max_concurrency`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig.max_concurrency) attribute in the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) dictionary.\n\n  ```python Batch with max concurrency theme={null}\n  model.batch(\n      list_of_inputs,\n      config={\n          'max_concurrency': 5,  # Limit to 5 parallel calls\n      }\n  )\n  ```\n\n  See the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) reference for a full list of supported attributes.\n</Tip>\n\nFor more details on batching, see the [reference](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch).\n\n***\n\n## Tool calling\n\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n\n1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\n2. A function or <Tooltip tip=\"A method that can suspend execution and resume at a later time\">coroutine</Tooltip> to execute.\n\n<Note>\n  You may hear the term \"function calling\". We use this interchangeably with \"tool calling\".\n</Note>\n\nHere's the basic tool calling flow between a user and a model:\n\n```mermaid  theme={null}\nsequenceDiagram\n    participant U as User\n    participant M as Model\n    participant T as Tools\n\n    U->>M: \"What's the weather in SF and NYC?\"\n    M->>M: Analyze request & decide tools needed\n\n    par Parallel Tool Calls\n        M->>T: get_weather(\"San Francisco\")\n        M->>T: get_weather(\"New York\")\n    end\n\n    par Tool Execution\n        T-->>M: SF weather data\n        T-->>M: NYC weather data\n    end\n\n    M->>M: Process results & generate response\n    M->>U: \"SF: 72°F sunny, NYC: 68°F cloudy\"\n```\n\nTo make tools that you have defined available for use by a model, you must bind them using [`bind_tools`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.bind_tools). In subsequent invocations, the model can choose to call any of the bound tools as needed.\n\nSome model providers offer <Tooltip tip=\"Tools that are executed server-side, such as web search and code interpreters\">built-in tools</Tooltip> that can be enabled via model or invocation parameters (e.g. [`ChatOpenAI`](/oss/python/integrations/chat/openai), [`ChatAnthropic`](/oss/python/integrations/chat/anthropic)). Check the respective [provider reference](/oss/python/integrations/providers/overview) for details.\n\n<Tip>\n  See the [tools guide](/oss/python/langchain/tools) for details and other options for creating tools.\n</Tip>\n\n```python Binding user tools theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    return f\"It's sunny in {location}.\"\n\n\nmodel_with_tools = model.bind_tools([get_weather])  # [!code highlight]\n\nresponse = model_with_tools.invoke(\"What's the weather like in Boston?\")\nfor tool_call in response.tool_calls:\n    # View tool calls made by the model\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n```\n\nWhen binding user-defined tools, the model's response includes a **request** to execute a tool. When using a model separately from an [agent](/oss/python/langchain/agents), it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an [agent](/oss/python/langchain/agents), the agent loop will handle the tool execution loop for you.\n\nBelow, we show some common ways you can use tool calling.\n\n<AccordionGroup>\n  <Accordion title=\"Tool execution loop\" icon=\"arrow-rotate-right\">\n    When a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes [agent](/oss/python/langchain/agents) abstractions that handle this orchestration for you.\n\n    Here's a simple example of how to do this:\n\n    ```python Tool execution loop theme={null}\n    # Bind (potentially multiple) tools to the model\n    model_with_tools = model.bind_tools([get_weather])\n\n    # Step 1: Model generates tool calls\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n    ai_msg = model_with_tools.invoke(messages)\n    messages.append(ai_msg)\n\n    # Step 2: Execute tools and collect results\n    for tool_call in ai_msg.tool_calls:\n        # Execute the tool with the generated arguments\n        tool_result = get_weather.invoke(tool_call)\n        messages.append(tool_result)\n\n    # Step 3: Pass results back to model for final response\n    final_response = model_with_tools.invoke(messages)\n    print(final_response.text)\n    # \"The current weather in Boston is 72°F and sunny.\"\n    ```\n\n    Each [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) returned by the tool includes a `tool_call_id` that matches the original tool call, helping the model correlate results with requests.\n  </Accordion>\n\n  <Accordion title=\"Forcing tool calls\" icon=\"asterisk\">\n    By default, the model has the freedom to choose which bound tool to use based on the user's input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or **any** tool from a given list:\n\n    <CodeGroup>\n      ```python Force use of any tool theme={null}\n      model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n      ```\n\n      ```python Force use of specific tools theme={null}\n      model_with_tools = model.bind_tools([tool_1], tool_choice=\"tool_1\")\n      ```\n    </CodeGroup>\n  </Accordion>\n\n  <Accordion title=\"Parallel tool calls\" icon=\"layer-group\">\n    Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.\n\n    ```python Parallel tool calls theme={null}\n    model_with_tools = model.bind_tools([get_weather])\n\n    response = model_with_tools.invoke(\n        \"What's the weather in Boston and Tokyo?\"\n    )\n\n\n    # The model may generate multiple tool calls\n    print(response.tool_calls)\n    # [\n    #   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n    #   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n    # ]\n\n\n    # Execute all tools (can be done in parallel with async)\n    results = []\n    for tool_call in response.tool_calls:\n        if tool_call['name'] == 'get_weather':\n            result = get_weather.invoke(tool_call)\n        ...\n        results.append(result)\n    ```\n\n    The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.\n\n    <Tip>\n      Most models supporting tool calling enable parallel tool calls by default. Some (including [OpenAI](/oss/python/integrations/chat/openai) and [Anthropic](/oss/python/integrations/chat/anthropic)) allow you to disable this feature. To do this, set `parallel_tool_calls=False`:\n\n      ```python  theme={null}\n      model.bind_tools([get_weather], parallel_tool_calls=False)\n      ```\n    </Tip>\n  </Accordion>\n\n  <Accordion title=\"Streaming tool calls\" icon=\"rss\">\n    When streaming responses, tool calls are progressively built through [`ToolCallChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolCallChunk). This allows you to see tool calls as they're being generated rather than waiting for the complete response.\n\n    ```python Streaming tool calls theme={null}\n    for chunk in model_with_tools.stream(\n        \"What's the weather in Boston and Tokyo?\"\n    ):\n        # Tool call chunks arrive progressively\n        for tool_chunk in chunk.tool_call_chunks:\n            if name := tool_chunk.get(\"name\"):\n                print(f\"Tool: {name}\")\n            if id_ := tool_chunk.get(\"id\"):\n                print(f\"ID: {id_}\")\n            if args := tool_chunk.get(\"args\"):\n                print(f\"Args: {args}\")\n\n    # Output:\n    # Tool: get_weather\n    # ID: call_SvMlU1TVIZugrFLckFE2ceRE\n    # Args: {\"lo\n    # Args: catio\n    # Args: n\": \"B\n    # Args: osto\n    # Args: n\"}\n    # Tool: get_weather\n    # ID: call_QMZdy6qInx13oWKE7KhuhOLR\n    # Args: {\"lo\n    # Args: catio\n    # Args: n\": \"T\n    # Args: okyo\n    # Args: \"}\n    ```\n\n    You can accumulate chunks to build complete tool calls:\n\n    ```python Accumulate tool calls theme={null}\n    gathered = None\n    for chunk in model_with_tools.stream(\"What's the weather in Boston?\"):\n        gathered = chunk if gathered is None else gathered + chunk\n        print(gathered.tool_calls)\n    ```\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Structured output\n\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\n\n<Tabs>\n  <Tab title=\"Pydantic\">\n    [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) provide the richest feature set with field validation, descriptions, and nested structures.\n\n    ```python  theme={null}\n    from pydantic import BaseModel, Field\n\n    class Movie(BaseModel):\n        \"\"\"A movie with details.\"\"\"\n        title: str = Field(..., description=\"The title of the movie\")\n        year: int = Field(..., description=\"The year the movie was released\")\n        director: str = Field(..., description=\"The director of the movie\")\n        rating: float = Field(..., description=\"The movie's rating out of 10\")\n\n    model_with_structure = model.with_structured_output(Movie)\n    response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n    print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\n    ```\n  </Tab>\n\n  <Tab title=\"TypedDict\">\n    `TypedDict` provides a simpler alternative using Python's built-in typing, ideal when you don't need runtime validation.\n\n    ```python  theme={null}\n    from typing_extensions import TypedDict, Annotated\n\n    class MovieDict(TypedDict):\n        \"\"\"A movie with details.\"\"\"\n        title: Annotated[str, ..., \"The title of the movie\"]\n        year: Annotated[int, ..., \"The year the movie was released\"]\n        director: Annotated[str, ..., \"The director of the movie\"]\n        rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n\n    model_with_structure = model.with_structured_output(MovieDict)\n    response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n    print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}\n    ```\n  </Tab>\n\n  <Tab title=\"JSON Schema\">\n    For maximum control or interoperability, you can provide a raw JSON Schema.\n\n    ```python  theme={null}\n    import json\n\n    json_schema = {\n        \"title\": \"Movie\",\n        \"description\": \"A movie with details\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"title\": {\n                \"type\": \"string\",\n                \"description\": \"The title of the movie\"\n            },\n            \"year\": {\n                \"type\": \"integer\",\n                \"description\": \"The year the movie was released\"\n            },\n            \"director\": {\n                \"type\": \"string\",\n                \"description\": \"The director of the movie\"\n            },\n            \"rating\": {\n                \"type\": \"number\",\n                \"description\": \"The movie's rating out of 10\"\n            }\n        },\n        \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n    }\n\n    model_with_structure = model.with_structured_output(\n        json_schema,\n        method=\"json_schema\",\n    )\n    response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n    print(response)  # {'title': 'Inception', 'year': 2010, ...}\n    ```\n  </Tab>\n</Tabs>\n\n<Note>\n  **Key considerations for structured output:**\n\n  * **Method parameter**: Some providers support different methods (`'json_schema'`, `'function_calling'`, `'json_mode'`)\n    * `'json_schema'` typically refers to dedicated structured output features offered by a provider\n    * `'function_calling'` derives structured output by forcing a [tool call](#tool-calling) following the given schema\n    * `'json_mode'` is a precursor to `'json_schema'` offered by some providers - it generates valid json, but the schema must be described in the prompt\n  * **Include raw**: Use `include_raw=True` to get both the parsed output and the raw AI message\n  * **Validation**: Pydantic models provide automatic validation, while `TypedDict` and JSON Schema require manual validation\n</Note>\n\n<Accordion title=\"Example: Message output alongside parsed structure\">\n  It can be useful to return the raw [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) object alongside the parsed representation to access response metadata such as [token counts](#token-usage). To do this, set [`include_raw=True`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output\\(include_raw\\)) when calling [`with_structured_output`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output):\n\n  ```python  theme={null}\n  from pydantic import BaseModel, Field\n\n  class Movie(BaseModel):\n      \"\"\"A movie with details.\"\"\"\n      title: str = Field(..., description=\"The title of the movie\")\n      year: int = Field(..., description=\"The year the movie was released\")\n      director: str = Field(..., description=\"The director of the movie\")\n      rating: float = Field(..., description=\"The movie's rating out of 10\")\n\n  model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]\n  response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n  response\n  # {\n  #     \"raw\": AIMessage(...),\n  #     \"parsed\": Movie(title=..., year=..., ...),\n  #     \"parsing_error\": None,\n  # }\n  ```\n</Accordion>\n\n<Accordion title=\"Example: Nested structures\">\n  Schemas can be nested:\n\n  <CodeGroup>\n    ```python Pydantic BaseModel theme={null}\n    from pydantic import BaseModel, Field\n\n    class Actor(BaseModel):\n        name: str\n        role: str\n\n    class MovieDetails(BaseModel):\n        title: str\n        year: int\n        cast: list[Actor]\n        genres: list[str]\n        budget: float | None = Field(None, description=\"Budget in millions USD\")\n\n    model_with_structure = model.with_structured_output(MovieDetails)\n    ```\n\n    ```python TypedDict theme={null}\n    from typing_extensions import Annotated, TypedDict\n\n    class Actor(TypedDict):\n        name: str\n        role: str\n\n    class MovieDetails(TypedDict):\n        title: str\n        year: int\n        cast: list[Actor]\n        genres: list[str]\n        budget: Annotated[float | None, ..., \"Budget in millions USD\"]\n\n    model_with_structure = model.with_structured_output(MovieDetails)\n    ```\n  </CodeGroup>\n</Accordion>\n\n***\n\n## Supported models\n\nLangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the [integrations page](/oss/python/integrations/providers/overview).\n\n***\n\n## Advanced topics\n\n### Model profiles\n\n<Warning> This is a beta feature. The format of model profiles is subject to change. </Warning>\n\n<Info> Model profiles require `langchain>=1.1`. </Info>\n\nLangChain chat models can expose a dictionary of supported features and capabilities through a `.profile` attribute:\n\n```python  theme={null}\nmodel.profile\n# {\n#   \"max_input_tokens\": 400000,\n#   \"image_inputs\": True,\n#   \"reasoning_output\": True,\n#   \"tool_calling\": True,\n#   ...\n# }\n```\n\nRefer to the full set of fields in the [API reference](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.profile).\n\nMuch of the model profile data is powered by the [models.dev](https://github.com/sst/models.dev) project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.\n\nModel profile data allow applications to work around model capabilities dynamically. For example:\n\n1. [Summarization middleware](/oss/python/langchain/middleware/built-in#summarization) can trigger summarization based on a model's context window size.\n2. [Structured output](/oss/python/langchain/structured-output) strategies in `create_agent` can be inferred automatically (e.g., by checking support for native structured output features).\n3. Model inputs can be gated based on supported [modalities](#multimodal) and maximum input tokens.\n\n<Accordion title=\"Updating or overwriting profile data\">\n  Model profile data can be changed if it is missing, stale, or incorrect.\n\n  **Option 1 (quick fix)**\n\n  You can instantiate a chat model with any valid profile:\n\n  ```python  theme={null}\n  custom_profile = {\n      \"max_input_tokens\": 100_000,\n      \"tool_calling\": True,\n      \"structured_output\": True,\n      # ...\n  }\n  model = init_chat_model(\"...\", profile=custom_profile)\n  ```\n\n  The `profile` is also a regular `dict` and can be updated in place. If the model instance is shared, consider using `model_copy` to avoid mutating shared state.\n\n  ```python  theme={null}\n  new_profile = model.profile | {\"key\": \"value\"}\n  model.model_copy(update={\"profile\": new_profile})\n  ```\n\n  **Option 2 (fix data upstream)**\n\n  The primary source for the data is the [models.dev](https://models.dev/) project. This data is merged with additional fields and overrides in LangChain [integration packages](/oss/python/integrations/providers/overview) and are shipped with those packages.\n\n  Model profile data can be updated through the following process:\n\n  1. (If needed) update the source data at [models.dev](https://models.dev/) through a pull request to its [repository on Github](https://github.com/sst/models.dev).\n  2. (If needed) update additional fields and overrides in `langchain_<package>/data/profile_augmentations.toml` through a pull request to the LangChain [integration package](/oss/python/integrations/providers/overview)\\`.\n  3. Use the [`langchain-model-profiles`](https://pypi.org/project/langchain-model-profiles/) CLI tool to pull the latest data from [models.dev](https://models.dev/), merge in the augmentations and update the profile data:\n\n  ```bash  theme={null}\n  pip install langchain-model-profiles\n  ```\n\n  ```bash  theme={null}\n  langchain-profiles refresh --provider <provider> --data-dir <data_dir>\n  ```\n\n  This command:\n\n  * Downloads the latest data for `<provider>` from models.dev\n  * Merges augmentations from `profile_augmentations.toml` in `<data_dir>`\n  * Writes merged profiles to `profiles.py` in `<data_dir>`\n\n  For example: from [`libs/partners/anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic) in the [LangChain monorepo](https://github.com/langchain-ai/langchain):\n\n  ```bash  theme={null}\n  uv run --with langchain-model-profiles --provider anthropic --data-dir langchain_anthropic/data\n  ```\n</Accordion>\n\n### Multimodal\n\nCertain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing [content blocks](/oss/python/langchain/messages#message-content).\n\n<Tip>\n  All LangChain chat models with underlying multimodal capabilities support:\n\n  1. Data in the cross-provider standard format (see [our messages guide](/oss/python/langchain/messages))\n  2. OpenAI [chat completions](https://platform.openai.com/docs/api-reference/chat) format\n  3. Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)\n</Tip>\n\nSee the [multimodal section](/oss/python/langchain/messages#multimodal) of the messages guide for details.\n\n<Tooltip tip=\"Not all LLMs are made equally!\" cta=\"See reference\" href=\"https://models.dev/\">Some models</Tooltip> can return multimodal data as part of their response. If invoked to do so, the resulting [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) will have content blocks with multimodal types.\n\n```python Multimodal output theme={null}\nresponse = model.invoke(\"Create a picture of a cat\")\nprint(response.content_blocks)\n# [\n#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n# ]\n```\n\nSee the [integrations page](/oss/python/integrations/providers/overview) for details on specific providers.\n\n### Reasoning\n\nMany models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.\n\n**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.\n\n<CodeGroup>\n  ```python Stream reasoning output theme={null}\n  for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n      reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\n      print(reasoning_steps if reasoning_steps else chunk.text)\n  ```\n\n  ```python Complete reasoning output theme={null}\n  response = model.invoke(\"Why do parrots have colorful feathers?\")\n  reasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\n  print(\" \".join(step[\"reasoning\"] for step in reasoning_steps))\n  ```\n</CodeGroup>\n\nDepending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical \"tiers\" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.\n\nFor details, see the [integrations page](/oss/python/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model.\n\n### Local models\n\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\n\n[Ollama](/oss/python/integrations/chat/ollama) is one of the easiest ways to run models locally. See the full list of local integrations on the [integrations page](/oss/python/integrations/providers/overview).\n\n### Prompt caching\n\nMany providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:\n\n* **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/python/integrations/chat/openai) and [Gemini](/oss/python/integrations/chat/google_generative_ai).\n* **Explicit caching:** providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:\n  * [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI) (via `prompt_cache_key`)\n  * Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/python/integrations/chat/anthropic#prompt-caching)\n  * [Gemini](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html).\n  * [AWS Bedrock](/oss/python/integrations/chat/bedrock#prompt-caching)\n\n<Warning>\n  Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/python/integrations/chat) for details.\n</Warning>\n\nCache usage will be reflected in the [usage metadata](/oss/python/langchain/messages#token-usage) of the model response.\n\n### Server-side tool use\n\nSome providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\n\nIf a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the [content blocks](/oss/python/langchain/messages#standard-content-blocks) of the response will return the server-side tool calls and results in a provider-agnostic format:\n\n```python Invoke with server-side tool use theme={null}\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-4.1-mini\")\n\ntool = {\"type\": \"web_search\"}\nmodel_with_tools = model.bind_tools([tool])\n\nresponse = model_with_tools.invoke(\"What was a positive news story from today?\")\nresponse.content_blocks\n```\n\n```python Result expandable theme={null}\n[\n    {\n        \"type\": \"server_tool_call\",\n        \"name\": \"web_search\",\n        \"args\": {\n            \"query\": \"positive news stories today\",\n            \"type\": \"search\"\n        },\n        \"id\": \"ws_abc123\"\n    },\n    {\n        \"type\": \"server_tool_result\",\n        \"tool_call_id\": \"ws_abc123\",\n        \"status\": \"success\"\n    },\n    {\n        \"type\": \"text\",\n        \"text\": \"Here are some positive news stories from today...\",\n        \"annotations\": [\n            {\n                \"end_index\": 410,\n                \"start_index\": 337,\n                \"title\": \"article title\",\n                \"type\": \"citation\",\n                \"url\": \"...\"\n            }\n        ]\n    }\n]\n```\n\nThis represents a single conversational turn; there are no associated [ToolMessage](/oss/python/langchain/messages#tool-message) objects that need to be passed in as in client-side [tool-calling](#tool-calling).\n\nSee the [integration page](/oss/python/integrations/chat) for your given provider for available tools and usage details.\n\n### Rate limiting\n\nMany chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\nTo help manage rate limits, chat model integrations accept a `rate_limiter` parameter that can be provided during initialization to control the rate at which requests are made.\n\n<Accordion title=\"Initialize and use a rate limiter\" icon=\"gauge-high\">\n  LangChain in comes with (an optional) built-in [`InMemoryRateLimiter`](https://reference.langchain.com/python/langchain_core/rate_limiters/#langchain_core.rate_limiters.InMemoryRateLimiter). This limiter is thread safe and can be shared by multiple threads in the same process.\n\n  ```python Define a rate limiter theme={null}\n  from langchain_core.rate_limiters import InMemoryRateLimiter\n\n  rate_limiter = InMemoryRateLimiter(\n      requests_per_second=0.1,  # 1 request every 10s\n      check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n      max_bucket_size=10,  # Controls the maximum burst size.\n  )\n\n  model = init_chat_model(\n      model=\"gpt-5\",\n      model_provider=\"openai\",\n      rate_limiter=rate_limiter  # [!code highlight]\n  )\n  ```\n\n  <Warning>\n    The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\n  </Warning>\n</Accordion>\n\n### Base URL or proxy\n\nFor many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.\n\n<Accordion title=\"Base URL\" icon=\"link\">\n  Many model providers offer OpenAI-compatible APIs (e.g., [Together AI](https://www.together.ai/), [vLLM](https://github.com/vllm-project/vllm)). You can use [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) with these providers by specifying the appropriate `base_url` parameter:\n\n  ```python  theme={null}\n  model = init_chat_model(\n      model=\"MODEL_NAME\",\n      model_provider=\"openai\",\n      base_url=\"BASE_URL\",\n      api_key=\"YOUR_API_KEY\",\n  )\n  ```\n\n  <Note>\n    When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective [reference](/oss/python/integrations/providers/overview) for details.\n  </Note>\n</Accordion>\n\n<Accordion title=\"Proxy configuration\" icon=\"shield\">\n  For deployments requiring HTTP proxies, some model integrations support proxy configuration:\n\n  ```python  theme={null}\n  from langchain_openai import ChatOpenAI\n\n  model = ChatOpenAI(\n      model=\"gpt-4o\",\n      openai_proxy=\"http://proxy.example.com:8080\"\n  )\n  ```\n\n  <Note>\n    Proxy support varies by integration. Check the specific model provider's [reference](/oss/python/integrations/providers/overview) for proxy configuration options.\n  </Note>\n</Accordion>\n\n### Log probabilities\n\nCertain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the `logprobs` parameter when initializing the model:\n\n```python  theme={null}\nmodel = init_chat_model(\n    model=\"gpt-4o\",\n    model_provider=\"openai\"\n).bind(logprobs=True)\n\nresponse = model.invoke(\"Why do parrots talk?\")\nprint(response.response_metadata[\"logprobs\"])\n```\n\n### Token usage\n\nA number of model providers return token usage information as part of the invocation response. When available, this information will be included on the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects produced by the corresponding model. For more details, see the [messages](/oss/python/langchain/messages) guide.\n\n<Note>\n  Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the [streaming usage metadata](/oss/python/integrations/chat/openai#streaming-usage-metadata) section of the integration guide for details.\n</Note>\n\nYou can track aggregate token counts across models in an application using either a callback or context manager, as shown below:\n\n<Tabs>\n  <Tab title=\"Callback handler\">\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n    from langchain_core.callbacks import UsageMetadataCallbackHandler\n\n    model_1 = init_chat_model(model=\"gpt-4o-mini\")\n    model_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n    callback = UsageMetadataCallbackHandler()\n    result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n    result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n    callback.usage_metadata\n    ```\n\n    ```python  theme={null}\n    {\n        'gpt-4o-mini-2024-07-18': {\n            'input_tokens': 8,\n            'output_tokens': 10,\n            'total_tokens': 18,\n            'input_token_details': {'audio': 0, 'cache_read': 0},\n            'output_token_details': {'audio': 0, 'reasoning': 0}\n        },\n        'claude-haiku-4-5-20251001': {\n            'input_tokens': 8,\n            'output_tokens': 21,\n            'total_tokens': 29,\n            'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n        }\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"Context manager\">\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n    from langchain_core.callbacks import get_usage_metadata_callback\n\n    model_1 = init_chat_model(model=\"gpt-4o-mini\")\n    model_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n    with get_usage_metadata_callback() as cb:\n        model_1.invoke(\"Hello\")\n        model_2.invoke(\"Hello\")\n        print(cb.usage_metadata)\n    ```\n\n    ```python  theme={null}\n    {\n        'gpt-4o-mini-2024-07-18': {\n            'input_tokens': 8,\n            'output_tokens': 10,\n            'total_tokens': 18,\n            'input_token_details': {'audio': 0, 'cache_read': 0},\n            'output_token_details': {'audio': 0, 'reasoning': 0}\n        },\n        'claude-haiku-4-5-20251001': {\n            'input_tokens': 8,\n            'output_tokens': 21,\n            'total_tokens': 29,\n            'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n        }\n    }\n    ```\n  </Tab>\n</Tabs>\n\n### Invocation config\n\nWhen invoking a model, you can pass additional configuration through the `config` parameter using a [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.\n\nCommon configuration options include:\n\n```python Invocation with config theme={null}\nresponse = model.invoke(\n    \"Tell me a joke\",\n    config={\n        \"run_name\": \"joke_generation\",      # Custom name for this run\n        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n        \"callbacks\": [my_callback_handler], # Callback handlers\n    }\n)\n```\n\nThese configuration values are particularly useful when:\n\n* Debugging with [LangSmith](https://docs.smith.langchain.com/) tracing\n* Implementing custom logging or monitoring\n* Controlling resource usage in production\n* Tracking invocations across complex pipelines\n\n<Accordion title=\"Key configuration attributes\">\n  <ParamField body=\"run_name\" type=\"string\">\n    Identifies this specific invocation in logs and traces. Not inherited by sub-calls.\n  </ParamField>\n\n  <ParamField body=\"tags\" type=\"string[]\">\n    Labels inherited by all sub-calls for filtering and organization in debugging tools.\n  </ParamField>\n\n  <ParamField body=\"metadata\" type=\"object\">\n    Custom key-value pairs for tracking additional context, inherited by all sub-calls.\n  </ParamField>\n\n  <ParamField body=\"max_concurrency\" type=\"number\">\n    Controls the maximum number of parallel calls when using [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) or [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed).\n  </ParamField>\n\n  <ParamField body=\"callbacks\" type=\"array\">\n    Handlers for monitoring and responding to events during execution.\n  </ParamField>\n\n  <ParamField body=\"recursion_limit\" type=\"number\">\n    Maximum recursion depth for chains to prevent infinite loops in complex pipelines.\n  </ParamField>\n</Accordion>\n\n<Tip>\n  See full [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) reference for all supported attributes.\n</Tip>\n\n### Configurable models\n\nYou can also create a runtime-configurable model by specifying [`configurable_fields`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.configurable_fields). If you don't specify a model value, then `'model'` and `'model_provider'` will be configurable by default.\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\n\nconfigurable_model = init_chat_model(temperature=0)\n\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\n)\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},  # Run with Claude\n)\n```\n\n<Accordion title=\"Configurable model with default values\">\n  We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:\n\n  ```python  theme={null}\n  first_model = init_chat_model(\n          model=\"gpt-4.1-mini\",\n          temperature=0,\n          configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n          config_prefix=\"first\",  # Useful when you have a chain with multiple models\n  )\n\n  first_model.invoke(\"what's your name\")\n  ```\n\n  ```python  theme={null}\n  first_model.invoke(\n      \"what's your name\",\n      config={\n          \"configurable\": {\n              \"first_model\": \"claude-sonnet-4-5-20250929\",\n              \"first_temperature\": 0.5,\n              \"first_max_tokens\": 100,\n          }\n      },\n  )\n  ```\n\n  See the [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) reference for more details on `configurable_fields` and `config_prefix`.\n</Accordion>\n\n<Accordion title=\"Using a configurable model declaratively\">\n  We can call declarative operations like `bind_tools`, `with_structured_output`, `with_configurable`, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.\n\n  ```python  theme={null}\n  from pydantic import BaseModel, Field\n\n\n  class GetWeather(BaseModel):\n      \"\"\"Get the current weather in a given location\"\"\"\n\n          location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n  class GetPopulation(BaseModel):\n      \"\"\"Get the current population in a given location\"\"\"\n\n          location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n  model = init_chat_model(temperature=0)\n  model_with_tools = model.bind_tools([GetWeather, GetPopulation])\n\n  model_with_tools.invoke(\n      \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}}\n  ).tool_calls\n  ```\n\n  ```\n  [\n      {\n          'name': 'GetPopulation',\n          'args': {'location': 'Los Angeles, CA'},\n          'id': 'call_Ga9m8FAArIyEjItHmztPYA22',\n          'type': 'tool_call'\n      },\n      {\n          'name': 'GetPopulation',\n          'args': {'location': 'New York, NY'},\n          'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\n          'type': 'tool_call'\n      }\n  ]\n  ```\n\n  ```python  theme={null}\n  model_with_tools.invoke(\n      \"what's bigger in 2024 LA or NYC\",\n      config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},\n  ).tool_calls\n  ```\n\n  ```\n  [\n      {\n          'name': 'GetPopulation',\n          'args': {'location': 'Los Angeles, CA'},\n          'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',\n          'type': 'tool_call'\n      },\n      {\n          'name': 'GetPopulation',\n          'args': {'location': 'New York City, NY'},\n          'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',\n          'type': 'tool_call'\n      }\n  ]\n  ```\n</Accordion>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/models.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 60849,
    "word_count": 6214
  },
  {
    "title": "Multi-agent",
    "source": "https://docs.langchain.com/oss/python/langchain/multi-agent",
    "content": "**Multi-agent systems** break a complex application into multiple specialized agents that work together to solve problems.\nInstead of relying on a single agent to handle every step, **multi-agent architectures** allow you to compose smaller, focused agents into a coordinated workflow.\n\nMulti-agent systems are useful when:\n\n* A single agent has too many tools and makes poor decisions about which to use.\n* Context or memory grows too large for one agent to track effectively.\n* Tasks require **specialization** (e.g., a planner, researcher, math expert).\n\n## Multi-agent patterns\n\n| Pattern                           | How it works                                                                                                                                                     | Control flow                                               | Example use case                                 |\n| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------ |\n| [**Tool Calling**](#tool-calling) | A **supervisor** agent calls other agents as *tools*. The “tool” agents don’t talk to the user directly — they just run their task and return results.           | Centralized: all routing passes through the calling agent. | Task orchestration, structured workflows.        |\n| [**Handoffs**](#handoffs)         | The current agent decides to **transfer control** to another agent. The active agent changes, and the user may continue interacting directly with the new agent. | Decentralized: agents can change who is active.            | Multi-domain conversations, specialist takeover. |\n\n<Card title=\"Tutorial: Build a supervisor agent\" icon=\"sitemap\" href=\"/oss/python/langchain/supervisor\" arrow cta=\"Learn more\">\n  Learn how to build a personal assistant using the supervisor pattern, where a central supervisor agent coordinates specialized worker agents.\n  This tutorial demonstrates:\n\n  * Creating specialized sub-agents for different domains (calendar and email)\n  * Wrapping sub-agents as tools for centralized orchestration\n  * Adding human-in-the-loop review for sensitive actions\n</Card>\n\n## Choosing a pattern\n\n| Question                                              | Tool Calling | Handoffs |\n| ----------------------------------------------------- | ------------ | -------- |\n| Need centralized control over workflow?               | ✅ Yes        | ❌ No     |\n| Want agents to interact directly with the user?       | ❌ No         | ✅ Yes    |\n| Complex, human-like conversation between specialists? | ❌ Limited    | ✅ Strong |\n\n<Tip>\n  You can mix both patterns — use **handoffs** for agent switching, and have each agent **call subagents as tools** for specialized tasks.\n</Tip>\n\n## Customizing agent context\n\nAt the heart of multi-agent design is **context engineering** - deciding what information each agent sees. LangChain gives you fine-grained control over:\n\n* Which parts of the conversation or state are passed to each agent.\n* Specialized prompts tailored to subagents.\n* Inclusion/exclusion of intermediate reasoning.\n* Customizing input/output formats per agent.\n\nThe quality of your system **heavily depends** on context engineering. The goal is to ensure that each agent has access to the correct data it needs to perform its task, whether it’s acting as a tool or as an active agent.\n\n## Tool calling\n\nIn **tool calling**, one agent (the “**controller**”) treats other agents as *tools* to be invoked when needed. The controller manages orchestration, while tool agents perform specific tasks and return results.\n\nFlow:\n\n1. The **controller** receives input and decides which tool (subagent) to call.\n2. The **tool agent** runs its task based on the controller’s instructions.\n3. The **tool agent** returns results to the controller.\n4. The **controller** decides the next step or finishes.\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Controller Agent]\n    B --> C[Tool Agent 1]\n    B --> D[Tool Agent 2]\n    C --> B\n    D --> B\n    B --> E[User Response]\n```\n\n<Tip>\n  Agents used as tools are generally **not expected** to continue conversation with the user.\n  Their role is to perform a task and return results to the controller agent.\n  If you need subagents to be able to converse with the user, use **handoffs** instead.\n</Tip>\n\n### Implementation\n\nBelow is a minimal example where the main agent is given access to a single subagent via a tool definition:\n\n```python  theme={null}\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\nsubagent1 = create_agent(model=\"...\", tools=[...])\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(query: str):\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return result[\"messages\"][-1].content\n\nagent = create_agent(model=\"...\", tools=[call_subagent1])\n```\n\nIn this pattern:\n\n1. The main agent invokes `call_subagent1` when it decides the task matches the subagent’s description.\n2. The subagent runs independently and returns its result.\n3. The main agent receives the result and continues orchestration.\n\n### Where to customize\n\nThere are several points where you can control how context is passed between the main agent and its subagents:\n\n1. **Subagent name** (`\"subagent1_name\"`): This is how the main agent refers to the subagent. Since it influences prompting, choose it carefully.\n2. **Subagent description** (`\"subagent1_description\"`): This is what the main agent “knows” about the subagent. It directly shapes how the main agent decides when to call it.\n3. **Input to the subagent**: You can customize this input to better shape how the subagent interprets tasks. In the example above, we pass the agent-generated `query` directly.\n4. **Output from the subagent**: This is the **response** passed back to the main agent. You can adjust what is returned to control how the main agent interprets results. In the example above, we return the final message text, but you could return additional state or metadata.\n\n### Control the input to the subagent\n\nThere are two main levers to control the input that the main agent passes to a subagent:\n\n* **Modify the prompt** – Adjust the main agent's prompt or the tool metadata (i.e., sub-agent's name and description) to better guide when and how it calls the subagent.\n* **Context injection** – Add input that isn’t practical to capture in a static prompt (e.g., full message history, prior results, task metadata) by adjusting the tool call to pull from the agent’s state.\n\n```python  theme={null}\nfrom langchain.agents import AgentState\nfrom langchain.tools import tool, ToolRuntime\n\nclass CustomState(AgentState):\n    example_state_key: str\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(query: str, runtime: ToolRuntime[None, CustomState]):\n    # Apply any logic needed to transform the messages into a suitable input\n    subagent_input = some_logic(query, runtime.state[\"messages\"])\n    result = subagent1.invoke({\n        \"messages\": subagent_input,\n        # You could also pass other state keys here as needed.\n        # Make sure to define these in both the main and subagent's\n        # state schemas.\n        \"example_state_key\": runtime.state[\"example_state_key\"]\n    })\n    return result[\"messages\"][-1].content\n```\n\n### Control the output from the subagent\n\nTwo common strategies for shaping what the main agent receives back from a subagent:\n\n* **Modify the prompt** – Refine the subagent’s prompt to specify exactly what should be returned.\n  * Useful when outputs are incomplete, too verbose, or missing key details.\n  * A common failure mode is that the subagent performs tool calls or reasoning but does **not include the results** in its final message. Remind it that the controller (and user) only see the final output, so all relevant info must be included there.\n* **Custom output formatting** – adjust or enrich the subagent's response in code before handing it back to the main agent.\n  * Example: pass specific state keys back to the main agent in addition to the final text.\n  * This requires wrapping the result in a [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) (or equivalent structure) so you can merge custom state with the subagent’s response.\n\n```python  theme={null}\nfrom typing import Annotated\nfrom langchain.agents import AgentState\nfrom langchain.tools import InjectedToolCallId\nfrom langgraph.types import Command\n\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\n# We need to pass the `tool_call_id` to the sub agent so it can use it to respond with the tool call result\ndef call_subagent1(\n    query: str,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n# You need to return a `Command` object to include more than just a final tool call\n) -> Command:\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return Command(update={\n        # This is the example state key we are passing back\n        \"example_state_key\": result[\"example_state_key\"],\n        \"messages\": [\n            ToolMessage(\n                content=result[\"messages\"][-1].content,\n                # We need to include the tool call id so it matches up with the right tool call\n                tool_call_id=tool_call_id\n            )\n        ]\n    })\n```\n\n## Handoffs\n\nIn **handoffs**, agents can directly pass control to each other. The “active” agent changes, and the user interacts with whichever agent currently has control.\n\nFlow:\n\n1. The **current agent** decides it needs help from another agent.\n2. It passes control (and state) to the **next agent**.\n3. The **new agent** interacts directly with the user until it decides to hand off again or finish.\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Agent A]\n    B --> C[Agent B]\n    C --> A\n```\n\n### Implementation (Coming soon)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 10532,
    "word_count": 1389
  },
  {
    "title": "LangSmith Observability",
    "source": "https://docs.langchain.com/oss/python/langchain/observability",
    "content": "As you build and run agents with LangChain, you need visibility into how they behave: which [tools](/oss/python/langchain/tools) they call, what prompts they generate, and how they make decisions. LangChain agents built with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) automatically support tracing through [LangSmith](/langsmith/home), a platform for capturing, debugging, evaluating, and monitoring LLM application behavior.\n\n[*Traces*](/langsmith/observability-concepts#traces) record every step of your agent's execution, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This execution data helps you debug issues, evaluate performance across different inputs, and monitor usage patterns in production.\n\nThis guide shows you how to enable tracing for your LangChain agents and use LangSmith to analyze their execution.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n* **A LangSmith account**: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com).\n* **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.\n\n## Enable tracing\n\nAll LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables:\n\n```bash  theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n```\n\n## Quickstart\n\nNo extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\n\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n    # ... email sending logic\n    return f\"Email sent to {to}\"\n\ndef search_web(query: str):\n    \"\"\"Search the web for information.\"\"\"\n    # ... web search logic\n    return f\"Search results for: {query}\"\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[send_email, search_web],\n    system_prompt=\"You are a helpful assistant that can send emails and search the web.\"\n)\n\n# Run the agent - all steps will be traced automatically\nresponse = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for the latest AI news and email a summary to john@example.com\"}]\n})\n```\n\nBy default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).\n\n## Trace selectively\n\nYou may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:\n\n```python  theme={null}\nimport langsmith as ls\n\n# This WILL be traced\nwith ls.tracing_context(enabled=True):\n    agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send a test email to alice@example.com\"}]})\n\n# This will NOT be traced (if LANGSMITH_TRACING is not set)\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send another email\"}]})\n```\n\n## Log to a project\n\n<Accordion title=\"Statically\">\n  You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:\n\n  ```bash  theme={null}\n  export LANGSMITH_PROJECT=my-agent-project\n  ```\n</Accordion>\n\n<Accordion title=\"Dynamically\">\n  You can set the project name programmatically for specific operations:\n\n  ```python  theme={null}\n  import langsmith as ls\n\n  with ls.tracing_context(project_name=\"email-agent-test\", enabled=True):\n      response = agent.invoke({\n          \"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]\n      })\n  ```\n</Accordion>\n\n## Add metadata to traces\n\nYou can annotate your traces with custom metadata and tags:\n\n```python  theme={null}\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]},\n    config={\n        \"tags\": [\"production\", \"email-assistant\", \"v1.0\"],\n        \"metadata\": {\n            \"user_id\": \"user_123\",\n            \"session_id\": \"session_456\",\n            \"environment\": \"production\"\n        }\n    }\n)\n```\n\n`tracing_context` also accepts tags and metadata for fine-grained control:\n\n```python  theme={null}\nwith ls.tracing_context(\n    project_name=\"email-agent-test\",\n    enabled=True,\n    tags=[\"production\", \"email-assistant\", \"v1.0\"],\n    metadata={\"user_id\": \"user_123\", \"session_id\": \"session_456\", \"environment\": \"production\"}):\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}\n    )\n```\n\nThis custom metadata and tags will be attached to the trace in LangSmith.\n\n<Tip>\n  To learn more about how to use traces to debug, evaluate, and monitor your agents, see the [LangSmith documentation](/langsmith/home).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/observability.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 5111,
    "word_count": 592
  },
  {
    "title": "LangChain overview",
    "source": "https://docs.langchain.com/oss/python/langchain/overview",
    "content": "<Callout icon=\"bullhorn\" color=\"#DFC5FE\" iconType=\"regular\">\n  **LangChain v1.x is now available!**\n\n  For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/python/releases/langchain-v1) and [migration guide](/oss/python/migrate/langchain-v1).\n\n  If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=01-langchain.yml) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langchain/tree/v0.3/docs/docs).\n</Callout>\n\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/python/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/python/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/python/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n```python  theme={null}\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n\nSee the [Installation instructions](/oss/python/langchain/install) and [Quickstart guide](/oss/python/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n  <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/python/langchain/models\" arrow cta=\"Learn more\">\n    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n  </Card>\n\n  <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/python/langchain/agents\" arrow cta=\"Learn more\">\n    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n  </Card>\n\n  <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/python/langgraph/overview\" arrow cta=\"Learn more\">\n    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n  </Card>\n\n  <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 4090,
    "word_count": 471
  },
  {
    "title": "Philosophy",
    "source": "https://docs.langchain.com/oss/python/langchain/philosophy",
    "content": "LangChain exists to be the easiest place to start building with LLMs, while also being flexible and production-ready.\n\nLangChain is driven by a few core beliefs:\n\n* Large Language Models (LLMs) are great, powerful new technology.\n* LLMs are even better when you combine them with external sources of data.\n* LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.\n* It is still very early on in that transformation.\n* While it's easy to build a prototype of those agentic applications, it's still really hard to build agents that are reliable enough to put into production.\n\nWith LangChain, we have two core focuses:\n\n<Steps>\n  <Step title=\"We want to enable developers to build with the best models.\">\n    Different providers expose different APIs, with different model parameters and different message formats.\n    Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.\n  </Step>\n\n  <Step title=\"We want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.\">\n    Models should be used for more than just *text generation* - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define [tools](/oss/python/langchain/tools) that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.\n  </Step>\n</Steps>\n\n## History\n\nGiven the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:\n\n<Update label=\"2022-10-24\" description=\"v0.0.1\">\n  A month before ChatGPT, **LangChain was launched as a Python package**. It consisted of two main components:\n\n  * LLM abstractions\n  * \"Chains\", or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.\n\n  The name LangChain comes from \"Language\" (like Language models) and \"Chains\".\n</Update>\n\n<Update label=\"2022-12\">\n  The first general purpose agents were added to LangChain.\n\n  These general purpose agents were based on the [ReAct paper](https://arxiv.org/abs/2210.03629) (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.\n</Update>\n\n<Update label=\"2023-01\">\n  OpenAI releases a 'Chat Completion' API.\n\n  Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.\n</Update>\n\n<Update label=\"2023-01\">\n  LangChain releases a JavaScript version.\n\n  LLMs and agents will change how applications are built and JavaScript is the language of application developers.\n</Update>\n\n<Update label=\"2023-02\">\n  **LangChain Inc. was formed as a company** around the open source LangChain project.\n\n  The main goal was to \"make intelligent agents ubiquitous\". The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.\n</Update>\n\n<Update label=\"2023-03\">\n  OpenAI releases 'function calling' in their API.\n\n  This allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).\n</Update>\n\n<Update label=\"2023-06\">\n  **LangSmith is released** as closed source platform by LangChain Inc., providing observability and evals\n\n  The main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.\n</Update>\n\n<Update label=\"2024-01\" description=\"v0.1.0\">\n  **LangChain releases 0.1.0**, its first non-0.0.x.\n\n  The industry matured from prototypes to production, and as such, LangChain increased its focus on stability.\n</Update>\n\n<Update label=\"2024-02\">\n  **LangGraph is released** as an open-source library.\n\n  The original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph.\n\n  When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.\n</Update>\n\n<Update label=\"2024-06\">\n  **LangChain has over 700 integrations.**\n\n  Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `langchain-community`.\n</Update>\n\n<Update label=\"2024-10\">\n  LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.\n\n  As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.\n</Update>\n\n<Update label=\"2025-04\">\n  Model APIs become more multimodal.\n\n  Models started to accept files, images, videos, and more. We updated the `langchain-core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.\n</Update>\n\n<Update label=\"2025-10-20\" description=\"v1.0.0\">\n  **LangChain releases 1.0** with two major changes:\n\n  1. Complete revamp of all chains and agents in `langchain`. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.\n\n     For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `langchain-classic` package.\n\n  2. A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.\n</Update>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/philosophy.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 7272,
    "word_count": 1034
  },
  {
    "title": "Quickstart",
    "source": "https://docs.langchain.com/oss/python/langchain/quickstart",
    "content": "This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n## Requirements\n\nFor these examples, you will need to:\n\n* [Install](/oss/python/langchain/install) the LangChain package\n* Set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key\n* Set the `ANTHROPIC_API_KEY` environment variable in your terminal\n\n<Tip>\n  **LangChain Docs MCP server**\n\n  If you're using an AI coding assistant, you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to the latest documentation and examples.\n</Tip>\n\n## Build a basic agent\n\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** create a fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n  <Step title=\"Define the system prompt\">\n    The system prompt defines your agent’s role and behavior. Keep it specific and actionable:\n\n    ```python wrap theme={null}\n    SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n    You have access to two tools:\n\n    - get_weather_for_location: use this to get the weather for a specific location\n    - get_user_location: use this to get the user's location\n\n    If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n    ```\n  </Step>\n\n  <Step title=\"Create tools\">\n    [Tools](/oss/python/langchain/tools) let a model interact with external systems by calling functions you define.\n    Tools can depend on [runtime context](/oss/python/langchain/runtime) and also interact with [agent memory](/oss/python/langchain/short-term-memory).\n\n    Notice below how the `get_user_location` tool uses runtime context:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n\n    @tool\n    def get_weather_for_location(city: str) -> str:\n        \"\"\"Get weather for a given city.\"\"\"\n        return f\"It's always sunny in {city}!\"\n\n    @dataclass\n    class Context:\n        \"\"\"Custom runtime context schema.\"\"\"\n        user_id: str\n\n    @tool\n    def get_user_location(runtime: ToolRuntime[Context]) -> str:\n        \"\"\"Retrieve user information based on user ID.\"\"\"\n        user_id = runtime.context.user_id\n        return \"Florida\" if user_id == \"1\" else \"SF\"\n    ```\n\n    <Tip>\n      Tools should be well-documented: their name, description, and argument names become part of the model's prompt.\n      LangChain's [`@tool` decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) adds metadata and enables runtime injection via the `ToolRuntime` parameter.\n    </Tip>\n  </Step>\n\n  <Step title=\"Configure your model\">\n    Set up your [language model](/oss/python/langchain/models) with the right parameters for your use case:\n\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0.5,\n        timeout=10,\n        max_tokens=1000\n    )\n    ```\n\n    Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n  </Step>\n\n  <Step title=\"Define response format\">\n    Optionally, define a structured response format if you need the agent responses to match\n    a specific schema.\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n\n    # We use a dataclass here, but Pydantic models are also supported.\n    @dataclass\n    class ResponseFormat:\n        \"\"\"Response schema for the agent.\"\"\"\n        # A punny response (always required)\n        punny_response: str\n        # Any interesting information about the weather if available\n        weather_conditions: str | None = None\n    ```\n  </Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/python/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.\n\n    ```python  theme={null}\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    checkpointer = InMemorySaver()\n    ```\n\n    <Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>\n\n  <Step title=\"Create and run the agent\">\n    Now assemble your agent with all the components and run it!\n\n    ```python  theme={null}\n    from langchain.agents.structured_output import ToolStrategy\n\n    agent = create_agent(\n        model=model,\n        system_prompt=SYSTEM_PROMPT,\n        tools=[get_user_location, get_weather_for_location],\n        context_schema=Context,\n        response_format=ToolStrategy(ResponseFormat),\n        checkpointer=checkpointer\n    )\n\n    # `thread_id` is a unique identifier for a given conversation.\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n    #     weather_conditions=\"It's always sunny in Florida!\"\n    # )\n\n\n    # Note that we can continue the conversation using the same `thread_id`.\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n    #     weather_conditions=None\n    # )\n    ```\n  </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n  ```python  theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain.tools import tool, ToolRuntime\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  # Define system prompt\n  SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n  You have access to two tools:\n\n  - get_weather_for_location: use this to get the weather for a specific location\n  - get_user_location: use this to get the user's location\n\n  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n\n  # Define context schema\n  @dataclass\n  class Context:\n      \"\"\"Custom runtime context schema.\"\"\"\n      user_id: str\n\n  # Define tools\n  @tool\n  def get_weather_for_location(city: str) -> str:\n      \"\"\"Get weather for a given city.\"\"\"\n      return f\"It's always sunny in {city}!\"\n\n  @tool\n  def get_user_location(runtime: ToolRuntime[Context]) -> str:\n      \"\"\"Retrieve user information based on user ID.\"\"\"\n      user_id = runtime.context.user_id\n      return \"Florida\" if user_id == \"1\" else \"SF\"\n\n  # Configure model\n  model = init_chat_model(\n      \"claude-sonnet-4-5-20250929\",\n      temperature=0\n  )\n\n  # Define response format\n  @dataclass\n  class ResponseFormat:\n      \"\"\"Response schema for the agent.\"\"\"\n      # A punny response (always required)\n      punny_response: str\n      # Any interesting information about the weather if available\n      weather_conditions: str | None = None\n\n  # Set up memory\n  checkpointer = InMemorySaver()\n\n  # Create agent\n  agent = create_agent(\n      model=model,\n      system_prompt=SYSTEM_PROMPT,\n      tools=[get_user_location, get_weather_for_location],\n      context_schema=Context,\n      response_format=ToolStrategy(ResponseFormat),\n      checkpointer=checkpointer\n  )\n\n  # Run agent\n  # `thread_id` is a unique identifier for a given conversation.\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n  #     weather_conditions=\"It's always sunny in Florida!\"\n  # )\n\n\n  # Note that we can continue the conversation using the same `thread_id`.\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n  #     weather_conditions=None\n  # )\n  ```\n</Expandable>\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n* **Understand context** and remember conversations\n* **Use multiple tools** intelligently\n* **Provide structured responses** in a consistent format\n* **Handle user-specific information** through context\n* **Maintain conversation state** across interactions\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 11662,
    "word_count": 1354
  },
  {
    "title": "Build a RAG agent with LangChain",
    "source": "https://docs.langchain.com/oss/python/langchain/rag",
    "content": "## Overview\n\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q\\&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/oss/python/langchain/retrieval/).\n\nThis tutorial will show how to build a simple Q\\&A application over an unstructured text data source. We will demonstrate:\n\n1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n### Concepts\n\nWe will cover the following concepts:\n\n* **Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens in a separate process.*\n\n* **Retrieval and generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we've indexed our data, we will use an [agent](/oss/python/langchain/agents) as our orchestration framework to implement the retrieval and generation steps.\n\n<Note>\n  The indexing portion of this tutorial will largely follow the [semantic search tutorial](/oss/python/langchain/knowledge-base).\n\n  If your data is already available for search (i.e., you have a function to execute a search), or you're comfortable with the content from that tutorial, feel free to skip to the section on [retrieval and generation](#2-retrieval-and-generation)\n</Note>\n\n### Preview\n\nIn this guide we'll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n\nWe can create a simple indexing pipeline and RAG chain to do this in \\~40 lines of code. See below for the full code snippet:\n\n<Accordion title=\"Expand for full code snippet\">\n  ```python  theme={null}\n  import bs4\n  from langchain.agents import AgentState, create_agent\n  from langchain_community.document_loaders import WebBaseLoader\n  from langchain.messages import MessageLikeRepresentation\n  from langchain_text_splitters import RecursiveCharacterTextSplitter\n\n  # Load and chunk contents of the blog\n  loader = WebBaseLoader(\n      web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n      bs_kwargs=dict(\n          parse_only=bs4.SoupStrainer(\n              class_=(\"post-content\", \"post-title\", \"post-header\")\n          )\n      ),\n  )\n  docs = loader.load()\n\n  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n  all_splits = text_splitter.split_documents(docs)\n\n  # Index chunks\n  _ = vector_store.add_documents(documents=all_splits)\n\n  # Construct a tool for retrieving context\n  @tool(response_format=\"content_and_artifact\")\n  def retrieve_context(query: str):\n      \"\"\"Retrieve information to help answer a query.\"\"\"\n      retrieved_docs = vector_store.similarity_search(query, k=2)\n      serialized = \"\\n\\n\".join(\n          (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n          for doc in retrieved_docs\n      )\n      return serialized, retrieved_docs\n\n  tools = [retrieve_context]\n  # If desired, specify custom instructions\n  prompt = (\n      \"You have access to a tool that retrieves context from a blog post. \"\n      \"Use the tool to help answer user queries.\"\n  )\n  agent = create_agent(model, tools, system_prompt=prompt)\n  ```\n\n  ```python  theme={null}\n  query = \"What is task decomposition?\"\n  for step in agent.stream(\n      {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n      stream_mode=\"values\",\n  ):\n      step[\"messages\"][-1].pretty_print()\n  ```\n\n  ```\n  ================================ Human Message =================================\n\n  What is task decomposition?\n  ================================== Ai Message ==================================\n  Tool Calls:\n    retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\n   Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\n    Args:\n      query: task decomposition\n  ================================= Tool Message =================================\n  Name: retrieve_context\n\n  Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n  Content: Task decomposition can be done by...\n\n  Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n  Content: Component One: Planning...\n  ================================== Ai Message ==================================\n\n  Task decomposition refers to...\n  ```\n\n  Check out the [LangSmith trace](https://smith.langchain.com/public/a117a1f8-c96c-4c16-a285-00b85646118e/r).\n</Accordion>\n\n## Setup\n\n### Installation\n\nThis tutorial requires these langchain dependencies:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain langchain-text-splitters langchain-community bs4\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain langchain-text-splitters langchain-community bs4\n  ```\n\n  ```bash conda theme={null}\n  conda install langchain langchain-text-splitters langchain-community bs4 -c conda-forge\n  ```\n</CodeGroup>\n\nFor more details, see our [Installation guide](/oss/python/langchain/install).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```shell  theme={null}\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\nOr, set them in Python:\n\n```python  theme={null}\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\n### Components\n\nWe will need to select three components from LangChain's suite of integrations.\n\nSelect a chat model:\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n\n    <Tab title=\"HuggingFace\">\n      👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n      ```shell  theme={null}\n      pip install -U \"langchain[huggingface]\"\n      ```\n\n      <CodeGroup>\n        ```python init_chat_model theme={null}\n        import os\n        from langchain.chat_models import init_chat_model\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        model = init_chat_model(\n            \"microsoft/Phi-3-mini-4k-instruct\",\n            model_provider=\"huggingface\",\n            temperature=0.7,\n            max_tokens=1024,\n        )\n        ```\n\n        ```python Model Class theme={null}\n        import os\n        from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        llm = HuggingFaceEndpoint(\n            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n            temperature=0.7,\n            max_length=1024,\n        )\n        model = ChatHuggingFace(llm=llm)\n        ```\n      </CodeGroup>\n    </Tab>\n  </Tab>\n</Tabs>\n\nSelect an embeddings model:\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"OPENAI_API_KEY\"):\n        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\n    from langchain_openai import OpenAIEmbeddings\n\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n    ```\n  </Tab>\n\n  <Tab title=\"Azure\">\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n        os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n\n    from langchain_openai import AzureOpenAIEmbeddings\n\n    embeddings = AzureOpenAIEmbeddings(\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n        openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    ```shell  theme={null}\n    pip install -qU langchain-google-genai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"GOOGLE_API_KEY\"):\n        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\n    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n    ```\n  </Tab>\n\n  <Tab title=\"Google Vertex\">\n    ```shell  theme={null}\n    pip install -qU langchain-google-vertexai\n    ```\n\n    ```python  theme={null}\n    from langchain_google_vertexai import VertexAIEmbeddings\n\n    embeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n    ```\n  </Tab>\n\n  <Tab title=\"AWS\">\n    ```shell  theme={null}\n    pip install -qU langchain-aws\n    ```\n\n    ```python  theme={null}\n    from langchain_aws import BedrockEmbeddings\n\n    embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n    ```\n  </Tab>\n\n  <Tab title=\"HuggingFace\">\n    ```shell  theme={null}\n    pip install -qU langchain-huggingface\n    ```\n\n    ```python  theme={null}\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    ```\n  </Tab>\n\n  <Tab title=\"Ollama\">\n    ```shell  theme={null}\n    pip install -qU langchain-ollama\n    ```\n\n    ```python  theme={null}\n    from langchain_ollama import OllamaEmbeddings\n\n    embeddings = OllamaEmbeddings(model=\"llama3\")\n    ```\n  </Tab>\n\n  <Tab title=\"Cohere\">\n    ```shell  theme={null}\n    pip install -qU langchain-cohere\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"COHERE_API_KEY\"):\n        os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n\n    from langchain_cohere import CohereEmbeddings\n\n    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n    ```\n  </Tab>\n\n  <Tab title=\"MistralAI\">\n    ```shell  theme={null}\n    pip install -qU langchain-mistralai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"MISTRALAI_API_KEY\"):\n        os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\n\n    from langchain_mistralai import MistralAIEmbeddings\n\n    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n    ```\n  </Tab>\n\n  <Tab title=\"Nomic\">\n    ```shell  theme={null}\n    pip install -qU langchain-nomic\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NOMIC_API_KEY\"):\n        os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\n\n    from langchain_nomic import NomicEmbeddings\n\n    embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n    ```\n  </Tab>\n\n  <Tab title=\"NVIDIA\">\n    ```shell  theme={null}\n    pip install -qU langchain-nvidia-ai-endpoints\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NVIDIA_API_KEY\"):\n        os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n\n    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\n    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n    ```\n  </Tab>\n\n  <Tab title=\"Voyage AI\">\n    ```shell  theme={null}\n    pip install -qU langchain-voyageai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"VOYAGE_API_KEY\"):\n        os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n\n    from langchain-voyageai import VoyageAIEmbeddings\n\n    embeddings = VoyageAIEmbeddings(model=\"voyage-3\")\n    ```\n  </Tab>\n\n  <Tab title=\"IBM watsonx\">\n    ```shell  theme={null}\n    pip install -qU langchain-ibm\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"WATSONX_APIKEY\"):\n        os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\n\n    from langchain_ibm import WatsonxEmbeddings\n\n    embeddings = WatsonxEmbeddings(\n        model_id=\"ibm/slate-125m-english-rtrvr\",\n        url=\"https://us-south.ml.cloud.ibm.com\",\n        project_id=\"<WATSONX PROJECT_ID>\",\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Fake\">\n    ```shell  theme={null}\n    pip install -qU langchain-core\n    ```\n\n    ```python  theme={null}\n    from langchain_core.embeddings import DeterministicFakeEmbedding\n\n    embeddings = DeterministicFakeEmbedding(size=4096)\n    ```\n  </Tab>\n\n  <Tab title=\"Isaacus\">\n    ```shell  theme={null}\n    pip install -qU langchain-isaacus\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"ISAACUS_API_KEY\"):\n    os.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\n\n    from langchain_isaacus import IsaacusEmbeddings\n\n    embeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\n    ```\n  </Tab>\n</Tabs>\n\nSelect a vector store:\n\n<Tabs>\n  <Tab title=\"In-memory\">\n    ```shell  theme={null}\n    pip install -U \"langchain-core\"\n    ```\n\n    ```python  theme={null}\n    from langchain_core.vectorstores import InMemoryVectorStore\n\n    vector_store = InMemoryVectorStore(embeddings)\n    ```\n  </Tab>\n\n  <Tab title=\"AstraDB\">\n    ```shell  theme={null}\n    pip install -U \"langchain-astradb\"\n    ```\n\n    ```python  theme={null}\n    from langchain_astradb import AstraDBVectorStore\n\n    vector_store = AstraDBVectorStore(\n        embedding=embeddings,\n        api_endpoint=ASTRA_DB_API_ENDPOINT,\n        collection_name=\"astra_vector_langchain\",\n        token=ASTRA_DB_APPLICATION_TOKEN,\n        namespace=ASTRA_DB_NAMESPACE,\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Chroma\">\n    ```shell  theme={null}\n    pip install -qU langchain-chroma\n    ```\n\n    ```python  theme={null}\n    from langchain_chroma import Chroma\n\n    vector_store = Chroma(\n        collection_name=\"example_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"FAISS\">\n    ```shell  theme={null}\n    pip install -qU langchain-community faiss-cpu\n    ```\n\n    ```python  theme={null}\n    import faiss\n    from langchain_community.docstore.in_memory import InMemoryDocstore\n    from langchain_community.vectorstores import FAISS\n\n    embedding_dim = len(embeddings.embed_query(\"hello world\"))\n    index = faiss.IndexFlatL2(embedding_dim)\n\n    vector_store = FAISS(\n        embedding_function=embeddings,\n        index=index,\n        docstore=InMemoryDocstore(),\n        index_to_docstore_id={},\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Milvus\">\n    ```shell  theme={null}\n    pip install -qU langchain-milvus\n    ```\n\n    ```python  theme={null}\n    from langchain_milvus import Milvus\n\n    URI = \"./milvus_example.db\"\n\n    vector_store = Milvus(\n        embedding_function=embeddings,\n        connection_args={\"uri\": URI},\n        index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"MongoDB\">\n    ```shell  theme={null}\n    pip install -qU langchain-mongodb\n    ```\n\n    ```python  theme={null}\n    from langchain_mongodb import MongoDBAtlasVectorSearch\n\n    vector_store = MongoDBAtlasVectorSearch(\n        embedding=embeddings,\n        collection=MONGODB_COLLECTION,\n        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n        relevance_score_fn=\"cosine\",\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"PGVector\">\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGVector\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=\"my_docs\",\n        connection=\"postgresql+psycopg://...\",\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"PGVectorStore\">\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGEngine, PGVectorStore\n\n    pg_engine = PGEngine.from_connection_string(\n        url=\"postgresql+psycopg://...\"\n    )\n\n    vector_store = PGVectorStore.create_sync(\n        engine=pg_engine,\n        table_name='test_table',\n        embedding_service=embedding\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Pinecone\">\n    ```shell  theme={null}\n    pip install -qU langchain-pinecone\n    ```\n\n    ```python  theme={null}\n    from langchain_pinecone import PineconeVectorStore\n    from pinecone import Pinecone\n\n    pc = Pinecone(api_key=...)\n    index = pc.Index(index_name)\n\n    vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n    ```\n  </Tab>\n\n  <Tab title=\"Qdrant\">\n    ```shell  theme={null}\n    pip install -qU langchain-qdrant\n    ```\n\n    ```python  theme={null}\n    from qdrant_client.models import Distance, VectorParams\n    from langchain_qdrant import QdrantVectorStore\n    from qdrant_client import QdrantClient\n\n    client = QdrantClient(\":memory:\")\n\n    vector_size = len(embeddings.embed_query(\"sample text\"))\n\n    if not client.collection_exists(\"test\"):\n        client.create_collection(\n            collection_name=\"test\",\n            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n        )\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=\"test\",\n        embedding=embeddings,\n    )\n    ```\n  </Tab>\n</Tabs>\n\n## 1. Indexing\n\n<Note>\n  **This section is an abbreviated version of the content in the [semantic search tutorial](/oss/python/langchain/knowledge-base).**\n\n  If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](/oss/python/langchain/retrieval#document_loaders), [embeddings](/oss/python/langchain/retrieval#embedding_models), and [vector stores](/oss/python/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](/oss/python/langchain/rag#2-retrieval-and-generation).\n</Note>\n\nIndexing commonly works as follows:\n\n1. **Load**: First we need to load our data. This is done with [Document Loaders](/oss/python/langchain/retrieval#document_loaders).\n2. **Split**: [Text splitters](/oss/python/langchain/retrieval#text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/oss/python/langchain/retrieval#vectorstores) and [Embeddings](/oss/python/langchain/retrieval#embedding_models) model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=21403ce0d0c772da84dcc5b75cff4451\" alt=\"index_diagram\" data-og-width=\"2583\" width=\"2583\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_indexing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=bf4eb8255b82a809dbbd2bc2a96d2ed7 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4ebc538b2c4765b609f416025e4dbbda 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=1838328a870c7353c42bf1cc2290a779 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=675f55e100bab5e2904d27db01775ccc 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4b9e544a7a3ec168651558bce854eb60 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=f5aeaaaea103128f374c03b05a317263 2500w\" />\n\n### Loading documents\n\nWe need to first load the blog post contents. We can use [DocumentLoaders](/oss/python/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.\n\nIn this case we'll use the [`WebBaseLoader`](/oss/python/integrations/document_loaders/web_base), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we'll remove all others.\n\n```python  theme={null}\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\n```\n\n```output  theme={null}\nTotal characters: 43131\n```\n\n```python  theme={null}\nprint(docs[0].page_content[:500])\n```\n\n```output  theme={null}\n      LLM Powered Autonomous Agents\n\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n```\n\n**Go deeper**\n\n`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n\n* [Integrations](/oss/python/integrations/document_loaders/): 160+ integrations to choose from.\n* [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader): API reference for the base interface.\n\n### Splitting documents\n\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n\nTo handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n\nAs in the [semantic search tutorial](/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\n```python  theme={null}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\n```\n\n```output  theme={null}\nSplit blog post into 66 sub-documents.\n```\n\n**Go deeper**\n\n`TextSplitter`: Object that splits a list of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects into smaller\nchunks for storage and retrieval.\n\n* [Integrations](/oss/python/integrations/splitters/)\n* [Interface](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API reference for the base interface.\n\n### Storing documents\n\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](/oss/python/langchain/knowledge-base), our approach is to [embed](/oss/python/langchain/retrieval#embedding_models/) the contents of each document split and insert these embeddings into a [vector store](/oss/python/langchain/retrieval#vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents.\n\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](/oss/python/langchain/rag#components).\n\n```python  theme={null}\ndocument_ids = vector_store.add_documents(documents=all_splits)\n\nprint(document_ids[:3])\n```\n\n```output  theme={null}\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\n```\n\n**Go deeper**\n\n`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.\n\n* [Integrations](/oss/python/integrations/text_embedding/): 30+ integrations to choose from.\n* [Interface](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings): API reference for the base interface.\n\n`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.\n\n* [Integrations](/oss/python/integrations/vectorstores/): 40+ integrations to choose from.\n* [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html): API reference for the base interface.\n\nThis completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\n## 2. Retrieval and Generation\n\nRAG applications commonly work as follows:\n\n1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/oss/python/langchain/retrieval#retrievers).\n2. **Generate**: A [model](/oss/python/langchain/models) produces an answer using a prompt that includes both the question with the retrieved data\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=994c3585cece93c80873d369960afd44\" alt=\"retrieval_diagram\" data-og-width=\"2532\" width=\"2532\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_retrieval_generation.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=3bd28b3662e08c8364b60b74f510751e 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=43484903ca631a47a54e86191eb5ba22 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=67fe2302e241fc24238a5df1cf56573d 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=d390a6a758e688ec36352d30b22249b0 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=59729377317a0631598b6a4a2a7d8c92 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=c07711c71153c3b2dfd5b0104ad3e324 2500w\" />\n\nNow let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n\nWe will demonstrate:\n\n1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n### RAG agents\n\nOne formulation of a RAG application is as a simple [agent](/oss/python/langchain/agents) with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a [tool](/oss/python/langchain/tools) that wraps our vector store:\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n```\n\n<Tip>\n  Here we use the [tool decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) to configure the tool to attach raw documents as [artifacts](/oss/python/langchain/messages#param-artifact) to each [ToolMessage](/oss/python/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n</Tip>\n\n<Tip>\n  Retrieval tools are not limited to a single string `query` argument, as in the above example. You can\n  force the LLM to specify additional search parameters by adding arguments— for example, a category:\n\n  ```python  theme={null}\n  from typing import Literal\n\n  def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n  ```\n</Tip>\n\nGiven our tool, we can construct the agent:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\n```\n\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\n```python  theme={null}\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhat is the standard method for Task Decomposition?\n\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\n  Args:\n    query: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\n```\n\nNote that the agent:\n\n1. Generates a query to search for a standard method for task decomposition;\n2. Receiving the answer, generates a second query to search for common extensions of it;\n3. Having received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).\n\n<Tip>\n  You can add a deeper level of control and customization using the [LangGraph](/oss/python/langgraph/overview) framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/python/langgraph/agentic-rag) for more advanced formulations.\n</Tip>\n\n### RAG chains\n\nIn the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/python/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\n| ✅ Benefits                                                                                                                                                 | ⚠️ Drawbacks                                                                                                                                |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Search only when needed** – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.                        | **Two inference calls** – When a search is performed, it requires one call to generate the query and another to produce the final response. |\n| **Contextual search queries** – By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.                    |\n| **Multiple searches allowed** – The LLM can execute several searches in support of a single user query.                                                    |                                                                                                                                             |\n\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\n\nIn this approach we no longer call the model in a loop, but instead make a single pass.\n\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\n\n```python  theme={null}\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n@dynamic_prompt\ndef prompt_with_context(request: ModelRequest) -> str:\n    \"\"\"Inject context into state messages.\"\"\"\n    last_query = request.state[\"messages\"][-1].text\n    retrieved_docs = vector_store.similarity_search(last_query)\n\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\n    system_message = (\n        \"You are a helpful assistant. Use the following context in your response:\"\n        f\"\\n\\n{docs_content}\"\n    )\n\n    return system_message\n\n\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\n```\n\nLet's try this out:\n\n```python  theme={null}\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\n\nTask decomposition is...\n```\n\nIn the [LangSmith trace](https://smith.langchain.com/public/0322904b-bc4c-4433-a568-54c6b31bbef4/r/9ef1c23e-380e-46bf-94b3-d8bb33df440c) we can see the retrieved context incorporated into the model prompt.\n\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n\n<Accordion title=\"Returning source documents\">\n  The above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run.\n\n  As in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n\n  1. Adding a key to the state to store the retrieved documents\n  2. Adding a new node via a [pre-model hook](/oss/python/langchain/agents#pre-model-hook) to populate that key (as well as inject the context).\n\n  ```python  theme={null}\n  from typing import Any\n  from langchain_core.documents import Document\n  from langchain.agents.middleware import AgentMiddleware, AgentState\n\n\n  class State(AgentState):\n      context: list[Document]\n\n\n  class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n      state_schema = State\n\n      def before_model(self, state: AgentState) -> dict[str, Any] | None:\n          last_message = state[\"messages\"][-1]\n          retrieved_docs = vector_store.similarity_search(last_message.text)\n\n          docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\n          augmented_message_content = (\n              f\"{last_message.text}\\n\\n\"\n              \"Use the following context to answer the query:\\n\"\n              f\"{docs_content}\"\n          )\n          return {\n              \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n              \"context\": retrieved_docs,\n          }\n\n\n  agent = create_agent(\n      model,\n      tools=[],\n      middleware=[RetrieveDocumentsMiddleware()],\n  )\n  ```\n</Accordion>\n\n## Next steps\n\nNow that we've implemented a simple RAG application via [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), we can easily incorporate new features and go deeper:\n\n* [Stream](/oss/python/langchain/streaming) tokens and other information for responsive user experiences\n* Add [conversational memory](/oss/python/langchain/short-term-memory) to support multi-turn interactions\n* Add [long-term memory](/oss/python/langchain/long-term-memory) to support memory across conversational threads\n* Add [structured responses](/oss/python/langchain/structured-output)\n* Deploy your application with [LangSmith Deployments](/langsmith/deployments)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/rag.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 44173,
    "word_count": 3959
  },
  {
    "title": "Retrieval",
    "source": "https://docs.langchain.com/oss/python/langchain/retrieval",
    "content": "Large Language Models (LLMs) are powerful, but they have two key limitations:\n\n* **Finite context** — they can’t ingest entire corpora at once.\n* **Static knowledge** — their training data is frozen at a point in time.\n\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.\n\n## Building a knowledge base\n\nA **knowledge base** is a repository of documents or structured data used during retrieval.\n\nIf you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.\n\n<Note>\n  If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:\n\n  * Connect it as a **tool** for an agent in Agentic RAG.\n  * Query it and supply the retrieved content as context to the LLM [(2-Step RAG)](#2-step-rag).\n</Note>\n\nSee the following tutorial to build a searchable knowledge base and minimal RAG workflow:\n\n<Card title=\"Tutorial: Semantic search\" icon=\"database\" href=\"/oss/python/langchain/knowledge-base\" arrow cta=\"Learn more\">\n  Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.\n  In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\n</Card>\n\n### From retrieval to RAG\n\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.\n\nThis is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\n\n### Retrieval Pipeline\n\nA typical retrieval workflow looks like this:\n\n```mermaid  theme={null}\nflowchart LR\n  S([\"Sources<br>(Google Drive, Slack, Notion, etc.)\"]) --> L[Document Loaders]\n  L --> A([Documents])\n  A --> B[Split into chunks]\n  B --> C[Turn into embeddings]\n  C --> D[(Vector Store)]\n  Q([User Query]) --> E[Query embedding]\n  E --> D\n  D --> F[Retriever]\n  F --> G[LLM uses retrieved info]\n  G --> H([Answer])\n```\n\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.\n\n### Building Blocks\n\n<Columns cols={2}>\n  <Card title=\"Document loaders\" icon=\"file-import\" href=\"/oss/python/integrations/document_loaders\" arrow cta=\"Learn more\">\n    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.\n  </Card>\n\n  <Card title=\"Text splitters\" icon=\"scissors\" href=\"/oss/python/integrations/splitters\" arrow cta=\"Learn more\">\n    Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.\n  </Card>\n\n  <Card title=\"Embedding models\" icon=\"diagram-project\" href=\"/oss/python/integrations/text_embedding\" arrow cta=\"Learn more\">\n    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\n  </Card>\n\n  <Card title=\"Vector stores\" icon=\"database\" href=\"/oss/python/integrations/vectorstores/\" arrow cta=\"Learn more\">\n    Specialized databases for storing and searching embeddings.\n  </Card>\n\n  <Card title=\"Retrievers\" icon=\"binoculars\" href=\"/oss/python/integrations/retrievers/\" arrow cta=\"Learn more\">\n    A retriever is an interface that returns documents given an unstructured query.\n  </Card>\n</Columns>\n\n## RAG Architectures\n\nRAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.\n\n| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |\n| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |\n| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |\n| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |\n| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\\&A with quality validation      |\n\n<Info>\n  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.\n</Info>\n\n### 2-step RAG\n\nIn **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\n\n```mermaid  theme={null}\ngraph LR\n    A[User Question] --> B[\"Retrieve Relevant Documents\"]\n    B --> C[\"Generate Answer\"]\n    C --> D[Return Answer to User]\n\n    %% Styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,D startend\n    class B,C process\n```\n\n<Card title=\"Tutorial: Retrieval-Augmented Generation (RAG)\" icon=\"robot\" href=\"/oss/python/langchain/rag#rag-chains\" arrow cta=\"Learn more\">\n  See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n  This tutorial walks through two approaches:\n\n  * A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\n  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\n</Card>\n\n### Agentic RAG\n\n**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.\n\n<Tip>\n  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.\n</Tip>\n\n```mermaid  theme={null}\ngraph LR\n    A[User Input / Question] --> B[\"Agent (LLM)\"]\n    B --> C{Need external info?}\n    C -- Yes --> D[\"Search using tool(s)\"]\n    D --> H{Enough to answer?}\n    H -- No --> B\n    H -- Yes --> I[Generate final answer]\n    C -- No --> I\n    I --> J[Return to user]\n\n    %% Dark-mode friendly styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,J startend\n    class B,D,I process\n    class C,H decision\n```\n\n```python  theme={null}\nimport requests\nfrom langchain.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom langchain.agents import create_agent\n\n\n@tool\ndef fetch_url(url: str) -> str:\n    \"\"\"Fetch text content from a URL\"\"\"\n    response = requests.get(url, timeout=10.0)\n    response.raise_for_status()\n    return response.text\n\nsystem_prompt = \"\"\"\\\nUse fetch_url when you need to fetch information from a web-page; quote relevant snippets.\n\"\"\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[fetch_url], # A tool for retrieval [!code highlight]\n    system_prompt=system_prompt,\n)\n```\n\n<Expandable title=\"Extended example: Agentic RAG for LangGraph's llms.txt\">\n  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.\n\n  ```python  theme={null}\n  import requests\n  from langchain.agents import create_agent\n  from langchain.messages import HumanMessage\n  from langchain.tools import tool\n  from markdownify import markdownify\n\n\n  ALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"]\n  LLMS_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt'\n\n\n  @tool\n  def fetch_documentation(url: str) -> str:  # [!code highlight]\n      \"\"\"Fetch and convert documentation from a URL\"\"\"\n      if not any(url.startswith(domain) for domain in ALLOWED_DOMAINS):\n          return (\n              \"Error: URL not allowed. \"\n              f\"Must start with one of: {', '.join(ALLOWED_DOMAINS)}\"\n          )\n      response = requests.get(url, timeout=10.0)\n      response.raise_for_status()\n      return markdownify(response.text)\n\n\n  # We will fetch the content of llms.txt, so this can\n  # be done ahead of time without requiring an LLM request.\n  llms_txt_content = requests.get(LLMS_TXT).text\n\n  # System prompt for the agent\n  system_prompt = f\"\"\"\n  You are an expert Python developer and technical assistant.\n  Your primary role is to help users with questions about LangGraph and related tools.\n\n  Instructions:\n\n  1. If a user asks a question you're unsure about — or one that likely involves API usage,\n     behavior, or configuration — you MUST use the `fetch_documentation` tool to consult the relevant docs.\n  2. When citing documentation, summarize clearly and include relevant context from the content.\n  3. Do not use any URLs outside of the allowed domain.\n  4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\n\n  You can access official documentation from the following approved sources:\n\n  {llms_txt_content}\n\n  You MUST consult the documentation to get up to date documentation\n  before answering a user's question about LangGraph.\n\n  Your answers should be clear, concise, and technically accurate.\n  \"\"\"\n\n  tools = [fetch_documentation]\n\n  model = init_chat_model(\"claude-sonnet-4-0\", max_tokens=32_000)\n\n  agent = create_agent(\n      model=model,\n      tools=tools,  # [!code highlight]\n      system_prompt=system_prompt,  # [!code highlight]\n      name=\"Agentic RAG\",\n  )\n\n  response = agent.invoke({\n      'messages': [\n          HumanMessage(content=(\n              \"Write a short example of a langgraph agent using the \"\n              \"prebuilt create react agent. the agent should be able \"\n              \"to look up stock pricing information.\"\n          ))\n      ]\n  })\n\n  print(response['messages'][-1].content)\n  ```\n</Expandable>\n\n<Card title=\"Tutorial: Retrieval-Augmented Generation (RAG)\" icon=\"robot\" href=\"/oss/python/langchain/rag\" arrow cta=\"Learn more\">\n  See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n  This tutorial walks through two approaches:\n\n  * A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\n  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\n</Card>\n\n### Hybrid RAG\n\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\n\nTypical components include:\n\n* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\n* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\n* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n\nThe architecture often supports multiple iterations between these steps:\n\n```mermaid  theme={null}\ngraph LR\n    A[User Question] --> B[Query Enhancement]\n    B --> C[Retrieve Documents]\n    C --> D{Sufficient Info?}\n    D -- No --> E[Refine Query]\n    E --> C\n    D -- Yes --> F[Generate Answer]\n    F --> G{Answer Quality OK?}\n    G -- No --> H{Try Different Approach?}\n    H -- Yes --> E\n    H -- No --> I[Return Best Answer]\n    G -- Yes --> I\n    I --> J[Return to User]\n\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,J startend\n    class B,C,E,F,I process\n    class D,G,H decision\n```\n\nThis architecture is suitable for:\n\n* Applications with ambiguous or underspecified queries\n* Systems that require validation or quality control steps\n* Workflows involving multiple sources or iterative refinement\n\n<Card title=\"Tutorial: Agentic RAG with Self-Correction\" icon=\"robot\" href=\"/oss/python/langgraph/agentic-rag\" arrow cta=\"Learn more\">\n  An example of **Hybrid RAG** that combines agentic reasoning with retrieval and self-correction.\n</Card>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/retrieval.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 14395,
    "word_count": 1783
  },
  {
    "title": "Runtime",
    "source": "https://docs.langchain.com/oss/python/langchain/runtime",
    "content": "## Overview\n\nLangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) runs on LangGraph's runtime under the hood.\n\nLangGraph exposes a [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object with the following information:\n\n1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation\n2. **Store**: a [BaseStore](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) instance used for [long-term memory](/oss/python/langchain/long-term-memory)\n3. **Stream writer**: an object used for streaming information via the `\"custom\"` stream mode\n\n<Tip>\n  Runtime context provides **dependency injection** for your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.\n</Tip>\n\nYou can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware).\n\n## Access\n\nWhen creating an agent with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), you can specify a `context_schema` to define the structure of the `context` stored in the agent [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime).\n\nWhen invoking the agent, pass the `context` argument with the relevant configuration for the run:\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_name: str\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    context_schema=Context  # [!code highlight]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")  # [!code highlight]\n)\n```\n\n### Inside tools\n\nYou can access the runtime information inside tools to:\n\n* Access the context\n* Read or write long-term memory\n* Write to the [custom stream](/oss/python/langchain/streaming#custom-updates) (ex, tool progress / updates)\n\nUse the `ToolRuntime` parameter to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside a tool.\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime  # [!code highlight]\n\n@dataclass\nclass Context:\n    user_id: str\n\n@tool\ndef fetch_user_email_preferences(runtime: ToolRuntime[Context]) -> str:  # [!code highlight]\n    \"\"\"Fetch the user's email preferences from the store.\"\"\"\n    user_id = runtime.context.user_id  # [!code highlight]\n\n    preferences: str = \"The user prefers you to write a brief and polite email.\"\n    if runtime.store:  # [!code highlight]\n        if memory := runtime.store.get((\"users\",), user_id):  # [!code highlight]\n            preferences = memory.value[\"preferences\"]\n\n    return preferences\n```\n\n### Inside middleware\n\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\n\nUse `request.runtime` to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside middleware decorators. The runtime object is available in the [`ModelRequest`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRequest) parameter passed to middleware functions.\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.messages import AnyMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:\n    user_name: str\n\n# Dynamic prompts\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context.user_name  # [!code highlight]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n# Before model hook\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Processing request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\n# After model hook\n@after_model\ndef log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Completed request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  # [!code highlight]\n    context_schema=Context\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")\n)\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 5419,
    "word_count": 531
  },
  {
    "title": "Short-term memory",
    "source": "https://docs.langchain.com/oss/python/langchain/short-term-memory",
    "content": "## Overview\n\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nShort term memory lets your application remember previous interactions within a single thread or conversation.\n\n<Note>\n  A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n</Note>\n\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors.\n\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs.\n\nChat models accept context using [messages](/oss/python/langchain/messages), which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or \"forget\" stale information.\n\n## Usage\n\nTo add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.\n\n<Info>\n  LangChain's agent manages short-term memory as a part of your agent's state.\n\n  By storing these in the graph's state, the agent can access the full context for a given conversation while maintaining separation between different threads.\n\n  State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\n\n  Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\n</Info>\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver  # [!code highlight]\n\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    checkpointer=InMemorySaver(),  # [!code highlight]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  # [!code highlight]\n)\n```\n\n### In production\n\nIn production, use a checkpointer backed by a database:\n\n```shell  theme={null}\npip install langgraph-checkpoint-postgres\n```\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\nfrom langgraph.checkpoint.postgres import PostgresSaver  # [!code highlight]\n\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    checkpointer.setup() # auto create tables in PostgresSql\n    agent = create_agent(\n        \"gpt-5\",\n        tools=[get_user_info],\n        checkpointer=checkpointer,  # [!code highlight]\n    )\n```\n\n## Customizing agent memory\n\nBy default, agents use [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to manage short term memory, specifically the conversation history via a `messages` key.\n\nYou can extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to add additional fields. Custom state schemas are passed to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) using the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter.\n\n```python  theme={null}\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass CustomAgentState(AgentState):  # [!code highlight]\n    user_id: str  # [!code highlight]\n    preferences: dict  # [!code highlight]\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    state_schema=CustomAgentState,  # [!code highlight]\n    checkpointer=InMemorySaver(),\n)\n\n# Custom state can be passed in invoke\nresult = agent.invoke(\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"user_id\": \"user_123\",  # [!code highlight]\n        \"preferences\": {\"theme\": \"dark\"}  # [!code highlight]\n    },\n    {\"configurable\": {\"thread_id\": \"1\"}})\n```\n\n## Common patterns\n\nWith [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n\n<CardGroup cols={2}>\n  <Card title=\"Trim messages\" icon=\"scissors\" href=\"#trim-messages\" arrow>\n    Remove first or last N messages (before calling LLM)\n  </Card>\n\n  <Card title=\"Delete messages\" icon=\"trash\" href=\"#delete-messages\" arrow>\n    Delete messages from LangGraph state permanently\n  </Card>\n\n  <Card title=\"Summarize messages\" icon=\"layer-group\" href=\"#summarize-messages\" arrow>\n    Summarize earlier messages in the history and replace them with a summary\n  </Card>\n\n  <Card title=\"Custom strategies\" icon=\"gears\">\n    Custom strategies (e.g., message filtering, etc.)\n  </Card>\n</CardGroup>\n\nThis allows the agent to keep track of the conversation without exceeding the LLM's context window.\n\n### Trim messages\n\nMost LLMs have a maximum supported context window (denominated in tokens).\n\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.\n\nTo trim message history in an agent, use the [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) middleware decorator:\n\n```python  theme={null}\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\nagent = create_agent(\n    your_model_here,\n    tools=your_tools_here,\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n```\n\n### Delete messages\n\nYou can delete messages from the graph state to manage the message history.\n\nThis is useful when you want to remove specific messages or clear the entire message history.\n\nTo delete messages from the graph state, you can use the `RemoveMessage`.\n\nFor `RemoveMessage` to work, you need to use a state key with [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) [reducer](/oss/python/langgraph/graph-api#reducers).\n\nThe default [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) provides this.\n\nTo remove specific messages:\n\n```python  theme={null}\nfrom langchain.messages import RemoveMessage  # [!code highlight]\n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]\n```\n\nTo remove **all** messages:\n\n```python  theme={null}\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]\n```\n\n<Warning>\n  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n\n  * Some providers expect message history to start with a `user` message\n  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n</Warning>\n\n```python  theme={null}\nfrom langchain.messages import RemoveMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\n\n\n@after_model\ndef delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n    return None\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    system_prompt=\"Please be concise and to the point.\",\n    middleware=[delete_old_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n```\n\n```\n[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n```\n\n### Summarize messages\n\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\nBecause of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938\" alt=\"\" data-og-width=\"609\" width=\"609\" data-og-height=\"242\" height=\"242\" data-path=\"oss/images/summary.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w\" />\n\nTo summarize message history in an agent, use the built-in [`SummarizationMiddleware`](/oss/python/langchain/middleware#summarization):\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_core.runnables import RunnableConfig\n\n\ncheckpointer = InMemorySaver()\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 20)\n        )\n    ],\n    checkpointer=checkpointer,\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob!\n\"\"\"\n```\n\nSee [`SummarizationMiddleware`](/oss/python/langchain/middleware#summarization) for more configuration options.\n\n## Access memory\n\nYou can access and modify the short-term memory (state) of an agent in several ways:\n\n### Tools\n\n#### Read short-term memory in a tool\n\nAccess short term memory (state) in a tool using the `ToolRuntime` parameter.\n\nThe `tool_runtime` parameter is hidden from the tool signature (so the model doesn't see it), but the tool can access the state through it.\n\n```python  theme={null}\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.tools import tool, ToolRuntime\n\n\nclass CustomState(AgentState):\n    user_id: str\n\n@tool\ndef get_user_info(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Look up user info.\"\"\"\n    user_id = runtime.state[\"user_id\"]\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_user_info],\n    state_schema=CustomState,\n)\n\nresult = agent.invoke({\n    \"messages\": \"look up user information\",\n    \"user_id\": \"user_123\"\n})\nprint(result[\"messages\"][-1].content)\n# > User is John Smith.\n```\n\n#### Write short-term memory from tools\n\nTo modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools.\n\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.messages import ToolMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.types import Command\nfrom pydantic import BaseModel\n\n\nclass CustomState(AgentState):  # [!code highlight]\n    user_name: str\n\nclass CustomContext(BaseModel):\n    user_id: str\n\n@tool\ndef update_user_info(\n    runtime: ToolRuntime[CustomContext, CustomState],\n) -> Command:\n    \"\"\"Look up and update user info.\"\"\"\n    user_id = runtime.context.user_id\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    return Command(update={  # [!code highlight]\n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ]\n    })\n\n@tool\ndef greet(\n    runtime: ToolRuntime[CustomContext, CustomState]\n) -> str | Command:\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\n    user_name = runtime.state.get(\"user_name\", None)\n    if user_name is None:\n       return Command(update={\n            \"messages\": [\n                ToolMessage(\n                    \"Please call the 'update_user_info' tool it will get and update the user's name.\",\n                    tool_call_id=runtime.tool_call_id\n                )\n            ]\n        })\n    return f\"Hello {user_name}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[update_user_info, greet],\n    state_schema=CustomState, # [!code highlight]\n    context_schema=CustomContext,\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n    context=CustomContext(user_id=\"user_123\"),\n)\n```\n\n### Prompt\n\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom typing import TypedDict\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass CustomContext(TypedDict):\n    user_name: str\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a city.\"\"\"\n    return f\"The weather in {city} is always sunny!\"\n\n\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context[\"user_name\"]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n    middleware=[dynamic_system_prompt],\n    context_schema=CustomContext,\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    context=CustomContext(user_name=\"John Smith\"),\n)\nfor msg in result[\"messages\"]:\n    msg.pretty_print()\n\n```\n\n```shell title=\"Output\" theme={null}\n================================ Human Message =================================\n\nWhat is the weather in SF?\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)\n Call ID: call_WFQlOGn4b2yoJrv7cih342FG\n  Args:\n    city: San Francisco\n================================= Tool Message =================================\nName: get_weather\n\nThe weather in San Francisco is always sunny!\n================================== Ai Message ==================================\n\nHi John Smith, the weather in San Francisco is always sunny!\n```\n\n### Before model\n\nAccess short term memory (state) in [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) middleware to process messages before model calls.\n\n```mermaid  theme={null}\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    PRE(before_model)\n    MODEL(model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> PRE\n    PRE --> MODEL\n    MODEL -.-> TOOLS\n    MODEL -.-> END\n    TOOLS --> PRE\n    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n    class S blueHighlight;\n    class END blueHighlight;\n```\n\n```python  theme={null}\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver()\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n```\n\n### After model\n\nAccess short term memory (state) in [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model) middleware to process messages after model calls.\n\n```mermaid  theme={null}\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    MODEL(model)\n    POST(after_model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> MODEL\n    MODEL --> POST\n    POST -.-> END\n    POST -.-> TOOLS\n    TOOLS --> MODEL\n    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n    class S blueHighlight;\n    class END blueHighlight;\n    class POST greenHighlight;\n```\n\n```python  theme={null}\nfrom langchain.messages import RemoveMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.runtime import Runtime\n\n\n@after_model\ndef validate_response(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove messages containing sensitive words.\"\"\"\n    STOP_WORDS = [\"password\", \"secret\"]\n    last_message = state[\"messages\"][-1]\n    if any(word in last_message.content for word in STOP_WORDS):\n        return {\"messages\": [RemoveMessage(id=last_message.id)]}\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[],\n    middleware=[validate_response],\n    checkpointer=InMemorySaver(),\n)\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/short-term-memory.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 23676,
    "word_count": 2424
  },
  {
    "title": "Build a SQL agent",
    "source": "https://docs.langchain.com/oss/python/langchain/sql-agent",
    "content": "## Overview\n\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain [agents](/oss/python/langchain/agents).\n\nAt a high level, the agent will:\n\n<Steps>\n  <Step title=\"Fetch the available tables and schemas from the database\" />\n\n  <Step title=\"Decide which tables are relevant to the question\" />\n\n  <Step title=\"Fetch the schemas for the relevant tables\" />\n\n  <Step title=\"Generate a query based on the question and information from the schemas\" />\n\n  <Step title=\"Double-check the query for common mistakes using an LLM\" />\n\n  <Step title=\"Execute the query and return the results\" />\n\n  <Step title=\"Correct mistakes surfaced by the database engine until the query is successful\" />\n\n  <Step title=\"Formulate a response based on the results\" />\n</Steps>\n\n<Warning>\n  Building Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\n</Warning>\n\n### Concepts\n\nWe will cover the following concepts:\n\n* [Tools](/oss/python/langchain/tools) for reading from SQL databases\n* LangChain [agents](/oss/python/langchain/agents)\n* [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) processes\n\n## Setup\n\n### Installation\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain  langgraph  langchain-community\n  ```\n</CodeGroup>\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:\n\n```shell  theme={null}\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n## 1. Select an LLM\n\nSelect a model that supports [tool-calling](/oss/python/integrations/providers/overview):\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n\n    <Tab title=\"HuggingFace\">\n      👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n      ```shell  theme={null}\n      pip install -U \"langchain[huggingface]\"\n      ```\n\n      <CodeGroup>\n        ```python init_chat_model theme={null}\n        import os\n        from langchain.chat_models import init_chat_model\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        model = init_chat_model(\n            \"microsoft/Phi-3-mini-4k-instruct\",\n            model_provider=\"huggingface\",\n            temperature=0.7,\n            max_tokens=1024,\n        )\n        ```\n\n        ```python Model Class theme={null}\n        import os\n        from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        llm = HuggingFaceEndpoint(\n            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n            temperature=0.7,\n            max_length=1024,\n        )\n        model = ChatHuggingFace(llm=llm)\n        ```\n      </CodeGroup>\n    </Tab>\n  </Tab>\n</Tabs>\n\nThe output shown in the examples below used OpenAI.\n\n## 2. Configure the database\n\nYou will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.\n\nFor convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.\n\n```python  theme={null}\nimport requests, pathlib\n\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path = pathlib.Path(\"Chinook.db\")\n\nif local_path.exists():\n    print(f\"{local_path} already exists, skipping download.\")\nelse:\n    response = requests.get(url)\n    if response.status_code == 200:\n        local_path.write_bytes(response.content)\n        print(f\"File downloaded and saved as {local_path}\")\n    else:\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\n```\n\nWe will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python  theme={null}\nfrom langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\nprint(f\"Dialect: {db.dialect}\")\nprint(f\"Available tables: {db.get_usable_table_names()}\")\nprint(f'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}')\n```\n\n```\nDialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n```\n\n## 3. Add tools for database interactions\n\nUse the `SQLDatabase` wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\n\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n```\n\n```\nsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n```\n\n## 4. Use `create_agent`\n\nUse [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) to build a [ReAct agent](https://arxiv.org/pdf/2210.03629) with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\n\nInitialize the agent with a descriptive system prompt to customize its behavior:\n\n```python  theme={null}\nsystem_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n```\n\nNow, create an agent with the model, tools, and prompt:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n)\n```\n\n## 5. Run the agent\n\nRun the agent on a sample query and observe its behavior:\n\n```python  theme={null}\nquestion = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)\n Call ID: call_BQsWg8P65apHc8BTJ1NPDvnM\n  Args:\n================================= Tool Message =================================\nName: sql_db_list_tables\n\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)\n Call ID: call_i89tjKECFSeERbuACYm4w0cU\n  Args:\n    table_names: Track, Genre\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n\t\"GenreId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(120),\n\tPRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(200) NOT NULL,\n\t\"AlbumId\" INTEGER,\n\t\"MediaTypeId\" INTEGER NOT NULL,\n\t\"GenreId\" INTEGER,\n\t\"Composer\" NVARCHAR(220),\n\t\"Milliseconds\" INTEGER NOT NULL,\n\t\"Bytes\" INTEGER,\n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\n\tPRIMARY KEY (\"TrackId\"),\n\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\n\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\n\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)\n Call ID: call_G64yYm6R6UauiVPCXJZMA49b\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query_checker\n\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\n Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nOn average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\n```\n\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\n\n<Note>\n  You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the [LangSmith trace](https://smith.langchain.com/public/cd2ce887-388a-4bb1-a29d-48208ce50d15/r).\n</Note>\n\n### (Optional) Use Studio\n\n[Studio](/langsmith/studio) provides a \"client side\" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \"Tell me the scheme of the database\" or \"Show me the invoices for the 5 top customers\". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\n\n<Accordion title=\"Run your agent in Studio\">\n  In addition to the previously mentioned packages, you will need to:\n\n  ```shell  theme={null}\n  pip install -U langgraph-cli[inmem]>=0.4.0\n  ```\n\n  In directory you will run in, you will need a `langgraph.json` file with the following contents:\n\n  ```json  theme={null}\n  {\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"agent\": \"./sql_agent.py:agent\",\n        \"graph\": \"./sql_agent_langgraph.py:graph\"\n    },\n    \"env\": \".env\"\n  }\n  ```\n\n  Create a file `sql_agent.py` and insert this:\n\n  ```python  theme={null}\n  #sql_agent.py for studio\n  import pathlib\n\n  from langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain_community.agent_toolkits import SQLDatabaseToolkit\n  from langchain_community.utilities import SQLDatabase\n  import requests\n\n\n  # Initialize an LLM\n  model = init_chat_model(\"gpt-4.1\")\n\n  # Get the database, store it locally\n  url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n  local_path = pathlib.Path(\"Chinook.db\")\n\n  if local_path.exists():\n      print(f\"{local_path} already exists, skipping download.\")\n  else:\n      response = requests.get(url)\n      if response.status_code == 200:\n          local_path.write_bytes(response.content)\n          print(f\"File downloaded and saved as {local_path}\")\n      else:\n          print(f\"Failed to download the file. Status code: {response.status_code}\")\n\n  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\n  # Create the tools\n  toolkit = SQLDatabaseToolkit(db=db, llm=model)\n\n  tools = toolkit.get_tools()\n\n  for tool in tools:\n      print(f\"{tool.name}: {tool.description}\\n\")\n\n  # Use create_agent\n  system_prompt = \"\"\"\n  You are an agent designed to interact with a SQL database.\n  Given an input question, create a syntactically correct {dialect} query to run,\n  then look at the results of the query and return the answer. Unless the user\n  specifies a specific number of examples they wish to obtain, always limit your\n  query to at most {top_k} results.\n\n  You can order the results by a relevant column to return the most interesting\n  examples in the database. Never query for all the columns from a specific table,\n  only ask for the relevant columns given the question.\n\n  You MUST double check your query before executing it. If you get an error while\n  executing a query, rewrite the query and try again.\n\n  DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\n  database.\n\n  To start you should ALWAYS look at the tables in the database to see what you\n  can query. Do NOT skip this step.\n\n  Then you should query the schema of the most relevant tables.\n  \"\"\".format(\n      dialect=db.dialect,\n      top_k=5,\n  )\n\n  agent = create_agent(\n      model,\n      tools,\n      system_prompt=system_prompt,\n  )\n  ```\n</Accordion>\n\n## 6. Implement human-in-the-loop review\n\nIt can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies.\n\nLangChain agents feature support for built-in [human-in-the-loop middleware](/oss/python/langchain/human-in-the-loop) to add oversight to agent tool calls. Let's configure the agent to pause for human review on calling the `sql_db_query` tool:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"sql_db_query\": True}, # [!code highlight]\n            description_prefix=\"Tool execution pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n    checkpointer=InMemorySaver(), # [!code highlight]\n)\n```\n\n<Note>\n  We've added a [checkpointer](/oss/python/langchain/short-term-memory) to our agent to allow execution to be paused and resumed. See the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for detalis on this as well as available middleware configurations.\n</Note>\n\nOn running the agent, it will now pause for review before executing the `sql_db_query` tool:\n\n```python  theme={null}\nquestion = \"Which genre on average has the longest tracks?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} # [!code highlight]\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    config, # [!code highlight]\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step: # [!code highlight]\n        print(\"INTERRUPTED:\") # [!code highlight]\n        interrupt = step[\"__interrupt__\"][0] # [!code highlight]\n        for request in interrupt.value[\"action_requests\"]: # [!code highlight]\n            print(request[\"description\"]) # [!code highlight]\n    else:\n        pass\n```\n\n```\n...\n\nINTERRUPTED:\nTool execution pending approval\n\nTool: sql_db_query\nArgs: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'}\n```\n\nWe can resume execution, in this case accepting the query, using [Command](/oss/python/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command):\n\n```python  theme={null}\nfrom langgraph.types import Command # [!code highlight]\n\nfor step in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), # [!code highlight]\n    config,\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step:\n        print(\"INTERRUPTED:\")\n        interrupt = step[\"__interrupt__\"][0]\n        for request in interrupt.value[\"action_requests\"]:\n            print(request[\"description\"])\n    else:\n        pass\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\n```\n\nRefer to the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for details.\n\n## Next steps\n\nFor deeper customization, check out [this tutorial](/oss/python/langgraph/sql-agent) for implementing a SQL agent directly using LangGraph primitives.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/sql-agent.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 24358,
    "word_count": 2665
  },
  {
    "title": "Streaming",
    "source": "https://docs.langchain.com/oss/python/langchain/streaming",
    "content": "LangChain implements a streaming system to surface real-time updates.\n\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n\nLangChain's streaming system lets you surface live feedback from agent runs to your application.\n\nWhat's possible with LangChain streaming:\n\n* <Icon icon=\"brain\" size={16} /> [**Stream agent progress**](#agent-progress) — get state updates after each agent step.\n* <Icon icon=\"square-binary\" size={16} /> [**Stream LLM tokens**](#llm-tokens) — stream language model tokens as they're generated.\n* <Icon icon=\"table\" size={16} /> [**Stream custom updates**](#custom-updates) — emit user-defined signals (e.g., `\"Fetched 10/100 records\"`).\n* <Icon icon=\"layer-plus\" size={16} /> [**Stream multiple modes**](#stream-multiple-modes) — choose from `updates` (agent progress), `messages` (LLM tokens + metadata), or `custom` (arbitrary user data).\n\n## Agent progress\n\nTo stream agent progress, use the [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods with `stream_mode=\"updates\"`. This emits an event after every agent step.\n\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n\n* **LLM node**: [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) with tool call requests\n* **Tool node**: [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) with execution result\n* **LLM node**: Final AI response\n\n```python title=\"Streaming agent progress\" theme={null}\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor chunk in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"updates\",\n):\n    for step, data in chunk.items():\n        print(f\"step: {step}\")\n        print(f\"content: {data['messages'][-1].content_blocks}\")\n```\n\n```shell title=\"Output\" theme={null}\nstep: model\ncontent: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]\n\nstep: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\nstep: model\ncontent: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]\n```\n\n## LLM tokens\n\nTo stream tokens as they are produced by the LLM, use `stream_mode=\"messages\"`. Below you can see the output of the agent streaming tool calls and the final response.\n\n```python title=\"Streaming LLM tokens\" theme={null}\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor token, metadata in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"messages\",\n):\n    print(f\"node: {metadata['langgraph_node']}\")\n    print(f\"content: {token.content_blocks}\")\n    print(\"\\n\")\n```\n\n```shell title=\"Output\" expandable theme={null}\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\"}', 'index': 0}]\n\n\nnode: model\ncontent: []\n\n\nnode: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\n\nnode: model\ncontent: []\n\n\nnode: model\ncontent: [{'type': 'text', 'text': 'Here'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ''s'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' what'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' I'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' got'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ':'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' \"'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': \"It's\"}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' always'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' sunny'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' in'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' San'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' Francisco'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': '!\"\\n\\n'}]\n```\n\n## Custom updates\n\nTo stream updates from tools as they are executed, you can use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer).\n\n```python title=\"Streaming custom updates\" theme={null}\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer  # [!code highlight]\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()  # [!code highlight]\n    # stream any arbitrary data\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n)\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"custom\"  # [!code highlight]\n):\n    print(chunk)\n```\n\n```shell title=\"Output\" theme={null}\nLooking up data for city: San Francisco\nAcquired data for city: San Francisco\n```\n\n<Note>\n  If you add [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.\n</Note>\n\n## Stream multiple modes\n\nYou can specify multiple streaming modes by passing stream mode as a list: `stream_mode=[\"updates\", \"custom\"]`:\n\n```python title=\"Streaming multiple modes\" theme={null}\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\n\nfor stream_mode, chunk in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=[\"updates\", \"custom\"]\n):\n    print(f\"stream_mode: {stream_mode}\")\n    print(f\"content: {chunk}\")\n    print(\"\\n\")\n```\n\n```shell title=\"Output\" theme={null}\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 132, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}\n\n\nstream_mode: custom\ncontent: Looking up data for city: San Francisco\n\n\nstream_mode: custom\ncontent: Acquired data for city: San Francisco\n\n\nstream_mode: updates\ncontent: {'tools': {'messages': [ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', tool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r')]}}\n\n\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\\n\\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}\n```\n\n## Disable streaming\n\nIn some applications you might need to disable streaming of individual tokens for a given model.\n\nThis is useful in [multi-agent](/oss/python/langchain/multi-agent) systems to control which agents stream their output.\n\nSee the [Models](/oss/python/langchain/models#disable-streaming) guide to learn how to disable streaming.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/streaming.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 10530,
    "word_count": 1093
  },
  {
    "title": "Structured output",
    "source": "https://docs.langchain.com/oss/python/langchain/structured-output",
    "content": "Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.\n\nLangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `'structured_response'` key of the agent's state.\n\n```python  theme={null}\ndef create_agent(\n    ...\n    response_format: Union[\n        ToolStrategy[StructuredResponseT],\n        ProviderStrategy[StructuredResponseT],\n        type[StructuredResponseT],\n    ]\n```\n\n## Response Format\n\nControls how the agent returns structured data:\n\n* **`ToolStrategy[StructuredResponseT]`**: Uses tool calling for structured output\n* **`ProviderStrategy[StructuredResponseT]`**: Uses provider-native structured output\n* **`type[StructuredResponseT]`**: Schema type - automatically selects best strategy based on model capabilities\n* **`None`**: No structured output\n\nWhen a schema type is provided directly, LangChain automatically chooses:\n\n* `ProviderStrategy` for models supporting native structured output (e.g. [OpenAI](/oss/python/integrations/providers/openai), [Anthropic](/oss/python/integrations/providers/anthropic), or [Grok](/oss/python/integrations/providers/xai)).\n* `ToolStrategy` for all other models.\n\n<Tip>\n  Support for native structured output features is read dynamically from the model's [profile data](/oss/python/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:\n\n  ```python  theme={null}\n  custom_profile = {\n      \"structured_output\": True,\n      # ...\n  }\n  model = init_chat_model(\"...\", profile=custom_profile)\n  ```\n\n  If tools are specified, the model must support simultaneous use of tools and structured output.\n</Tip>\n\nThe structured response is returned in the `structured_response` key of the agent's final state.\n\n## Provider strategy\n\nSome model providers support structured output natively through their APIs (e.g. OpenAI, Grok, Gemini). This is the most reliable method when available.\n\nTo use this strategy, configure a `ProviderStrategy`:\n\n```python  theme={null}\nclass ProviderStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n```\n\n<ParamField path=\"schema\" required>\n  The schema defining the structured output format. Supports:\n\n  * **Pydantic models**: `BaseModel` subclasses with field validation\n  * **Dataclasses**: Python dataclasses with type annotations\n  * **TypedDict**: Typed dictionary classes\n  * **JSON Schema**: Dictionary with JSON schema specification\n</ParamField>\n\nLangChain automatically uses `ProviderStrategy` when you pass a schema type directly to [`create_agent.response_format`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(response_format\\)) and the model supports native structured output:\n\n<CodeGroup>\n  ```python Pydantic Model theme={null}\n  from pydantic import BaseModel, Field\n  from langchain.agents import create_agent\n\n\n  class ContactInfo(BaseModel):\n      \"\"\"Contact information for a person.\"\"\"\n      name: str = Field(description=\"The name of the person\")\n      email: str = Field(description=\"The email address of the person\")\n      phone: str = Field(description=\"The phone number of the person\")\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      response_format=ContactInfo  # Auto-selects ProviderStrategy\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n  })\n\n  print(result[\"structured_response\"])\n  # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n  ```\n\n  ```python Dataclass theme={null}\n  from dataclasses import dataclass\n  from langchain.agents import create_agent\n\n\n  @dataclass\n  class ContactInfo:\n      \"\"\"Contact information for a person.\"\"\"\n      name: str # The name of the person\n      email: str # The email address of the person\n      phone: str # The phone number of the person\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ContactInfo  # Auto-selects ProviderStrategy\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n  })\n\n  result[\"structured_response\"]\n  # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n  ```\n\n  ```python TypedDict theme={null}\n  from typing_extensions import TypedDict\n  from langchain.agents import create_agent\n\n\n  class ContactInfo(TypedDict):\n      \"\"\"Contact information for a person.\"\"\"\n      name: str # The name of the person\n      email: str # The email address of the person\n      phone: str # The phone number of the person\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ContactInfo  # Auto-selects ProviderStrategy\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n  })\n\n  result[\"structured_response\"]\n  # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'}\n  ```\n\n  ```python JSON Schema theme={null}\n  from langchain.agents import create_agent\n\n\n  contact_info_schema = {\n      \"type\": \"object\",\n      \"description\": \"Contact information for a person.\",\n      \"properties\": {\n          \"name\": {\"type\": \"string\", \"description\": \"The name of the person\"},\n          \"email\": {\"type\": \"string\", \"description\": \"The email address of the person\"},\n          \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the person\"}\n      },\n      \"required\": [\"name\", \"email\", \"phone\"]\n  }\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ProviderStrategy(contact_info_schema)\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n  })\n\n  result[\"structured_response\"]\n  # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'}\n  ```\n</CodeGroup>\n\nProvider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.\n\n<Note>\n  If the provider natively supports structured output for your model choice, it is functionally equivalent to write `response_format=ProductReview` instead of `response_format=ProviderStrategy(ProductReview)`. In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.\n</Note>\n\n## Tool calling strategy\n\nFor models that don't support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.\n\nTo use this strategy, configure a `ToolStrategy`:\n\n```python  theme={null}\nclass ToolStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n    tool_message_content: str | None\n    handle_errors: Union[\n        bool,\n        str,\n        type[Exception],\n        tuple[type[Exception], ...],\n        Callable[[Exception], str],\n    ]\n```\n\n<ParamField path=\"schema\" required>\n  The schema defining the structured output format. Supports:\n\n  * **Pydantic models**: `BaseModel` subclasses with field validation\n  * **Dataclasses**: Python dataclasses with type annotations\n  * **TypedDict**: Typed dictionary classes\n  * **JSON Schema**: Dictionary with JSON schema specification\n  * **Union types**: Multiple schema options. The model will choose the most appropriate schema based on the context.\n</ParamField>\n\n<ParamField path=\"tool_message_content\">\n  Custom content for the tool message returned when structured output is generated.\n  If not provided, defaults to a message showing the structured response data.\n</ParamField>\n\n<ParamField path=\"handle_errors\">\n  Error handling strategy for structured output validation failures. Defaults to `True`.\n\n  * **`True`**: Catch all errors with default error template\n  * **`str`**: Catch all errors with this custom message\n  * **`type[Exception]`**: Only catch this exception type with default message\n  * **`tuple[type[Exception], ...]`**: Only catch these exception types with default message\n  * **`Callable[[Exception], str]`**: Custom function that returns error message\n  * **`False`**: No retry, let exceptions propagate\n</ParamField>\n\n<CodeGroup>\n  ```python Pydantic Model theme={null}\n  from pydantic import BaseModel, Field\n  from typing import Literal\n  from langchain.agents import create_agent\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  class ProductReview(BaseModel):\n      \"\"\"Analysis of a product review.\"\"\"\n      rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5)\n      sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n      key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ToolStrategy(ProductReview)\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n  })\n  result[\"structured_response\"]\n  # ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n  ```\n\n  ```python Dataclass theme={null}\n  from dataclasses import dataclass\n  from typing import Literal\n  from langchain.agents import create_agent\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  @dataclass\n  class ProductReview:\n      \"\"\"Analysis of a product review.\"\"\"\n      rating: int | None  # The rating of the product (1-5)\n      sentiment: Literal[\"positive\", \"negative\"]  # The sentiment of the review\n      key_points: list[str]  # The key points of the review\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ToolStrategy(ProductReview)\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n  })\n  result[\"structured_response\"]\n  # ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n  ```\n\n  ```python TypedDict theme={null}\n  from typing import Literal\n  from typing_extensions import TypedDict\n  from langchain.agents import create_agent\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  class ProductReview(TypedDict):\n      \"\"\"Analysis of a product review.\"\"\"\n      rating: int | None  # The rating of the product (1-5)\n      sentiment: Literal[\"positive\", \"negative\"]  # The sentiment of the review\n      key_points: list[str]  # The key points of the review\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ToolStrategy(ProductReview)\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n  })\n  result[\"structured_response\"]\n  # {'rating': 5, 'sentiment': 'positive', 'key_points': ['fast shipping', 'expensive']}\n  ```\n\n  ```python JSON Schema theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  product_review_schema = {\n      \"type\": \"object\",\n      \"description\": \"Analysis of a product review.\",\n      \"properties\": {\n          \"rating\": {\n              \"type\": [\"integer\", \"null\"],\n              \"description\": \"The rating of the product (1-5)\",\n              \"minimum\": 1,\n              \"maximum\": 5\n          },\n          \"sentiment\": {\n              \"type\": \"string\",\n              \"enum\": [\"positive\", \"negative\"],\n              \"description\": \"The sentiment of the review\"\n          },\n          \"key_points\": {\n              \"type\": \"array\",\n              \"items\": {\"type\": \"string\"},\n              \"description\": \"The key points of the review\"\n          }\n      },\n      \"required\": [\"sentiment\", \"key_points\"]\n  }\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ToolStrategy(product_review_schema)\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n  })\n  result[\"structured_response\"]\n  # {'rating': 5, 'sentiment': 'positive', 'key_points': ['fast shipping', 'expensive']}\n  ```\n\n  ```python Union Types theme={null}\n  from pydantic import BaseModel, Field\n  from typing import Literal, Union\n  from langchain.agents import create_agent\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  class ProductReview(BaseModel):\n      \"\"\"Analysis of a product review.\"\"\"\n      rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5)\n      sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n      key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n\n  class CustomerComplaint(BaseModel):\n      \"\"\"A customer complaint about a product or service.\"\"\"\n      issue_type: Literal[\"product\", \"service\", \"shipping\", \"billing\"] = Field(description=\"The type of issue\")\n      severity: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"The severity of the complaint\")\n      description: str = Field(description=\"Brief description of the complaint\")\n\n  agent = create_agent(\n      model=\"gpt-5\",\n      tools=tools,\n      response_format=ToolStrategy(Union[ProductReview, CustomerComplaint])\n  )\n\n  result = agent.invoke({\n      \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n  })\n  result[\"structured_response\"]\n  # ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n  ```\n</CodeGroup>\n\n### Custom tool message content\n\nThe `tool_message_content` parameter allows you to customize the message that appears in the conversation history when structured output is generated:\n\n```python  theme={null}\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass MeetingAction(BaseModel):\n    \"\"\"Action items extracted from a meeting transcript.\"\"\"\n    task: str = Field(description=\"The specific task to be completed\")\n    assignee: str = Field(description=\"Person responsible for the task\")\n    priority: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Priority level\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(\n        schema=MeetingAction,\n        tool_message_content=\"Action item captured and added to meeting notes!\"\n    )\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n})\n```\n\n```\n================================ Human Message =================================\n\nFrom our meeting: Sarah needs to update the project timeline as soon as possible\n================================== Ai Message ==================================\nTool Calls:\n  MeetingAction (call_1)\n Call ID: call_1\n  Args:\n    task: Update the project timeline\n    assignee: Sarah\n    priority: high\n================================= Tool Message =================================\nName: MeetingAction\n\nAction item captured and added to meeting notes!\n```\n\nWithout `tool_message_content`, our final [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) would be:\n\n```\n================================= Tool Message =================================\nName: MeetingAction\n\nReturning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}\n```\n\n### Error handling\n\nModels can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.\n\n#### Multiple structured outputs error\n\nWhen a model incorrectly calls multiple structured output tools, the agent provides error feedback in a [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) and prompts the model to retry:\n\n```python  theme={null}\nfrom pydantic import BaseModel, Field\nfrom typing import Union\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str = Field(description=\"Person's name\")\n    email: str = Field(description=\"Email address\")\n\nclass EventDetails(BaseModel):\n    event_name: str = Field(description=\"Name of the event\")\n    date: str = Field(description=\"Event date\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(Union[ContactInfo, EventDetails])  # Default: handle_errors=True\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n```\n\n```\n================================ Human Message =================================\n\nExtract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\nNone\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_1)\n Call ID: call_1\n  Args:\n    name: John Doe\n    email: john@email.com\n  EventDetails (call_2)\n Call ID: call_2\n  Args:\n    event_name: Tech Conference\n    date: March 15th\n================================= Tool Message =================================\nName: ContactInfo\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================= Tool Message =================================\nName: EventDetails\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_3)\n Call ID: call_3\n  Args:\n    name: John Doe\n    email: john@email.com\n================================= Tool Message =================================\nName: ContactInfo\n\nReturning structured response: {'name': 'John Doe', 'email': 'john@email.com'}\n```\n\n#### Schema validation error\n\nWhen structured output doesn't match the expected schema, the agent provides specific error feedback:\n\n```python  theme={null}\nfrom pydantic import BaseModel, Field\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ProductRating(BaseModel):\n    rating: int | None = Field(description=\"Rating from 1-5\", ge=1, le=5)\n    comment: str = Field(description=\"Review comment\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True\n    system_prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\"\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Parse this: Amazing product, 10/10!\"}]\n})\n```\n\n```\n================================ Human Message =================================\n\nParse this: Amazing product, 10/10!\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_1)\n Call ID: call_1\n  Args:\n    rating: 10\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nError: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_2)\n Call ID: call_2\n  Args:\n    rating: 5\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nReturning structured response: {'rating': 5, 'comment': 'Amazing product'}\n```\n\n#### Error handling strategies\n\nYou can customize how errors are handled using the `handle_errors` parameter:\n\n**Custom error message:**\n\n```python  theme={null}\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=\"Please provide a valid rating between 1-5 and include a comment.\"\n)\n```\n\nIf `handle_errors` is a string, the agent will *always* prompt the model to re-try with a fixed tool message:\n\n```\n================================= Tool Message =================================\nName: ProductRating\n\nPlease provide a valid rating between 1-5 and include a comment.\n```\n\n**Handle specific exceptions only:**\n\n```python  theme={null}\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=ValueError  # Only retry on ValueError, raise others\n)\n```\n\nIf `handle_errors` is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.\n\n**Handle multiple exception types:**\n\n```python  theme={null}\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\n)\n```\n\nIf `handle_errors` is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.\n\n**Custom error handler function:**\n\n```python  theme={null}\n\nfrom langchain.agents.structured_output import StructuredOutputValidationError\nfrom langchain.agents.structured_output import MultipleStructuredOutputsError\n\ndef custom_error_handler(error: Exception) -> str:\n    if isinstance(error, StructuredOutputValidationError):\n        return \"There was an issue with the format. Try again.\n    elif isinstance(error, MultipleStructuredOutputsError):\n        return \"Multiple structured outputs were returned. Pick the most relevant one.\"\n    else:\n        return f\"Error: {str(error)}\"\n\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(\n                        schema=Union[ContactInfo, EventDetails],\n                        handle_errors=custom_error_handler\n                    )  # Default: handle_errors=True\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n\nfor msg in result['messages']:\n    # If message is actually a ToolMessage object (not a dict), check its class name\n    if type(msg).__name__ == \"ToolMessage\":\n        print(msg.content)\n    # If message is a dictionary or you want a fallback\n    elif isinstance(msg, dict) and msg.get('tool_call_id'):\n        print(msg['content'])\n\n```\n\nOn `StructuredOutputValidationError`:\n\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nThere was an issue with the format. Try again.\n```\n\nOn `MultipleStructuredOutputsError`:\n\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nMultiple structured outputs were returned. Pick the most relevant one.\n```\n\nOn other errors:\n\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nError: <error message>\n```\n\n**No error handling:**\n\n```python  theme={null}\nresponse_format = ToolStrategy(\n    schema=ProductRating,\n    handle_errors=False  # All errors raised\n)\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/structured-output.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 24477,
    "word_count": 2526
  },
  {
    "title": "LangSmith Studio",
    "source": "https://docs.langchain.com/oss/python/langchain/studio",
    "content": "When building agents with LangChain locally, it's helpful to visualize what's happening inside your agent, interact with it in real-time, and debug issues as they occur. **LangSmith Studio** is a free visual interface for developing and testing your LangChain agents from your local machine.\n\nStudio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent's behavior without additional code or deployment.\n\nThis pages describes how to set up Studio with your local LangChain agent.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n* **A LangSmith account**: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com).\n* **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.\n* If you don't want data [traced](/langsmith/observability-concepts#traces) to LangSmith, set `LANGSMITH_TRACING=false` in your application's `.env` file. With tracing disabled, no data leaves your local server.\n\n## Set up local Agent server\n\n### 1. Install the LangGraph CLI\n\nThe [LangGraph CLI](/langsmith/cli) provides a local development server (also called [Agent Server](/langsmith/agent-server)) that connects your agent to Studio.\n\n```shell  theme={null}\n# Python >= 3.11 is required.\npip install --upgrade \"langgraph-cli[inmem]\"\n```\n\n### 2. Prepare your agent\n\nIf you already have a LangChain agent, you can use it directly. This example uses a simple email agent:\n\n```python title=\"agent.py\" theme={null}\nfrom langchain.agents import create_agent\n\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email\"\"\"\n    email = {\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body\n    }\n    # ... email sending logic\n\n    return f\"Email sent to {to}\"\n\nagent = create_agent(\n    \"gpt-4o\",\n    tools=[send_email],\n    system_prompt=\"You are an email assistant. Always use the send_email tool.\",\n)\n```\n\n### 3. Environment variables\n\nStudio requires a LangSmith API key to connect your local agent. Create a `.env` file in the root of your project and add your API key from [LangSmith](https://smith.langchain.com/settings).\n\n<Warning>\n  Ensure your `.env` file is not committed to version control, such as Git.\n</Warning>\n\n```bash .env theme={null}\nLANGSMITH_API_KEY=lsv2...\n```\n\n### 4. Create a LangGraph config file\n\nThe LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a `langgraph.json` file in your app's directory:\n\n```json title=\"langgraph.json\" theme={null}\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./src/agent.py:agent\"\n  },\n  \"env\": \".env\"\n}\n```\n\nThe [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) function automatically returns a compiled LangGraph graph, which is what the `graphs` key expects in the configuration file.\n\n<Info>\n  For detailed explanations of each key in the JSON object of the configuration file, refer to the [LangGraph configuration file reference](/langsmith/cli#configuration-file).\n</Info>\n\nAt this point, the project structure will look like this:\n\n```bash  theme={null}\nmy-app/\n├── src\n│   └── agent.py\n├── .env\n└── langgraph.json\n```\n\n### 5. Install dependencies\n\nInstall your project dependencies from the root directory:\n\n<CodeGroup>\n  ```shell pip theme={null}\n  pip install -e .\n  ```\n\n  ```shell uv theme={null}\n  uv sync\n  ```\n</CodeGroup>\n\n### 6. View your agent in Studio\n\nStart the development server to connect your agent to Studio:\n\n```shell  theme={null}\nlanggraph dev\n```\n\n<Warning>\n  Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.\n</Warning>\n\nOnce the server is running, your agent is accessible both via API at `http://127.0.0.1:2024` and through the Studio UI at `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`:\n\n<Frame>\n    <img src=\"https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=ebd259e9fa24af7d011dfcc568f74be2\" alt=\"Agent view in the Studio UI\" data-og-width=\"2836\" width=\"2836\" data-og-height=\"1752\" height=\"1752\" data-path=\"oss/images/studio_create-agent.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=280&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=cf9c05bdd08661d4d546c540c7a28cbe 280w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=560&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=484b2fd56957d048bd89280ce97065a0 560w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=840&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=92991302ac24604022ab82ac22729f68 840w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=1100&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=ed366abe8dabc42a9d7c300a591e1614 1100w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=1650&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=d5865d3c4b0d26e9d72e50d474547a63 1650w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=2500&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=6b254add2df9cc3c10ac0c2bcb3a589c 2500w\" />\n</Frame>\n\nWith Studio connected to your local agent, you can iterate quickly on your agent's behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened.\n\nThe development server supports hot-reloading—make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs.\n\nFor more information on how to run Studio, refer to the following guides in the [LangSmith docs](/langsmith/home):\n\n* [Run application](/langsmith/use-studio#run-application)\n* [Manage assistants](/langsmith/use-studio#manage-assistants)\n* [Manage threads](/langsmith/use-studio#manage-threads)\n* [Iterate on prompts](/langsmith/observability-studio)\n* [Debug LangSmith traces](/langsmith/observability-studio#debug-langsmith-traces)\n* [Add node to dataset](/langsmith/observability-studio#add-node-to-dataset)\n\n## Video guide\n\n<Frame>\n  <iframe className=\"w-full aspect-video rounded-xl\" src=\"https://www.youtube.com/embed/Mi1gSlHwZLM?si=zA47TNuTC5aH0ahd\" title=\"Studio\" frameBorder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowFullScreen />\n</Frame>\n\n<Tip>\n  For more information about local and deployed agents, see [Set up local Agent Server](/oss/python/langchain/studio#setup-local-agent-server) and [Deploy](/oss/python/langchain/deploy).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/studio.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 7617,
    "word_count": 776
  },
  {
    "title": "Build a supervisor agent",
    "source": "https://docs.langchain.com/oss/python/langchain/supervisor",
    "content": "## Overview\n\nThe **supervisor pattern** is a [multi-agent](/oss/python/langchain/multi-agent) architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\n\nIn this tutorial, you'll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\n\n* A **calendar agent** that handles scheduling, availability checking, and event management.\n* An **email agent** that manages communication, drafts messages, and sends notifications.\n\nWe will also incorporate [human-in-the-loop review](/oss/python/langchain/human-in-the-loop) to allow users to approve, edit, and reject actions (such as outbound emails) as desired.\n\n### Why use a supervisor?\n\nMulti-agent architectures allow you to partition [tools](/oss/python/langchain/tools) across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\n\n### Concepts\n\nWe will cover the following concepts:\n\n* [Multi-agent systems](/oss/python/langchain/multi-agent)\n* [Human-in-the-loop review](/oss/python/langchain/human-in-the-loop)\n\n## Setup\n\n### Installation\n\nThis tutorial requires the `langchain` package:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain\n  ```\n\n  ```bash conda theme={null}\n  conda install langchain -c conda-forge\n  ```\n</CodeGroup>\n\nFor more details, see our [Installation guide](/oss/python/langchain/install).\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:\n\n<CodeGroup>\n  ```bash bash theme={null}\n  export LANGSMITH_TRACING=\"true\"\n  export LANGSMITH_API_KEY=\"...\"\n  ```\n\n  ```python python theme={null}\n  import getpass\n  import os\n\n  os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n  os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n  ```\n</CodeGroup>\n\n### Components\n\nWe will need to select a chat model from LangChain's suite of integrations:\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n\n    <Tab title=\"HuggingFace\">\n      👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n      ```shell  theme={null}\n      pip install -U \"langchain[huggingface]\"\n      ```\n\n      <CodeGroup>\n        ```python init_chat_model theme={null}\n        import os\n        from langchain.chat_models import init_chat_model\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        model = init_chat_model(\n            \"microsoft/Phi-3-mini-4k-instruct\",\n            model_provider=\"huggingface\",\n            temperature=0.7,\n            max_tokens=1024,\n        )\n        ```\n\n        ```python Model Class theme={null}\n        import os\n        from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        llm = HuggingFaceEndpoint(\n            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n            temperature=0.7,\n            max_length=1024,\n        )\n        model = ChatHuggingFace(llm=llm)\n        ```\n      </CodeGroup>\n    </Tab>\n  </Tab>\n</Tabs>\n\n## 1. Define tools\n\nStart by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, you'll use stubs to demonstrate the pattern.\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # email addresses\n    location: str = \"\"\n) -> str:\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n\n\n@tool\ndef send_email(\n    to: list[str],  # email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = []\n) -> str:\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -> list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    # Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"]\n```\n\n## 2. Create specialized sub-agents\n\nNext, we'll create specialized sub-agents that handle each domain.\n\n### Create a calendar agent\n\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant. \"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n    \"into proper ISO datetime formats. \"\n    \"Use get_available_time_slots to check availability when needed. \"\n    \"Use create_calendar_event to schedule events. \"\n    \"Always confirm what was scheduled in your final response.\"\n)\n\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n)\n```\n\nTest the calendar agent to see how it handles natural language scheduling:\n\n```python  theme={null}\nquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\n\nfor step in calendar_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  get_available_time_slots (call_EIeoeIi1hE2VmwZSfHStGmXp)\n Call ID: call_EIeoeIi1hE2VmwZSfHStGmXp\n  Args:\n    attendees: []\n    date: 2024-06-18\n    duration_minutes: 60\n================================= Tool Message =================================\nName: get_available_time_slots\n\n[\"09:00\", \"14:00\", \"16:00\"]\n================================== Ai Message ==================================\nTool Calls:\n  create_calendar_event (call_zgx3iJA66Ut0W8S3NpT93kEB)\n Call ID: call_zgx3iJA66Ut0W8S3NpT93kEB\n  Args:\n    title: Team Meeting\n    start_time: 2024-06-18T14:00:00\n    end_time: 2024-06-18T15:00:00\n    attendees: []\n================================= Tool Message =================================\nName: create_calendar_event\n\nEvent created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees\n================================== Ai Message ==================================\n\nThe team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!\n```\n\nThe agent parses \"next Tuesday at 2pm\" into ISO format (\"2024-01-16T14:00:00\"), calculates the end time, calls `create_calendar_event`, and returns a natural language confirmation.\n\n### Create an email agent\n\nThe email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\n\n```python  theme={null}\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant. \"\n    \"Compose professional emails based on natural language requests. \"\n    \"Extract recipient information and craft appropriate subject lines and body text. \"\n    \"Use send_email to send the message. \"\n    \"Always confirm what was sent in your final response.\"\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n)\n```\n\nTest the email agent with a natural language request:\n\n```python  theme={null}\nquery = \"Send the design team a reminder about reviewing the new mockups\"\n\nfor step in email_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  send_email (call_OMl51FziTVY6CRZvzYfjYOZr)\n Call ID: call_OMl51FziTVY6CRZvzYfjYOZr\n  Args:\n    to: ['design-team@example.com']\n    subject: Reminder: Please Review the New Mockups\n    body: Hi Design Team,\n\nThis is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline.\n\nPlease let me know if you have any questions or need additional information.\n\nThank you!\n\nBest regards,\n================================= Tool Message =================================\nName: send_email\n\nEmail sent to design-team@example.com - Subject: Reminder: Please Review the New Mockups\n================================== Ai Message ==================================\n\nI've sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know!\n```\n\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls `send_email`, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.\n\n## 3. Wrap sub-agents as tools\n\nNow wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like \"schedule\\_event\", not low-level tools like \"create\\_calendar\\_event\".\n\n```python  theme={null}\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\n\n    Use this when the user wants to create, modify, or check calendar appointments.\n    Handles date/time parsing, availability checking, and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str) -> str:\n    \"\"\"Send emails using natural language.\n\n    Use this when the user wants to send notifications, reminders, or any email\n    communication. Handles recipient extraction, subject generation, and email\n    composition.\n\n    Input: Natural language email request (e.g., 'send them a reminder about\n    the meeting')\n    \"\"\"\n    result = email_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n```\n\nThe tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agent's final response, as the supervisor doesn't need to see intermediate reasoning or tool calls.\n\n## 4. Create the supervisor agent\n\nNow create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\n\n```python  theme={null}\nSUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can schedule calendar events and send emails. \"\n    \"Break down user requests into appropriate tool calls and coordinate the results. \"\n    \"When a request involves multiple actions, use multiple tools in sequence.\"\n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n```\n\n## 5. Use the supervisor\n\nNow test your complete system with complex requests that require coordination across multiple domains:\n\n### Example 1: Simple single-domain request\n\n```python  theme={null}\nquery = \"Schedule a team standup for tomorrow at 9am\"\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_mXFJJDU8bKZadNUZPaag8Lct)\n Call ID: call_mXFJJDU8bKZadNUZPaag8Lct\n  Args:\n    request: Schedule a team standup for tomorrow at 9am with Alice and Bob.\n================================= Tool Message =================================\nName: schedule_event\n\nThe team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know!\n================================== Ai Message ==================================\n\nThe team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know!\n```\n\nThe supervisor identifies this as a calendar task, calls `schedule_event`, and the calendar agent handles date parsing and event creation.\n\n<Tip>\n  For full transparency into the information flow, including prompts and responses for each chat model call, check out the [LangSmith trace](https://smith.langchain.com/public/91a9a95f-fba9-4e84-aff0-371861ad2f4a/r) for the above run.\n</Tip>\n\n### Example 2: Complex multi-domain request\n\n```python  theme={null}\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_YA68mqF0koZItCFPx0kGQfZi)\n Call ID: call_YA68mqF0koZItCFPx0kGQfZi\n  Args:\n    request: meeting with the design team next Tuesday at 2pm for 1 hour\n  manage_email (call_XxqcJBvVIuKuRK794ZIzlLxx)\n Call ID: call_XxqcJBvVIuKuRK794ZIzlLxx\n  Args:\n    request: send the design team an email reminder about reviewing the new mockups\n================================= Tool Message =================================\nName: schedule_event\n\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes!\n================================= Tool Message =================================\nName: manage_email\n\nI've sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know!\n================================== Ai Message ==================================\n\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.\n\nI've also sent an email reminder to the design team, asking them to review the new mockups.\n\nLet me know if you'd like to add more details to the meeting or include additional information in the email!\n```\n\nThe supervisor recognizes this requires both calendar and email actions, calls `schedule_event` for the meeting, then calls `manage_email` for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\n\n<Tip>\n  Refer to the [LangSmith trace](https://smith.langchain.com/public/95cd00a3-d1f9-4dba-9731-7bf733fb6a3c/r) to see the detailed information flow for the above run, including individual chat model prompts and responses.\n</Tip>\n\n### Complete working example\n\nHere's everything together in a runnable script:\n\n<Expandable title=\"View complete code\" defaultOpen={false}>\n  ```python  theme={null}\n  \"\"\"\n  Personal Assistant Supervisor Example\n\n  This example demonstrates the tool calling pattern for multi-agent systems.\n  A supervisor agent coordinates specialized sub-agents (calendar and email)\n  that are wrapped as tools.\n  \"\"\"\n\n  from langchain.tools import tool\n  from langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n\n  # ============================================================================\n  # Step 1: Define low-level API tools (stubbed)\n  # ============================================================================\n\n  @tool\n  def create_calendar_event(\n      title: str,\n      start_time: str,  # ISO format: \"2024-01-15T14:00:00\"\n      end_time: str,    # ISO format: \"2024-01-15T15:00:00\"\n      attendees: list[str],  # email addresses\n      location: str = \"\"\n  ) -> str:\n      \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n      return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n\n\n  @tool\n  def send_email(\n      to: list[str],      # email addresses\n      subject: str,\n      body: str,\n      cc: list[str] = []\n  ) -> str:\n      \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n      return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n  @tool\n  def get_available_time_slots(\n      attendees: list[str],\n      date: str,  # ISO format: \"2024-01-15\"\n      duration_minutes: int\n  ) -> list[str]:\n      \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n      return [\"09:00\", \"14:00\", \"16:00\"]\n\n\n  # ============================================================================\n  # Step 2: Create specialized sub-agents\n  # ============================================================================\n\n  model = init_chat_model(\"claude-haiku-4-5-20251001\")  # for example\n\n  calendar_agent = create_agent(\n      model,\n      tools=[create_calendar_event, get_available_time_slots],\n      system_prompt=(\n          \"You are a calendar scheduling assistant. \"\n          \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n          \"into proper ISO datetime formats. \"\n          \"Use get_available_time_slots to check availability when needed. \"\n          \"Use create_calendar_event to schedule events. \"\n          \"Always confirm what was scheduled in your final response.\"\n      )\n  )\n\n  email_agent = create_agent(\n      model,\n      tools=[send_email],\n      system_prompt=(\n          \"You are an email assistant. \"\n          \"Compose professional emails based on natural language requests. \"\n          \"Extract recipient information and craft appropriate subject lines and body text. \"\n          \"Use send_email to send the message. \"\n          \"Always confirm what was sent in your final response.\"\n      )\n  )\n\n  # ============================================================================\n  # Step 3: Wrap sub-agents as tools for the supervisor\n  # ============================================================================\n\n  @tool\n  def schedule_event(request: str) -> str:\n      \"\"\"Schedule calendar events using natural language.\n\n      Use this when the user wants to create, modify, or check calendar appointments.\n      Handles date/time parsing, availability checking, and event creation.\n\n      Input: Natural language scheduling request (e.g., 'meeting with design team\n      next Tuesday at 2pm')\n      \"\"\"\n      result = calendar_agent.invoke({\n          \"messages\": [{\"role\": \"user\", \"content\": request}]\n      })\n      return result[\"messages\"][-1].text\n\n\n  @tool\n  def manage_email(request: str) -> str:\n      \"\"\"Send emails using natural language.\n\n      Use this when the user wants to send notifications, reminders, or any email\n      communication. Handles recipient extraction, subject generation, and email\n      composition.\n\n      Input: Natural language email request (e.g., 'send them a reminder about\n      the meeting')\n      \"\"\"\n      result = email_agent.invoke({\n          \"messages\": [{\"role\": \"user\", \"content\": request}]\n      })\n      return result[\"messages\"][-1].text\n\n\n  # ============================================================================\n  # Step 4: Create the supervisor agent\n  # ============================================================================\n\n  supervisor_agent = create_agent(\n      model,\n      tools=[schedule_event, manage_email],\n      system_prompt=(\n          \"You are a helpful personal assistant. \"\n          \"You can schedule calendar events and send emails. \"\n          \"Break down user requests into appropriate tool calls and coordinate the results. \"\n          \"When a request involves multiple actions, use multiple tools in sequence.\"\n      )\n  )\n\n  # ============================================================================\n  # Step 5: Use the supervisor\n  # ============================================================================\n\n  if __name__ == \"__main__\":\n      # Example: User request requiring both calendar and email coordination\n      user_request = (\n          \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n          \"and send them an email reminder about reviewing the new mockups.\"\n      )\n\n      print(\"User Request:\", user_request)\n      print(\"\\n\" + \"=\"*80 + \"\\n\")\n\n      for step in supervisor_agent.stream(\n          {\"messages\": [{\"role\": \"user\", \"content\": user_request}]}\n      ):\n          for update in step.values():\n              for message in update.get(\"messages\", []):\n                  message.pretty_print()\n  ```\n</Expandable>\n\n### Understanding the architecture\n\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\n\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\n\n## 6. Add human-in-the-loop review\n\nIt can be prudent to incorporate [human-in-the-loop review](/oss/python/langchain/human-in-the-loop) of sensitive actions. LangChain includes [built-in middleware](/oss/python/langchain/human-in-the-loop#configuring-interrupts) to review tool calls, in this case the tools invoked by sub-agents.\n\nLet's add human-in-the-loop review to both sub-agents:\n\n* We configure the `create_calendar_event` and `send_email` tools to interrupt, permitting all [response types](/oss/python/langchain/human-in-the-loop) (`approve`, `edit`, `reject`)\n* We add a [checkpointer](/oss/python/langchain/short-term-memory) **only to the top-level agent**. This is required to pause and resume execution.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"create_calendar_event\": True}, # [!code highlight]\n            description_prefix=\"Calendar event pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"send_email\": True}, # [!code highlight]\n            description_prefix=\"Outbound email pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n    checkpointer=InMemorySaver(), # [!code highlight]\n)\n```\n\nLet's repeat the query. Note that we gather interrupt events into a list to access downstream:\n\n```python  theme={null}\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)\n Call ID: call_t4Wyn32ohaShpEZKuzZbl83z\n  Args:\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\n  manage_email (call_JWj4vDJ5VMnvkySymhCBm4IR)\n Call ID: call_JWj4vDJ5VMnvkySymhCBm4IR\n  Args:\n    request: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm.\n\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\n\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\n```\n\nThis time we've interrupted execution. Let's inspect the interrupt events:\n\n```python  theme={null}\nfor interrupt_ in interrupts:\n    for request in interrupt_.value[\"action_requests\"]:\n        print(f\"INTERRUPTED: {interrupt_.id}\")\n        print(f\"{request['description']}\\n\")\n```\n\n```\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\nCalendar event pending approval\n\nTool: create_calendar_event\nArgs: {'title': 'Meeting with the Design Team', 'start_time': '2024-06-18T14:00:00', 'end_time': '2024-06-18T15:00:00', 'attendees': ['design team']}\n\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\nOutbound email pending approval\n\nTool: send_email\nArgs: {'to': ['designteam@example.com'], 'subject': 'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm', 'body': \"Hello Team,\\n\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\n\\nPlease ensure you've gone through the designs and are ready to share your thoughts during the meeting.\\n\\nThank you!\\n\\nBest regards,\\n[Your Name]\"}\n```\n\nWe can specify decisions for each interrupt by referring to its ID using a [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command). Refer to the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:\n\n```python  theme={null}\nfrom langgraph.types import Command # [!code highlight]\n\nresume = {}\nfor interrupt_ in interrupts:\n    if interrupt_.id == \"2b56f299be313ad8bc689eff02973f16\":\n        # Edit email\n        edited_action = interrupt_.value[\"action_requests\"][0].copy()\n        edited_action[\"arguments\"][\"subject\"] = \"Mockups reminder\"\n        resume[interrupt_.id] = {\n            \"decisions\": [{\"type\": \"edit\", \"edited_action\": edited_action}]\n        }\n    else:\n        resume[interrupt_.id] = {\"decisions\": [{\"type\": \"approve\"}]}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    Command(resume=resume), # [!code highlight]\n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n```\n\n```\n================================= Tool Message =================================\nName: schedule_event\n\nYour meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n================================= Tool Message =================================\nName: manage_email\n\nYour email reminder to the design team has been sent. Here’s what was sent:\n\n- Recipient: designteam@example.com\n- Subject: Mockups reminder\n- Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion.\n\nLet me know if you need any further assistance!\n================================== Ai Message ==================================\n\n- Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n- An email reminder has been sent to the design team about reviewing the new mockups before the meeting.\n\nLet me know if you need any further assistance!\n```\n\nThe run proceeds with our input.\n\n## 7. Advanced: Control information flow\n\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\n\n### Pass additional conversational context to sub-agents\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef schedule_event(\n    request: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    # Customize context received by sub-agent\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"]\n        if message.type == \"human\"\n    )\n    prompt = (\n        \"You are assisting with the following user inquiry:\\n\\n\"\n        f\"{original_user_message.text}\\n\\n\"\n        \"You are tasked with the following sub-request:\\n\\n\"\n        f\"{request}\"\n    )\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n    })\n    return result[\"messages\"][-1].text\n```\n\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like \"schedule it for the same time tomorrow\" (referencing a previous conversation).\n\n<Tip>\n  You can see the full context received by the sub agent in the [chat model call](https://smith.langchain.com/public/c7d54882-afb8-4039-9c5a-4112d0f458b0/r/6803571e-af78-4c68-904a-ecf55771084d) of the LangSmith trace.\n</Tip>\n\n### Control what supervisor receives\n\nYou can also customize what information flows back to the supervisor:\n\n```python  theme={null}\nimport json\n\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n\n    # Option 1: Return just the confirmation message\n    return result[\"messages\"][-1].text\n\n    # Option 2: Return structured data\n    # return json.dumps({\n    #     \"status\": \"success\",\n    #     \"event_id\": \"evt_123\",\n    #     \"summary\": result[\"messages\"][-1].text\n    # })\n```\n\n**Important:** Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don't include the results in their final response.\n\n## 8. Key takeaways\n\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\n\n<Tip>\n  **When to use the supervisor pattern**\n\n  Use the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don't need to converse directly with users.\n\n  For simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use [handoffs](/oss/python/langchain/multi-agent#handoffs) instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.\n</Tip>\n\n## Next steps\n\nLearn about [handoffs](/oss/python/langchain/multi-agent#handoffs) for agent-to-agent conversations, explore [context engineering](/oss/python/langchain/context-engineering) to fine-tune information flow, read the [multi-agent overview](/oss/python/langchain/multi-agent) to compare different patterns, and use [LangSmith](https://smith.langchain.com) to debug and monitor your multi-agent system.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/supervisor.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 37604,
    "word_count": 4075
  },
  {
    "title": "Test",
    "source": "https://docs.langchain.com/oss/python/langchain/test",
    "content": "Agentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model's black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.\n\nThere are a few approaches to testing your agents:\n\n* [Unit tests](#unit-testing) exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n\n* [Integration tests](#integration-testing) test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.\n\nAgentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs.\n\n## Unit Testing\n\n### Mocking Chat Model\n\nFor logic not requiring API calls, you can use an in-memory stub for mocking responses.\n\nLangChain provides [`GenericFakeChatModel`](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.fake_chat_models.GenericFakeChatModel.html) for mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage.\n\n```python  theme={null}\nfrom langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n\nmodel = GenericFakeChatModel(messages=iter([\n    AIMessage(content=\"\", tool_calls=[ToolCall(name=\"foo\", args={\"bar\": \"baz\"}, id=\"call_1\")]),\n    \"bar\"\n]))\n\nmodel.invoke(\"hello\")\n# AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])\n```\n\nIf we invoke the model again, it will return the next item in the iterator:\n\n```python  theme={null}\nmodel.invoke(\"hello, again!\")\n# AIMessage(content='bar', ...)\n```\n\n### InMemorySaver Checkpointer\n\nTo enable persistence during testing, you can use the [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:\n\n```python  theme={null}\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model,\n    tools=[],\n    checkpointer=InMemorySaver()\n)\n\n# First invocation\nagent.invoke(HumanMessage(content=\"I live in Sydney, Australia.\"))\n\n# Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time\nagent.invoke(HumanMessage(content=\"What's my local time?\"))\n```\n\n## Integration Testing\n\nMany agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain's [`agentevals`](https://github.com/langchain-ai/agentevals) package provides evaluators specifically designed for testing agent trajectories with live models.\n\nAgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a **trajectory match** or by using an **LLM judge**:\n\n<Card title=\"Trajectory match\" icon=\"equals\" arrow=\"true\" href=\"#trajectory-match-evaluator\">\n  Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.\n\n  Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn't require additional LLM calls.\n</Card>\n\n<Card title=\"LLM-as-judge\" icon=\"gavel\" arrow=\"true\" href=\"#llm-as-judge-evaluator\">\n  Use a LLM to qualitatively validate your agent's execution trajectory. The \"judge\" LLM reviews the agent's decisions against a prompt rubric (which can include a reference trajectory).\n\n  More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent's trajectory without strict tool call or ordering requirements.\n</Card>\n\n### Installing AgentEvals\n\n```bash  theme={null}\npip install agentevals\n```\n\nOr, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly.\n\n### Trajectory Match Evaluator\n\nAgentEvals offers the `create_trajectory_match_evaluator` function to match your agent's trajectory against a reference trajectory. There are four modes to choose from:\n\n| Mode        | Description                                               | Use Case                                                              |\n| ----------- | --------------------------------------------------------- | --------------------------------------------------------------------- |\n| `strict`    | Exact match of messages and tool calls in the same order  | Testing specific sequences (e.g., policy lookup before authorization) |\n| `unordered` | Same tool calls allowed in any order                      | Verifying information retrieval when order doesn't matter             |\n| `subset`    | Agent calls only tools from reference (no extras)         | Ensuring agent doesn't exceed expected scope                          |\n| `superset`  | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken                          |\n\n<Accordion title=\"Strict match\">\n  The `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather])\n\n  evaluator = create_trajectory_match_evaluator(  # [!code highlight]\n      trajectory_match_mode=\"strict\",  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_weather_tool_called_strict():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]\n      })\n\n      reference_trajectory = [\n          HumanMessage(content=\"What's the weather in San Francisco?\"),\n          AIMessage(content=\"\", tool_calls=[\n              {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}}\n          ]),\n          ToolMessage(content=\"It's 75 degrees and sunny in San Francisco.\", tool_call_id=\"call_1\"),\n          AIMessage(content=\"The weather in San Francisco is 75 degrees and sunny.\"),\n      ]\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n          reference_outputs=reference_trajectory\n      )\n      # {\n      #     'key': 'trajectory_strict_match',\n      #     'score': True,\n      #     'comment': None,\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n</Accordion>\n\n<Accordion title=\"Unordered match\">\n  The `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don't care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn't matter.\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  @tool\n  def get_events(city: str):\n      \"\"\"Get events happening in a city.\"\"\"\n      return f\"Concert at the park in {city} tonight.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather, get_events])\n\n  evaluator = create_trajectory_match_evaluator(  # [!code highlight]\n      trajectory_match_mode=\"unordered\",  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_multiple_tools_any_order():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's happening in SF today?\")]\n      })\n\n      # Reference shows tools called in different order than actual execution\n      reference_trajectory = [\n          HumanMessage(content=\"What's happening in SF today?\"),\n          AIMessage(content=\"\", tool_calls=[\n              {\"id\": \"call_1\", \"name\": \"get_events\", \"args\": {\"city\": \"SF\"}},\n              {\"id\": \"call_2\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n          ]),\n          ToolMessage(content=\"Concert at the park in SF tonight.\", tool_call_id=\"call_1\"),\n          ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_2\"),\n          AIMessage(content=\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"),\n      ]\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n          reference_outputs=reference_trajectory,\n      )\n      # {\n      #     'key': 'trajectory_unordered_match',\n      #     'score': True,\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n</Accordion>\n\n<Accordion title=\"Subset and superset match\">\n  The `superset` and `subset` modes match partial trajectories. The `superset` mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The `subset` mode ensures the agent did not call any tools beyond those in the reference.\n\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  @tool\n  def get_detailed_forecast(city: str):\n      \"\"\"Get detailed weather forecast for a city.\"\"\"\n      return f\"Detailed forecast for {city}: sunny all week.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather, get_detailed_forecast])\n\n  evaluator = create_trajectory_match_evaluator(  # [!code highlight]\n      trajectory_match_mode=\"superset\",  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_agent_calls_required_tools_plus_extra():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's the weather in Boston?\")]\n      })\n\n      # Reference only requires get_weather, but agent may call additional tools\n      reference_trajectory = [\n          HumanMessage(content=\"What's the weather in Boston?\"),\n          AIMessage(content=\"\", tool_calls=[\n              {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"Boston\"}},\n          ]),\n          ToolMessage(content=\"It's 75 degrees and sunny in Boston.\", tool_call_id=\"call_1\"),\n          AIMessage(content=\"The weather in Boston is 75 degrees and sunny.\"),\n      ]\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n          reference_outputs=reference_trajectory,\n      )\n      # {\n      #     'key': 'trajectory_superset_match',\n      #     'score': True,\n      #     'comment': None,\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n</Accordion>\n\n<Info>\n  You can also set the `tool_args_match_mode` property and/or `tool_args_match_overrides` to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#tool-args-match-modes) for more details.\n</Info>\n\n### LLM-as-Judge Evaluator\n\nYou can also use an LLM to evaluate the agent's execution path with the `create_trajectory_llm_as_judge` function. Unlike the trajectory match evaluators, it doesn't require a reference trajectory, but one can be provided if available.\n\n<Accordion title=\"Without reference trajectory\">\n  ```python  theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather])\n\n  evaluator = create_trajectory_llm_as_judge(  # [!code highlight]\n      model=\"openai:o3-mini\",  # [!code highlight]\n      prompt=TRAJECTORY_ACCURACY_PROMPT,  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_trajectory_quality():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n      })\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n      )\n      # {\n      #     'key': 'trajectory_accuracy',\n      #     'score': True,\n      #     'comment': 'The provided agent trajectory is reasonable...'\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n</Accordion>\n\n<Accordion title=\"With reference trajectory\">\n  If you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE` prompt and configure the `reference_outputs` variable:\n\n  ```python  theme={null}\n  evaluator = create_trajectory_llm_as_judge(\n      model=\"openai:o3-mini\",\n      prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n  )\n  evaluation = judge_with_reference(\n      outputs=result[\"messages\"],\n      reference_outputs=reference_trajectory,\n  )\n  ```\n</Accordion>\n\n<Info>\n  For more configurability over how the LLM evaluates the trajectory, visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#trajectory-llm-as-judge).\n</Info>\n\n### Async Support\n\nAll `agentevals` evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding `async` after `create_` in the function name.\n\n<Accordion title=\"Async judge and evaluator example\">\n  ```python  theme={null}\n  from agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n  from agentevals.trajectory.match import create_async_trajectory_match_evaluator\n\n  async_judge = create_async_trajectory_llm_as_judge(\n      model=\"openai:o3-mini\",\n      prompt=TRAJECTORY_ACCURACY_PROMPT,\n  )\n\n  async_evaluator = create_async_trajectory_match_evaluator(\n      trajectory_match_mode=\"strict\",\n  )\n\n  async def test_async_evaluation():\n      result = await agent.ainvoke({\n          \"messages\": [HumanMessage(content=\"What's the weather?\")]\n      })\n\n      evaluation = await async_judge(outputs=result[\"messages\"])\n      assert evaluation[\"score\"] is True\n  ```\n</Accordion>\n\n## LangSmith Integration\n\nFor tracking experiments over time, you can log evaluator results to [LangSmith](https://smith.langchain.com/), a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools.\n\nFirst, set up LangSmith by setting the required environment variables:\n\n```bash  theme={null}\nexport LANGSMITH_API_KEY=\"your_langsmith_api_key\"\nexport LANGSMITH_TRACING=\"true\"\n```\n\nLangSmith offers two main approaches for running evaluations: [pytest](/langsmith/pytest) integration and the `evaluate` function.\n\n<Accordion title=\"Using pytest integration\">\n  ```python  theme={null}\n  import pytest\n  from langsmith import testing as t\n  from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\n  trajectory_evaluator = create_trajectory_llm_as_judge(\n      model=\"openai:o3-mini\",\n      prompt=TRAJECTORY_ACCURACY_PROMPT,\n  )\n\n  @pytest.mark.langsmith\n  def test_trajectory_accuracy():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's the weather in SF?\")]\n      })\n\n      reference_trajectory = [\n          HumanMessage(content=\"What's the weather in SF?\"),\n          AIMessage(content=\"\", tool_calls=[\n              {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n          ]),\n          ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_1\"),\n          AIMessage(content=\"The weather in SF is 75 degrees and sunny.\"),\n      ]\n\n      # Log inputs, outputs, and reference outputs to LangSmith\n      t.log_inputs({})\n      t.log_outputs({\"messages\": result[\"messages\"]})\n      t.log_reference_outputs({\"messages\": reference_trajectory})\n\n      trajectory_evaluator(\n          outputs=result[\"messages\"],\n          reference_outputs=reference_trajectory\n      )\n  ```\n\n  Run the evaluation with pytest:\n\n  ```bash  theme={null}\n  pytest test_trajectory.py --langsmith-output\n  ```\n\n  Results will be automatically logged to LangSmith.\n</Accordion>\n\n<Accordion title=\"Using the evaluate function\">\n  Alternatively, you can create a dataset in LangSmith and use the `evaluate` function:\n\n  ```python  theme={null}\n  from langsmith import Client\n  from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\n  client = Client()\n\n  trajectory_evaluator = create_trajectory_llm_as_judge(\n      model=\"openai:o3-mini\",\n      prompt=TRAJECTORY_ACCURACY_PROMPT,\n  )\n\n  def run_agent(inputs):\n      \"\"\"Your agent function that returns trajectory messages.\"\"\"\n      return agent.invoke(inputs)[\"messages\"]\n\n  experiment_results = client.evaluate(\n      run_agent,\n      data=\"your_dataset_name\",\n      evaluators=[trajectory_evaluator]\n  )\n  ```\n\n  Results will be automatically logged to LangSmith.\n</Accordion>\n\n<Tip>\n  To learn more about evaluating your agent, see the [LangSmith docs](/langsmith/pytest).\n</Tip>\n\n## Recording & Replaying HTTP Calls\n\nIntegration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls.\n\nYou can use [`vcrpy`](https://pypi.org/project/vcrpy/1.5.2/) to achieve this. If you're using `pytest`, the [`pytest-recording` plugin](https://pypi.org/project/pytest-recording/) provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs.\n\nSet up your `conftest.py` file to filter out sensitive information from the cassettes:\n\n```py conftest.py theme={null}\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef vcr_config():\n    return {\n        \"filter_headers\": [\n            (\"authorization\", \"XXXX\"),\n            (\"x-api-key\", \"XXXX\"),\n            # ... other headers you want to mask\n        ],\n        \"filter_query_parameters\": [\n            (\"api_key\", \"XXXX\"),\n            (\"key\", \"XXXX\"),\n        ],\n    }\n```\n\nThen configure your project to recognise the `vcr` marker:\n\n<CodeGroup>\n  ```ini pytest.ini theme={null}\n  [pytest]\n  markers =\n      vcr: record/replay HTTP via VCR\n  addopts = --record-mode=once\n  ```\n\n  ```toml pyproject.toml theme={null}\n  [tool.pytest.ini_options]\n  markers = [\n    \"vcr: record/replay HTTP via VCR\"\n  ]\n  addopts = \"--record-mode=once\"\n  ```\n</CodeGroup>\n\n<Info>\n  The `--record-mode=once` option records HTTP interactions on the first run and replays them on subsequent runs.\n</Info>\n\nNow, simply decorate your tests with the `vcr` marker:\n\n```python  theme={null}\n@pytest.mark.vcr()\ndef test_agent_trajectory():\n    # ...\n```\n\nThe first time you run this test, your agent will make real network calls and pytest will generate a cassette file `test_agent_trajectory.yaml` in the `tests/cassettes` directory. Subsequent runs will use that cassette to mock the real network calls, granted the agent's requests don't change from the previous run. If they do, the test will fail and you'll need to delete the cassette and rerun the test to record fresh interactions.\n\n<Warning>\n  When you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests **will fail**. You should delete the corresponding cassette files and rerun the tests to record fresh interactions.\n</Warning>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/test.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 21171,
    "word_count": 2225
  },
  {
    "title": "Tools",
    "source": "https://docs.langchain.com/oss/python/langchain/tools",
    "content": "Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems—such as APIs, databases, or file systems—using structured input.\n\nTools are components that [agents](/oss/python/langchain/agents) call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs.\n\nTools encapsulate a callable function and its input schema. These can be passed to compatible [chat models](/oss/python/langchain/models), allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/python/integrations/chat/openai), [Anthropic](/oss/python/integrations/chat/anthropic), and [Gemini](/oss/python/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/python/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/python/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is with the [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it:\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n```\n\nType hints are **required** as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose.\n\n### Customize tool properties\n\n#### Custom tool name\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\n\n```python  theme={null}\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n```\n\n#### Custom tool description\n\nOverride the auto-generated tool description for clearer model guidance:\n\n```python  theme={null}\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n```\n\n### Advanced schema definition\n\nDefine complex inputs with Pydantic models or JSON schemas:\n\n<CodeGroup>\n  ```python Pydantic model theme={null}\n  from pydantic import BaseModel, Field\n  from typing import Literal\n\n  class WeatherInput(BaseModel):\n      \"\"\"Input for weather queries.\"\"\"\n      location: str = Field(description=\"City name or coordinates\")\n      units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n          default=\"celsius\",\n          description=\"Temperature unit preference\"\n      )\n      include_forecast: bool = Field(\n          default=False,\n          description=\"Include 5-day forecast\"\n      )\n\n  @tool(args_schema=WeatherInput)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n\n  ```python JSON Schema theme={null}\n  weather_schema = {\n      \"type\": \"object\",\n      \"properties\": {\n          \"location\": {\"type\": \"string\"},\n          \"units\": {\"type\": \"string\"},\n          \"include_forecast\": {\"type\": \"boolean\"}\n      },\n      \"required\": [\"location\", \"units\", \"include_forecast\"]\n  }\n\n  @tool(args_schema=weather_schema)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n</CodeGroup>\n\n### Reserved argument names\n\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\n\n| Parameter name | Purpose                                                                |\n| -------------- | ---------------------------------------------------------------------- |\n| `config`       | Reserved for passing `RunnableConfig` to tools internally              |\n| `runtime`      | Reserved for `ToolRuntime` parameter (accessing state, context, store) |\n\nTo access runtime information, use the [`ToolRuntime`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.ToolRuntime) parameter instead of naming your own arguments `config` or `runtime`.\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\n</Info>\n\nTools can access runtime information through the `ToolRuntime` parameter, which provides:\n\n* **State** - Mutable data that flows through execution (e.g., messages, counters, custom fields)\n* **Context** - Immutable configuration like user IDs, session details, or application-specific configuration\n* **Store** - Persistent long-term memory across conversations\n* **Stream Writer** - Stream custom updates as tools execute\n* **Config** - `RunnableConfig` for the execution\n* **Tool Call ID** - ID of the current tool call\n\n```mermaid  theme={null}\ngraph LR\n    %% Runtime Context\n    subgraph \"🔧 Tool Runtime Context\"\n        A[Tool Call] --> B[ToolRuntime]\n        B --> C[State Access]\n        B --> D[Context Access]\n        B --> E[Store Access]\n        B --> F[Stream Writer]\n    end\n\n    %% Available Resources\n    subgraph \"📊 Available Resources\"\n        C --> G[Messages]\n        C --> H[Custom State]\n        D --> I[User ID]\n        D --> J[Session Info]\n        E --> K[Long-term Memory]\n        E --> L[User Preferences]\n    end\n\n    %% Tool Capabilities\n    subgraph \"⚡ Enhanced Tool Capabilities\"\n        M[Context-Aware Tools]\n        N[Stateful Tools]\n        O[Memory-Enabled Tools]\n        P[Streaming Tools]\n    end\n\n    %% Connections\n    G --> M\n    H --> N\n    I --> M\n    J --> M\n    K --> O\n    L --> O\n    F --> P\n```\n\n### `ToolRuntime`\n\nUse `ToolRuntime` to access all runtime information in a single parameter. Simply add `runtime: ToolRuntime` to your tool signature, and it will be automatically injected without being exposed to the LLM.\n\n<Info>\n  **`ToolRuntime`**: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate [`InjectedState`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedState), [`InjectedStore`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedStore), [`get_runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.get_runtime), and [`InjectedToolCallId`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.InjectedToolCallId) annotations.\n\n  The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.\n</Info>\n\n**Accessing state:**\n\nTools can access the current graph state using `ToolRuntime`:\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n# Access the current conversation state\n@tool\ndef summarize_conversation(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Summarize the conversation so far.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n\n    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n```\n\n<Warning>\n  The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `runtime` is *not* included in the request.\n</Warning>\n\n**Updating state:**\n\nUse [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:\n\n```python  theme={null}\nfrom langgraph.types import Command\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langchain.tools import tool, ToolRuntime\n\n# Update the conversation history by removing all messages\n@tool\ndef clear_conversation() -> Command:\n    \"\"\"Clear the conversation history.\"\"\"\n\n    return Command(\n        update={\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n        }\n    )\n\n# Update the user_name in the agent state\n@tool\ndef update_user_name(\n    new_name: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Update the user's name.\"\"\"\n    return Command(update={\"user_name\": new_name})\n```\n\n#### Context\n\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`.\n\nTools can access runtime context through `ToolRuntime`:\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data.\n\nTools can access and update the store through `ToolRuntime`:\n\n```python expandable theme={null}\nfrom typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n```\n\n<Note>\n  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.\n</Note>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 14421,
    "word_count": 1635
  },
  {
    "title": "Agent Chat UI",
    "source": "https://docs.langchain.com/oss/python/langchain/ui",
    "content": "[Agent Chat UI](https://github.com/langchain-ai/agent-chat-ui) is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using [`create_agent`](../langchain/agents) and provides interactive experiences for your agents with minimal setup, whether you're running locally or in a deployed context (such as [LangSmith](/langsmith/home)).\n\nAgent Chat UI is open source and can be adapted to your application needs.\n\n<Frame>\n  <iframe className=\"w-full aspect-video rounded-xl\" src=\"https://www.youtube.com/embed/lInrwVnZ83o?si=Uw66mPtCERJm0EjU\" title=\"Agent Chat UI\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowFullScreen />\n</Frame>\n\n<Tip>\n  You can use generative UI in the Agent Chat UI. For more information, see [Implement generative user interfaces with LangGraph](/langsmith/generative-ui-react).\n</Tip>\n\n### Quick start\n\nThe fastest way to get started is using the hosted version:\n\n1. **Visit [Agent Chat UI](https://agentchat.vercel.app)**\n2. **Connect your agent** by entering your deployment URL or local server address\n3. **Start chatting** - the UI will automatically detect and render tool calls and interrupts\n\n### Local development\n\nFor customization or local development, you can run Agent Chat UI locally:\n\n<CodeGroup>\n  ```bash Use npx theme={null}\n  # Create a new Agent Chat UI project\n  npx create-agent-chat-app --project-name my-chat-ui\n  cd my-chat-ui\n\n  # Install dependencies and start\n  pnpm install\n  pnpm dev\n  ```\n\n  ```bash Clone repository theme={null}\n  # Clone the repository\n  git clone https://github.com/langchain-ai/agent-chat-ui.git\n  cd agent-chat-ui\n\n  # Install dependencies and start\n  pnpm install\n  pnpm dev\n  ```\n</CodeGroup>\n\n### Connect to your agent\n\nAgent Chat UI can connect to both [local](/oss/python/langchain/studio#setup-local-agent-server) and [deployed agents](/oss/python/langchain/deploy).\n\nAfter starting Agent Chat UI, you'll need to configure it to connect to your agent:\n\n1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)\n2. **Deployment URL**: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)\n3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local Agent server)\n\nOnce configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n\n<Tip>\n  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/ui.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/langchain",
    "char_count": 3288,
    "word_count": 391
  },
  {
    "title": "LangChain v1 migration guide",
    "source": "https://docs.langchain.com/oss/python/migrate/langchain-v1",
    "content": "This guide outlines the major changes between [LangChain v1](/oss/python/releases/langchain-v1) and previous versions.\n\n## Simplified package\n\nThe `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\n\n### Namespace\n\n| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |\n| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |\n| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |\n| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |\n| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |\n| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |\n| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |\n\n### `langchain-classic`\n\nIf you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:\n\n* Legacy chains (`LLMChain`, `ConversationChain`, etc.)\n* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)\n* The indexing API\n* The hub module (for managing prompts programmatically)\n* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)\n* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports\n* Other deprecated functionality\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  # Chains\n  from langchain_classic.chains import LLMChain\n\n  # Retrievers\n  from langchain_classic.retrievers import ...\n\n  # Indexing\n  from langchain_classic.indexes import ...\n\n  # Hub\n  from langchain_classic import hub\n  ```\n\n  ```python v0 (old) theme={null}\n  # Chains\n  from langchain.chains import LLMChain\n\n  # Retrievers\n  from langchain.retrievers import ...\n\n  # Indexing\n  from langchain.indexes import ...\n\n  # Hub\n  from langchain import hub\n  ```\n</CodeGroup>\n\nInstall with:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-classic\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-classic\n  ```\n</CodeGroup>\n\n***\n\n## Migrate to `create_agent`\n\nPrior to v1.0, we recommended using [`langgraph.prebuilt.create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to build agents. Now, we recommend you use [`langchain.agents.create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) to build agents.\n\nThe table below outlines what functionality has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):\n\n| Section                                            | TL;DR - What's changed                                                                                                                                                                     |\n| -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [Import path](#import-path)                        | Package moved from `langgraph.prebuilt` to `langchain.agents`                                                                                                                              |\n| [Prompts](#prompts)                                | Parameter renamed to [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)), dynamic prompts use middleware            |\n| [Pre-model hook](#pre-model-hook)                  | Replaced by middleware with `before_model` method                                                                                                                                          |\n| [Post-model hook](#post-model-hook)                | Replaced by middleware with `after_model` method                                                                                                                                           |\n| [Custom state](#custom-state)                      | `TypedDict` only, can be defined via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) or middleware |\n| [Model](#model)                                    | Dynamic selection via middleware, pre-bound models not supported                                                                                                                           |\n| [Tools](#tools)                                    | Tool error handling moved to middleware with `wrap_tool_call`                                                                                                                              |\n| [Structured output](#structured-output)            | prompted output removed, use `ToolStrategy`/`ProviderStrategy`                                                                                                                             |\n| [Streaming node name](#streaming-node-name-rename) | Node name changed from `\"agent\"` to `\"model\"`                                                                                                                                              |\n| [Runtime context](#runtime-context)                | Dependency injection via `context` argument instead of `config[\"configurable\"]`                                                                                                            |\n| [Namespace](#simplified-package)                   | Streamlined to focus on agent building blocks, legacy code moved to `langchain-classic`                                                                                                    |\n\n### Import path\n\nThe import path for the agent prebuilt has changed from `langgraph.prebuilt` to `langchain.agents`.\nThe name of the function has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):\n\n```python  theme={null}\nfrom langgraph.prebuilt import create_react_agent # [!code --]\nfrom langchain.agents import create_agent # [!code ++]\n```\n\nFor more information, see [Agents](/oss/python/langchain/agents).\n\n### Prompts\n\n#### Static prompt rename\n\nThe `prompt` parameter has been renamed to [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)):\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      system_prompt=\"You are a helpful assistant\"  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      prompt=\"You are a helpful assistant\"  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### `SystemMessage` to string\n\nIf using [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects in the system prompt, extract the string content:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      system_prompt=\"You are a helpful assistant\"  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langchain.messages import SystemMessage\n  from langgraph.prebuilt import create_react_agent\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      prompt=SystemMessage(content=\"You are a helpful assistant\")  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### Dynamic prompts\n\nDynamic prompts are a core context engineering pattern— they adapt what you tell the model based on the current conversation state. To do this, use the [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) decorator:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import dynamic_prompt, ModelRequest\n  from langgraph.runtime import Runtime\n\n\n  @dataclass\n  class Context:  # [!code highlight]\n      user_role: str = \"user\"\n\n  @dynamic_prompt  # [!code highlight]\n  def dynamic_prompt(request: ModelRequest) -> str:  # [!code highlight]\n      user_role = request.runtime.context.user_role\n      base_prompt = \"You are a helpful assistant.\"\n\n      if user_role == \"expert\":\n          prompt = (\n              f\"{base_prompt} Provide detailed technical responses.\"\n          )\n      elif user_role == \"beginner\":\n          prompt = (\n              f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n          )\n      else:\n          prompt = base_prompt\n\n      return prompt  # [!code highlight]\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=tools,\n      middleware=[dynamic_prompt],  # [!code highlight]\n      context_schema=Context\n  )\n\n  # Use with context\n  agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]},\n      context=Context(user_role=\"expert\")\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from dataclasses import dataclass\n\n  from langgraph.prebuilt import create_react_agent, AgentState\n  from langgraph.runtime import get_runtime\n\n  @dataclass\n  class Context:\n      user_role: str\n\n  def dynamic_prompt(state: AgentState) -> str:\n      runtime = get_runtime(Context)  # [!code highlight]\n      user_role = runtime.context.user_role\n      base_prompt = \"You are a helpful assistant.\"\n\n      if user_role == \"expert\":\n          return f\"{base_prompt} Provide detailed technical responses.\"\n      elif user_role == \"beginner\":\n          return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n      return base_prompt\n\n  agent = create_react_agent(\n      model=\"gpt-4o\",\n      tools=tools,\n      prompt=dynamic_prompt,\n      context_schema=Context\n  )\n\n  # Use with context\n  agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]},\n      context=Context(user_role=\"expert\")\n  )\n  ```\n</CodeGroup>\n\n### Pre-model hook\n\nPre-model hooks are now implemented as middleware with the `before_model` method.\nThis new pattern is more extensible--you can define multiple middlewares to run before the model is called,\nreusing common patterns across different agents.\n\nCommon use cases include:\n\n* Summarizing conversation history\n* Trimming messages\n* Input guardrails, like PII redaction\n\nv1 now has summarization middleware as a built in option:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import SummarizationMiddleware\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      middleware=[\n          SummarizationMiddleware(  # [!code highlight]\n              model=\"claude-sonnet-4-5-20250929\",  # [!code highlight]\n              trigger={\"tokens\": 1000}  # [!code highlight]\n          )  # [!code highlight]\n      ]  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent, AgentState\n\n  def custom_summarization_function(state: AgentState):\n      \"\"\"Custom logic for message summarization.\"\"\"\n      ...\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      pre_model_hook=custom_summarization_function\n  )\n  ```\n</CodeGroup>\n\n### Post-model hook\n\nPost-model hooks are now implemented as middleware with the `after_model` method.\nThis new pattern is more extensible--you can define multiple middlewares to run after the model is called,\nreusing common patterns across different agents.\n\nCommon use cases include:\n\n* [Human in the loop](/oss/python/langchain/human-in-the-loop)\n* Output guardrails\n\nv1 has a built in middleware for human in the loop approval for tool calls:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import HumanInTheLoopMiddleware\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[read_email, send_email],\n      middleware=[\n          HumanInTheLoopMiddleware(\n              interrupt_on={\n                  \"send_email\": {\n                      \"description\": \"Please review this email before sending\",\n                      \"allowed_decisions\": [\"approve\", \"reject\"]\n                  }\n              }\n          )\n      ]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n  from langgraph.prebuilt import AgentState\n\n  def custom_human_in_the_loop_hook(state: AgentState):\n      \"\"\"Custom logic for human in the loop approval.\"\"\"\n      ...\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[read_email, send_email],\n      post_model_hook=custom_human_in_the_loop_hook\n  )\n  ```\n</CodeGroup>\n\n### Custom state\n\nCustom state extends the default agent state with additional fields. You can define custom state in two ways:\n\n1. **Via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)** - Best for state used in tools\n2. **Via middleware** - Best for state managed by specific middleware hooks and tools attached to said middleware\n\n<Note>\n  Defining custom state via middleware is preferred over defining it via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\n\n  `state_schema` is still supported for backwards compatibility on `create_agent`.\n</Note>\n\n#### Defining state via `state_schema`\n\nUse the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter when your custom state needs to be accessed by tools:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.tools import tool, ToolRuntime\n  from langchain.agents import create_agent, AgentState  # [!code highlight]\n\n\n  # Define custom state extending AgentState\n  class CustomState(AgentState):\n      user_name: str\n\n  @tool  # [!code highlight]\n  def greet(\n      runtime: ToolRuntime[None, CustomState]\n  ) -> str:\n      \"\"\"Use this to greet the user by name.\"\"\"\n      user_name = runtime.state.get(\"user_name\", \"Unknown\")  # [!code highlight]\n      return f\"Hello {user_name}!\"\n\n  agent = create_agent(  # [!code highlight]\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[greet],\n      state_schema=CustomState  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from typing import Annotated\n  from langgraph.prebuilt import InjectedState, create_react_agent\n  from langgraph.prebuilt.chat_agent_executor import AgentState\n\n  class CustomState(AgentState):\n      user_name: str\n\n  def greet(\n      state: Annotated[CustomState, InjectedState]\n  ) -> str:\n      \"\"\"Use this to greet the user by name.\"\"\"\n      user_name = state[\"user_name\"]\n      return f\"Hello {user_name}!\"\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[greet],\n      state_schema=CustomState\n  )\n  ```\n</CodeGroup>\n\n#### Defining state via middleware\n\nMiddleware can also define custom state by setting the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) attribute.\nThis helps to keep state extensions conceptually scoped to the relevant middleware and tools.\n\n```python  theme={null}\nfrom langchain.agents.middleware import AgentState, AgentMiddleware\nfrom typing_extensions import NotRequired\nfrom typing import Any\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\n    state_schema = CustomState  # [!code highlight]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[...],\n    middleware=[CallCounterMiddleware()]  # [!code highlight]\n)\n```\n\nSee the [middleware documentation](/oss/python/langchain/middleware#custom-state-schema) for more details on defining custom state via middleware.\n\n#### State type restrictions\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) only supports `TypedDict` for state schemas. Pydantic models and dataclasses are no longer supported.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import AgentState, create_agent\n\n  # AgentState is a TypedDict\n  class CustomAgentState(AgentState):  # [!code highlight]\n      user_id: str\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      state_schema=CustomAgentState  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from typing_extensions import Annotated\n\n  from pydantic import BaseModel\n  from langgraph.graph import StateGraph\n  from langgraph.graph.messages import add_messages\n  from langchain.messages import AnyMessage\n\n\n  class AgentState(BaseModel):  # [!code highlight]\n      messages: Annotated[list[AnyMessage], add_messages]\n      user_id: str\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      state_schema=AgentState\n  )\n  ```\n</CodeGroup>\n\nSimply inherit from `langchain.agents.AgentState` instead of `BaseModel` or decorating with `dataclass`.\nIf you need to perform validation, handle it in middleware hooks instead.\n\n### Model\n\nDynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) released in v0.6 of [`langgraph-prebuilt`](https://pypi.org/project/langgraph-prebuilt) supported dynamic model and tool selection via a callable passed to the `model` parameter.\n\nThis functionality has been ported to the middleware interface in v1.\n\n#### Dynamic model selection\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import (\n      AgentMiddleware, ModelRequest\n  )\n  from langchain.agents.middleware.types import ModelResponse\n  from langchain_openai import ChatOpenAI\n  from typing import Callable\n\n  basic_model = ChatOpenAI(model=\"gpt-5-nano\")\n  advanced_model = ChatOpenAI(model=\"gpt-5\")\n\n  class DynamicModelMiddleware(AgentMiddleware):\n\n      def wrap_model_call(self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n          if len(request.state.messages) > self.messages_threshold:\n              model = advanced_model\n          else:\n              model = basic_model\n          return handler(request.override(model=model))\n\n      def __init__(self, messages_threshold: int) -> None:\n          self.messages_threshold = messages_threshold\n\n  agent = create_agent(\n      model=basic_model,\n      tools=tools,\n      middleware=[DynamicModelMiddleware(messages_threshold=10)]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent, AgentState\n  from langchain_openai import ChatOpenAI\n\n  basic_model = ChatOpenAI(model=\"gpt-5-nano\")\n  advanced_model = ChatOpenAI(model=\"gpt-5\")\n\n  def select_model(state: AgentState) -> BaseChatModel:\n      # use a more advanced model for longer conversations\n      if len(state.messages) > 10:\n          return advanced_model\n      return basic_model\n\n  agent = create_react_agent(\n      model=select_model,\n      tools=tools,\n  )\n  ```\n</CodeGroup>\n\n#### Pre-bound models\n\nTo better support structured output, [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) no longer accepts pre-bound models with tools or configuration:\n\n```python  theme={null}\n# No longer supported\nmodel_with_tools = ChatOpenAI().bind_tools([some_tool])\nagent = create_agent(model_with_tools, tools=[])\n\n# Use instead\nagent = create_agent(\"gpt-4o-mini\", tools=[some_tool])\n```\n\n<Note>\n  Dynamic model functions can return pre-bound models if structured output is *not* used.\n</Note>\n\n### Tools\n\nThe [`tools`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(tools\\)) argument to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) accepts a list of:\n\n* LangChain [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) instances (functions decorated with [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool))\n* Callable objects (functions) with proper type hints and a docstring\n* `dict` that represents a built-in provider tools\n\nThe argument will no longer accept [`ToolNode`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.ToolNode) instances.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather, search_web]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent, ToolNode\n\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=ToolNode([check_weather, search_web]) # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### Handling tool errors\n\nYou can now configure the handling of tool errors with middleware implementing the `wrap_tool_call` method.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n\n  @wrap_tool_call\n  def retry_on_error(request, handler):\n      max_retries = 3\n      for attempt in range(max_retries):\n          try:\n              return handler(request)\n          except Exception:\n              if attempt == max_retries - 1:\n                  raise\n  ```\n\n  ```python v0 (old) theme={null}\n  # Example coming soon\n  ```\n</CodeGroup>\n\n### Structured output\n\n#### Node changes\n\nStructured output used to be generated in a separate node from the main agent. This is no longer the case.\nWe generate structured output in the main loop, reducing cost and latency.\n\n#### Tool and provider strategies\n\nIn v1, there are two new structured output strategies:\n\n* `ToolStrategy` uses artificial tool calling to generate structured output\n* `ProviderStrategy` uses provider-native structured output generation\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.structured_output import ToolStrategy, ProviderStrategy\n  from pydantic import BaseModel\n\n\n  class OutputSchema(BaseModel):\n      summary: str\n      sentiment: str\n\n  # Using ToolStrategy\n  agent = create_agent(\n      model=\"gpt-4o-mini\",\n      tools=tools,\n      # explicitly using tool strategy\n      response_format=ToolStrategy(OutputSchema)  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n  from pydantic import BaseModel\n\n  class OutputSchema(BaseModel):\n      summary: str\n      sentiment: str\n\n  agent = create_react_agent(\n      model=\"gpt-4o-mini\",\n      tools=tools,\n      # using tool strategy by default with no option for provider strategy\n      response_format=OutputSchema  # [!code highlight]\n  )\n\n  # OR\n\n  agent = create_react_agent(\n      model=\"gpt-4o-mini\",\n      tools=tools,\n      # using a custom prompt to instruct the model to generate the output schema\n      response_format=(\"please generate ...\", OutputSchema)  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### Prompted output removed\n\n**Prompted output** is no longer supported via the `response_format` argument. Compared to strategies\nlike artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable.\n\n### Streaming node name rename\n\nWhen streaming events from agents, the node name has changed from `\"agent\"` to `\"model\"` to better reflect the node's purpose.\n\n### Runtime context\n\nWhen you invoke an agent, it's often the case that you want to pass two types of data:\n\n* Dynamic state that changes throughout the conversation (e.g., message history)\n* Static context that doesn't change during the conversation (e.g., user metadata)\n\nIn v1, static context is supported by setting the `context` parameter to `invoke` and `stream`.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n\n\n  @dataclass\n  class Context:\n      user_id: str\n      session_id: str\n\n  agent = create_agent(\n      model=model,\n      tools=tools,\n      context_schema=Context  # [!code highlight]\n  )\n\n  result = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n      context=Context(user_id=\"123\", session_id=\"abc\")  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n\n\n  agent = create_react_agent(model, tools)\n\n  # Pass context via configurable\n  result = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n      config={  # [!code highlight]\n          \"configurable\": {  # [!code highlight]\n              \"user_id\": \"123\",  # [!code highlight]\n              \"session_id\": \"abc\"  # [!code highlight]\n          }  # [!code highlight]\n      }  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n<Note>\n  The old `config[\"configurable\"]` pattern still works for backward compatibility, but using the new `context` parameter is recommended for new applications or applications migrating to v1.\n</Note>\n\n***\n\n## Standard content\n\nIn v1, messages gain provider-agnostic standard content blocks. Access them via [`message.content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) for a consistent, typed view across providers. The existing [`message.content`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) field remains unchanged for strings or provider-native structures.\n\n### What changed\n\n* New [`content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) property on messages for normalized content\n* Standardized block shapes, documented in [Messages](/oss/python/langchain/messages#standard-content-blocks)\n* Optional serialization of standard blocks into `content` via `LC_OUTPUT_VERSION=v1` or `output_version=\"v1\"`\n\n### Read standardized content\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(\"gpt-5-nano\")\n  response = model.invoke(\"Explain AI\")\n\n  for block in response.content_blocks:\n      if block[\"type\"] == \"reasoning\":\n          print(block.get(\"reasoning\"))\n      elif block[\"type\"] == \"text\":\n          print(block.get(\"text\"))\n  ```\n\n  ```python v0 (old) theme={null}\n  # Provider-native formats vary; you needed per-provider handling\n  response = model.invoke(\"Explain AI\")\n  for item in response.content:\n      if item.get(\"type\") == \"reasoning\":\n          ...  # OpenAI-style reasoning\n      elif item.get(\"type\") == \"thinking\":\n          ...  # Anthropic-style thinking\n      elif item.get(\"type\") == \"text\":\n          ...  # Text\n  ```\n</CodeGroup>\n\n### Create multimodal messages\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.messages import HumanMessage\n\n  message = HumanMessage(content_blocks=[\n      {\"type\": \"text\", \"text\": \"Describe this image.\"},\n      {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n  ])\n  res = model.invoke([message])\n  ```\n\n  ```python v0 (old) theme={null}\n  from langchain.messages import HumanMessage\n\n  message = HumanMessage(content=[\n      # Provider-native structure\n      {\"type\": \"text\", \"text\": \"Describe this image.\"},\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}},\n  ])\n  res = model.invoke([message])\n  ```\n</CodeGroup>\n\n### Example block shapes\n\n```python  theme={null}\n# Text block\ntext_block = {\n    \"type\": \"text\",\n    \"text\": \"Hello world\",\n}\n\n# Image block\nimage_block = {\n    \"type\": \"image\",\n    \"url\": \"https://example.com/image.png\",\n    \"mime_type\": \"image/png\",\n}\n```\n\nSee the content blocks [reference](/oss/python/langchain/messages#content-block-reference) for more details.\n\n### Serialize standard content\n\nStandard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.\n\n<CodeGroup>\n  ```bash Environment variable theme={null}\n  export LC_OUTPUT_VERSION=v1\n  ```\n\n  ```python Initialization parameter theme={null}\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(\n      \"gpt-5-nano\",\n      output_version=\"v1\",\n  )\n  ```\n</CodeGroup>\n\n<Note>\n  Learn more: [Messages](/oss/python/langchain/messages#message-content), [Standard content blocks](/oss/python/langchain/messages#standard-content-blocks), and [Multimodal](/oss/python/langchain/messages#multimodal).\n</Note>\n\n***\n\n## Simplified package\n\nThe `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\n\n### Namespace\n\n| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |\n| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |\n| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |\n| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |\n| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |\n| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |\n| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |\n\n### `langchain-classic`\n\nIf you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:\n\n* Legacy chains (`LLMChain`, `ConversationChain`, etc.)\n* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)\n* The indexing API\n* The hub module (for managing prompts programmatically)\n* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)\n* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports\n* Other deprecated functionality\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  # Chains\n  from langchain_classic.chains import LLMChain\n\n  # Retrievers\n  from langchain_classic.retrievers import ...\n\n  # Indexing\n  from langchain_classic.indexes import ...\n\n  # Hub\n  from langchain_classic import hub\n  ```\n\n  ```python v0 (old) theme={null}\n  # Chains\n  from langchain.chains import LLMChain\n\n  # Retrievers\n  from langchain.retrievers import ...\n\n  # Indexing\n  from langchain.indexes import ...\n\n  # Hub\n  from langchain import hub\n  ```\n</CodeGroup>\n\n**Installation**:\n\n```bash  theme={null}\nuv pip install langchain-classic\n```\n\n***\n\n## Breaking changes\n\n### Dropped Python 3.9 support\n\nAll LangChain packages now require **Python 3.10 or higher**. Python 3.9 reaches [end of life](https://devguide.python.org/versions/) in October 2025.\n\n### Updated return type for chat models\n\nThe return type signature for chat model invocation has been fixed from [`BaseMessage`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage) to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage). Custom chat models implementing [`bind_tools`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.bind_tools) should update their return signature:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  def bind_tools(\n          ...\n      ) -> Runnable[LanguageModelInput, AIMessage]:\n  ```\n\n  ```python v0 (old) theme={null}\n  def bind_tools(\n          ...\n      ) -> Runnable[LanguageModelInput, BaseMessage]:\n  ```\n</CodeGroup>\n\n### Default message format for OpenAI Responses API\n\nWhen interacting with the Responses API, `langchain-openai` now defaults to storing response items in message `content`. To restore previous behavior, set the `LC_OUTPUT_VERSION` environment variable to `v0`, or specify `output_version=\"v0\"` when instantiating [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI).\n\n```python  theme={null}\n# Enforce previous behavior with output_version flag\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", output_version=\"v0\")\n```\n\n### Default `max_tokens` in `langchain-anthropic`\n\nThe `max_tokens` parameter in `langchain-anthropic` now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max_tokens=1024`.\n\n### Legacy code moved to `langchain-classic`\n\nExisting functionality outside the focus of standard interfaces and agents has been moved to the [`langchain-classic`](https://pypi.org/project/langchain-classic) package. See the [Simplified namespace](#simplified-package) section for details on what's available in the core `langchain` package and what moved to `langchain-classic`.\n\n### Removal of deprecated APIs\n\nMethods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the [deprecation notices](https://python.langchain.com/docs/versions/migrating_chains) from previous versions for replacement APIs.\n\n### Text property\n\nUse of the `.text()` method on message objects should drop the parentheses, as it is now a property:\n\n```python  theme={null}\n# Property access\ntext = response.text\n\n# Deprecated method call\ntext = response.text()\n```\n\nExisting usage patterns (i.e., `.text()`) will continue to function but now emit a warning. The method form will be removed in v2.\n\n### `example` parameter removed from `AIMessage`\n\nThe `example` parameter has been removed from [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects. We recommend migrating to use `additional_kwargs` for passing extra metadata as needed.\n\n## Minor changes\n\n* `AIMessageChunk` objects now include a `chunk_position` attribute with position `'last'` to indicate the final chunk in a stream. This allows for clearer handling of streamed messages. If the chunk is not the final one, `chunk_position` will be `None`.\n* `LanguageModelOutputVar` is now typed to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) instead of [`BaseMessage`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage).\n* The logic for merging message chunks (`AIMessageChunk.add`) has been updated with more sophisticated selection handling for the final id for the merged chunk. It prioritizes provider-assigned IDs over LangChain-generated IDs.\n* We now open files with `utf-8` encoding by default.\n* Standard tests now use multimodal content blocks.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langchain-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/migrate",
    "char_count": 41694,
    "word_count": 3453
  },
  {
    "title": "LangGraph v1 migration guide",
    "source": "https://docs.langchain.com/oss/python/migrate/langgraph-v1",
    "content": "This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of changes, see the [what's new](/oss/python/releases/langgraph-v1) page.\n\nTo upgrade:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph langchain-core\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph langchain-core\n  ```\n</CodeGroup>\n\n## Summary of changes\n\nLangGraph v1 is largely backwards compatible with previous versions. The main change is the deprecation of [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) in favor of LangChain's new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) function.\n\n## Deprecations\n\nThe following table lists all items deprecated in LangGraph v1:\n\n| Deprecated item                            | Alternative                                                                                                                                                                                                                                             |\n| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `create_react_agent`                       | [`langchain.agents.create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)                                                                                                                               |\n| `AgentState`                               | [`langchain.agents.AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                                                                                                   |\n| `AgentStatePydantic`                       | `langchain.agents.AgentState` (no more pydantic state)                                                                                                                                                                                                  |\n| `AgentStateWithStructuredResponse`         | `langchain.agents.AgentState`                                                                                                                                                                                                                           |\n| `AgentStateWithStructuredResponsePydantic` | `langchain.agents.AgentState` (no more pydantic state)                                                                                                                                                                                                  |\n| `HumanInterruptConfig`                     | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig`                                                                                                                                                                                       |\n| `ActionRequest`                            | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig`                                                                                                                                                                                       |\n| `HumanInterrupt`                           | `langchain.agents.middleware.human_in_the_loop.HITLRequest`                                                                                                                                                                                             |\n| `ValidationNode`                           | Tools automatically validate input with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)                                                                                                        |\n| `MessageGraph`                             | [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with a `messages` key, like [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides |\n\n## `create_react_agent` → `create_agent`\n\nLangGraph v1 deprecates the [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt. Use LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), which runs on LangGraph and adds a flexible middleware system.\n\nSee the LangChain v1 docs for details:\n\n* [Release notes](/oss/python/releases/langchain-v1#createagent)\n* [Migration guide](/oss/python/migrate/langchain-v1#migrate-to-create_agent)\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n\n  agent = create_agent(  # [!code highlight]\n      model,\n      tools,\n      system_prompt=\"You are a helpful assistant.\",\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n\n  agent = create_react_agent(  # [!code highlight]\n      model,\n      tools,\n      prompt=\"You are a helpful assistant.\",  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n## Breaking changes\n\n### Dropped Python 3.9 support\n\nAll LangChain packages now require **Python 3.10 or higher**. Python 3.9 reached [end of life](https://devguide.python.org/versions/) in October 2025.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/migrate",
    "char_count": 6064,
    "word_count": 311
  },
  {
    "title": "Changelog",
    "source": "https://docs.langchain.com/oss/python/releases/changelog",
    "content": "Log of updates and improvements to our Python packages\n\n<Update label=\"Nov 25, 2025\" tags={[\"langchain\"]}>\n  ## `langchain` v1.1.0\n\n  * [Model profiles](/oss/python/langchain/models#model-profiles): Chat models now expose supported features and capabilities through a `.profile` attribute. These data are derived from [models.dev](https://models.dev), an open source project providing model capability data.\n  * [Summarization middleware](/oss/python/langchain/middleware/built-in#summarization): Updated to support flexible trigger points using model profiles for context-aware summarization.\n  * [Structured output](/oss/python/langchain/structured-output): `ProviderStrategy` support (native structured output) can now be inferred from model profiles.\n  * [`SystemMessage` for `create_agent`](/oss/python/langchain/middleware/custom#working-with-system-messages): Support for passing `SystemMessage` instances directly to `create_agent`'s `system_prompt` parameter, enabling advanced features like cache control and structured content blocks.\n  * [Model retry middleware](/oss/python/langchain/middleware/built-in#model-retry): New middleware for automatically retrying failed model calls with configurable exponential backoff.\n  * [Content moderation middleware](/oss/python/langchain/middleware/built-in#content-moderation): OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.\n</Update>\n\n<Update label=\"Oct 20, 2025\" tags={[\"langchain\", \"langgraph\"]}>\n  ## v1.0.0\n\n  ### `langchain`\n\n  * [Release notes](/oss/python/releases/langchain-v1)\n  * [Migration guide](/oss/python/migrate/langchain-v1)\n\n  ### `langgraph`\n\n  * [Release notes](/oss/python/releases/langgraph-v1)\n  * [Migration guide](/oss/python/migrate/langgraph-v1)\n</Update>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/changelog.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/releases",
    "char_count": 2214,
    "word_count": 199
  },
  {
    "title": "What's new in LangChain v1",
    "source": "https://docs.langchain.com/oss/python/releases/langchain-v1",
    "content": "**LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n\n<CardGroup cols={1}>\n  <Card title=\"create_agent\" icon=\"robot\" href=\"#create-agent\" arrow>\n    The new standard for building agents in LangChain, replacing `langgraph.prebuilt.create_react_agent`.\n  </Card>\n\n  <Card title=\"Standard content blocks\" icon=\"cube\" href=\"#standard-content-blocks\" arrow>\n    A new `content_blocks` property that provides unified access to modern LLM features across providers.\n  </Card>\n\n  <Card title=\"Simplified namespace\" icon=\"sitemap\" href=\"#simplified-package\" arrow>\n    The `langchain` namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `langchain-classic`.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langchain\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain\n  ```\n</CodeGroup>\n\nFor a complete list of changes, see the [migration guide](/oss/python/migrate/langchain-v1).\n\n## `create_agent`\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) is the standard way to build agents in LangChain 1.0. It provides a simpler interface than [`langgraph.prebuilt.create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) while offering greater customization potential by using [middleware](#middleware).\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[search_web, analyze_data, send_email],\n    system_prompt=\"You are a helpful research assistant.\"\n)\n\nresult = agent.invoke({\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Research AI safety trends\"}\n    ]\n})\n```\n\nUnder the hood, [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063\" alt=\"Core agent loop diagram\" className=\"rounded-lg\" data-og-width=\"300\" width=\"300\" data-og-height=\"268\" height=\"268\" data-path=\"oss/images/core_agent_loop.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w\" />\n</div>\n\nFor more information, see [Agents](/oss/python/langchain/agents).\n\n### Middleware\n\nMiddleware is the defining feature of [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent). It offers a highly customizable entry-point, raising the ceiling for what you can build.\n\nGreat agents require [context engineering](/oss/python/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n\n#### Prebuilt middleware\n\nLangChain provides a few [prebuilt middlewares](/oss/python/langchain/middleware#built-in-middleware) for common patterns, including:\n\n* [`PIIMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.PIIMiddleware): Redact sensitive information before sending to the model\n* [`SummarizationMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.SummarizationMiddleware): Condense conversation history when it gets too long\n* [`HumanInTheLoopMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.HumanInTheLoopMiddleware): Require approval for sensitive tool calls\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    PIIMiddleware,\n    SummarizationMiddleware,\n    HumanInTheLoopMiddleware\n)\n\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[read_email, send_email],\n    middleware=[\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\n            \"phone_number\",\n            detector=(\n                r\"(?:\\+?\\d{1,3}[\\s.-]?)?\"\n                r\"(?:\\(?\\d{2,4}\\)?[\\s.-]?)?\"\n                r\"\\d{3,4}[\\s.-]?\\d{4}\"\n\t\t\t),\n\t\t\tstrategy=\"block\"\n        ),\n        SummarizationMiddleware(\n            model=\"claude-sonnet-4-5-20250929\",\n            trigger={\"tokens\": 500}\n        ),\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                \"send_email\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]\n                }\n            }\n        ),\n    ]\n)\n```\n\n#### Custom middleware\n\nYou can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agent's execution:\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware flow diagram\" className=\"rounded-lg\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n</div>\n\nBuild custom middleware by implementing any of these hooks on a subclass of the [`AgentMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware) class:\n\n| Hook              | When it runs             | Use cases                               |\n| ----------------- | ------------------------ | --------------------------------------- |\n| `before_agent`    | Before calling the agent | Load memory, validate input             |\n| `before_model`    | Before each LLM call     | Update prompts, trim messages           |\n| `wrap_model_call` | Around each LLM call     | Intercept and modify requests/responses |\n| `wrap_tool_call`  | Around each tool call    | Intercept and modify tool execution     |\n| `after_model`     | After each LLM response  | Validate output, apply guardrails       |\n| `after_agent`     | After agent completes    | Save results, cleanup                   |\n\nExample custom middleware:\n\n```python expandable theme={null}\nfrom dataclasses import dataclass\nfrom typing import Callable\n\nfrom langchain_openai import ChatOpenAI\n\nfrom langchain.agents.middleware import (\n    AgentMiddleware,\n    ModelRequest\n)\nfrom langchain.agents.middleware.types import ModelResponse\n\n@dataclass\nclass Context:\n    user_expertise: str = \"beginner\"\n\nclass ExpertiseBasedToolMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        user_level = request.runtime.context.user_expertise\n\n        if user_level == \"expert\":\n            # More powerful model\n            model = ChatOpenAI(model=\"gpt-5\")\n            tools = [advanced_search, data_analysis]\n        else:\n            # Less powerful model\n            model = ChatOpenAI(model=\"gpt-5-nano\")\n            tools = [simple_search, basic_calculator]\n\n        return handler(request.override(model=model, tools=tools))\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[\n        simple_search,\n        advanced_search,\n        basic_calculator,\n        data_analysis\n    ],\n    middleware=[ExpertiseBasedToolMiddleware()],\n    context_schema=Context\n)\n```\n\nFor more information, see [the complete middleware guide](/oss/python/langchain/middleware).\n\n### Built on LangGraph\n\nBecause [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) is built on [LangGraph](/oss/python/langgraph), you automatically get built in support for long running and reliable agents via:\n\n<CardGroup cols={2}>\n  <Card title=\"Persistence\" icon=\"database\">\n    Conversations automatically persist across sessions with built-in checkpointing\n  </Card>\n\n  <Card title=\"Streaming\" icon=\"water\">\n    Stream tokens, tool calls, and reasoning traces in real-time\n  </Card>\n\n  <Card title=\"Human-in-the-loop\" icon=\"hand\">\n    Pause agent execution for human approval before sensitive actions\n  </Card>\n\n  <Card title=\"Time travel\" icon=\"clock-rotate-left\">\n    Rewind conversations to any point and explore alternate paths and prompts\n  </Card>\n</CardGroup>\n\nYou don't need to learn LangGraph to use these features—they work out of the box.\n\n### Structured output\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) has improved structured output generation:\n\n* **Main loop integration**: Structured output is now generated in the main loop instead of requiring an additional LLM call\n* **Structured output strategy**: Models can choose between calling tools or using provider-side structured output generation\n* **Cost reduction**: Eliminates extra expense from additional LLM calls\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\nfrom pydantic import BaseModel\n\n\nclass Weather(BaseModel):\n    temperature: float\n    condition: str\n\ndef weather_tool(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"it's sunny and 70 degrees in {city}\"\n\nagent = create_agent(\n    \"gpt-4o-mini\",\n    tools=[weather_tool],\n    response_format=ToolStrategy(Weather)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}]\n})\n\nprint(repr(result[\"structured_response\"]))\n# results in `Weather(temperature=70.0, condition='sunny')`\n```\n\n**Error handling**: Control error handling via the `handle_errors` parameter to `ToolStrategy`:\n\n* **Parsing errors**: Model generates data that doesn't match desired structure\n* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas\n\n***\n\n## Standard content blocks\n\n<Note>\n  Content block support is currently only available for the following integrations:\n\n  * [`langchain-anthropic`](https://pypi.org/project/langchain-anthropic/)\n  * [`langchain-aws`](https://pypi.org/project/langchain-aws/)\n  * [`langchain-openai`](https://pypi.org/project/langchain-openai/)\n  * [`langchain-google-genai`](https://pypi.org/project/langchain-google-genai/)\n  * [`langchain-ollama`](https://pypi.org/project/langchain-ollama/)\n\n  Broader support for content blocks will be rolled out gradually across more providers.\n</Note>\n\nThe new [`content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) property introduces a standard representation for message content that works across providers:\n\n```python  theme={null}\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\nresponse = model.invoke(\"What's the capital of France?\")\n\n# Unified access to content blocks\nfor block in response.content_blocks:\n    if block[\"type\"] == \"reasoning\":\n        print(f\"Model reasoning: {block['reasoning']}\")\n    elif block[\"type\"] == \"text\":\n        print(f\"Response: {block['text']}\")\n    elif block[\"type\"] == \"tool_call\":\n        print(f\"Tool call: {block['name']}({block['args']})\")\n```\n\n### Benefits\n\n* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\n* **Type safe**: Full type hints for all content block types\n* **Backward compatible**: Standard content can be [loaded lazily](/oss/python/langchain/messages#standard-content-blocks), so there are no associated breaking changes\n\nFor more information, see our guide on [content blocks](/oss/python/langchain/messages#standard-content-blocks).\n\n***\n\n## Simplified package\n\nLangChain v1 streamlines the [`langchain`](https://pypi.org/project/langchain/) package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:\n\n### Namespace\n\n| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                                                                                       |\n| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality                                                           |\n| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from [`langchain-core`](https://reference.langchain.com/python/langchain_core/) |\n| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from [`langchain-core`](https://reference.langchain.com/python/langchain_core/) |\n| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization                                                                |\n| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                                                                            |\n\nMost of these are re-exported from `langchain-core` for convenience, which gives you a focused API surface for building agents.\n\n```python  theme={null}\n# Agent building\nfrom langchain.agents import create_agent\n\n# Messages and content\nfrom langchain.messages import AIMessage, HumanMessage\n\n# Tools\nfrom langchain.tools import tool\n\n# Model initialization\nfrom langchain.chat_models import init_chat_model\nfrom langchain.embeddings import init_embeddings\n```\n\n### `langchain-classic`\n\nLegacy functionality has moved to [`langchain-classic`](https://pypi.org/project/langchain-classic) to keep the core packages lean and focused.\n\n**What's in `langchain-classic`:**\n\n* Legacy chains and chain implementations\n* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)\n* The indexing API\n* The hub module (for managing prompts programmatically)\n* [`langchain-community`](https://pypi.org/project/langchain-community) exports\n* Other deprecated functionality\n\nIf you use any of this functionality, install [`langchain-classic`](https://pypi.org/project/langchain-classic):\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-classic\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-classic\n  ```\n</CodeGroup>\n\nThen update your imports:\n\n```python  theme={null}\nfrom langchain import ...  # [!code --]\nfrom langchain_classic import ...  # [!code ++]\n\nfrom langchain.chains import ...  # [!code --]\nfrom langchain_classic.chains import ...  # [!code ++]\n\nfrom langchain.retrievers import ...  # [!code --]\nfrom langchain_classic.retrievers import ...  # [!code ++]\n\nfrom langchain import hub  # [!code --]\nfrom langchain_classic import hub  # [!code ++]\n```\n\n## Migration guide\n\nSee our [migration guide](/oss/python/migrate/langchain-v1) for help updating your code to LangChain v1.\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchain/issues) using the `'v1'` [label](https://github.com/langchain-ai/langchain/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangChain 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Middleware guide\" icon=\"puzzle-piece\" href=\"https://blog.langchain.com/agent-middleware/\">\n    Deep dive into middleware\n  </Card>\n\n  <Card title=\"Agents Documentation\" icon=\"book\" href=\"/oss/python/langchain/agents\" arrow>\n    Full agent documentation\n  </Card>\n\n  <Card title=\"Message Content\" icon=\"message\" href=\"/oss/python/langchain/messages#message-content\" arrow>\n    New content blocks API\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/python/migrate/langchain-v1\" arrow>\n    How to migrate to LangChain v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langchain\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/python/versioning) – Understanding version numbers\n* [Release policy](/oss/python/release-policy) – Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langchain-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/releases",
    "char_count": 21203,
    "word_count": 1531
  },
  {
    "title": "What's new in LangGraph v1",
    "source": "https://docs.langchain.com/oss/python/releases/langgraph-v1",
    "content": "**LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt's designed to work hand-in-hand with [LangChain v1](/oss/python/releases/langchain-v1) (whose `create_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n\n<CardGroup cols={1}>\n  <Card title=\"Stable core APIs\" icon=\"diagram-project\">\n    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n  </Card>\n\n  <Card title=\"Reliability, by default\" icon=\"database\">\n    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n  </Card>\n\n  <Card title=\"Seamless with LangChain v1\" icon=\"link\">\n    LangChain's `create_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\n## Deprecation of `create_react_agent`\n\nThe LangGraph [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt has been deprecated in favor of LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent). It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n\n* For information on the new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) API, see the [LangChain v1 release notes](/oss/python/releases/langchain-v1#create-agent).\n* For information on migrating from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), see the [LangChain v1 migration guide](/oss/python/migrate/langchain-v1#create-agent).\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraph/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangGraph 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Overview\" icon=\"book\" href=\"/oss/python/langgraph/overview\" arrow>\n    What LangGraph is and when to use it\n  </Card>\n\n  <Card title=\"Graph API\" icon=\"diagram-project\" href=\"/oss/python/langgraph/graph-api\" arrow>\n    Build graphs with state, nodes, and edges\n  </Card>\n\n  <Card title=\"LangChain Agents\" icon=\"robot\" href=\"/oss/python/langchain/agents\" arrow>\n    High-level agents built on LangGraph\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/python/migrate/langgraph-v1\" arrow>\n    How to migrate to LangGraph v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langgraph\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/python/versioning) – Understanding version numbers\n* [Release policy](/oss/python/release-policy) – Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
    "source_file": "langchain",
    "category": "oss/python/releases",
    "char_count": 3906,
    "word_count": 339
  }
]