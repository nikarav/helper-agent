offline:
  prompts_dir: prompts/offline
  llm:
    default:
      model: gemini-2.0-flash
      temperature: 0.7
      max_tokens: 1024
    generate:
      max_tokens: 2048
    evaluate:
      temperature: 0.0  # Deterministic for consistent evaluation
    reformulate:
      temperature: 0.3
  retrieval:
    top_k: 5
    vectordb_path: data/vectordb
    collection_name: langgraph_docs
  agent:
    max_retries: 2
online:
  prompts_dir: prompts/online
  llm:
    default:
      model: gemini-2.0-flash
      temperature: 0.7
      max_tokens: 1024
    generate:
      temperature: 0.0
      max_tokens: 2048
  retrieval:
    top_k: 5
    vectordb_path: data/vectordb
    collection_name: langgraph_docs
  web_search:
    max_results: 5
